{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat basic implemention of transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp models.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import SGD\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import Counter, OrderedDict\n",
    "from dataclasses import dataclass, asdict\n",
    "from plum import dispatch\n",
    "\n",
    "# nimrod\n",
    "from nimrod.models.lm import Vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting\n",
    "- https://buomsoo-kim.github.io/attention/2020/04/21/Attention-mechanism-19.md/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/syl20/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bde618833f14f0c87f34c415c538586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ''}\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen \\'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe:                                                 0\n",
      "0                                 First Citizen:\n",
      "1  Before we proceed any further, hear me speak.\n",
      "List:  ['First Citizen:', 'Before we proceed any further, hear me speak.']\n",
      "Vocab:  68  !$&',-.3:;<bos><eos><pad><unk>?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# read unstructured text into pd\n",
    "df = pd.read_fwf('../data/en/tiny_shakespeare.txt', header=None)\n",
    "print(\"Dataframe: \", df.head(2))\n",
    "data = df[0].tolist()\n",
    "print(\"List: \", data[:2])\n",
    "v = Vocab(data)\n",
    "print(\"Vocab: \", len(v), ''.join(v.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "['F', 'i', 'r'] ['i', 'r', 's']\n",
      "['i', 'r', 's'] ['r', 's', 't']\n",
      "['r', 's', 't'] ['s', 't', ' ']\n",
      "['s', 't', ' '] ['t', ' ', 'C']\n",
      "['t', ' ', 'C'] [' ', 'C', 'i']\n",
      "[' ', 'C', 'i'] ['C', 'i', 't']\n",
      "['C', 'i', 't'] ['i', 't', 'i']\n",
      "['i', 't', 'i'] ['t', 'i', 'z']\n",
      "['t', 'i', 'z'] ['i', 'z', 'e']\n",
      "['i', 'z', 'e'] ['z', 'e', 'n']\n",
      "['z', 'e', 'n'] ['e', 'n', ':']\n",
      "['e', 'n', ':'] ['n', ':']\n",
      "['n', ':'] [':']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def make_dataset(names:List[str], verbose:bool=False, pad_value=0, context_length=3):\n",
    "#     X = []\n",
    "#     y = []\n",
    "#     return torch.tensor(X),torch.tensor(y)\n",
    "context_length = 3\n",
    "print(data[0])\n",
    "for i in range(len(data[0])-1):\n",
    "    print(list(data[0])[i:context_length+i], list(data[0])[i+1:context_length+i+1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
