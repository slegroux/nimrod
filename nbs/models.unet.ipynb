{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_show: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "> Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.models.conv import ConvLayer\n",
    "from nimrod.models.resnet import ResBlock\n",
    "from nimrod.models.superres import UpBlock\n",
    "from nimrod.utils import get_device, set_seed\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from typing import List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(ni, nf, kernel_size=3, norm=None):\n",
    "    return nn.Sequential(\n",
    "        nn.UpsamplingNearest2d(scale_factor=2),\n",
    "        ResBlock(ni, nf, kernel_size=kernel_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 128, 128)\n",
    "up_block(3, 6)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def init_weights(m, leaky=0.):\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)\n",
    "\n",
    "def zero_weights(layer):\n",
    "    with torch.no_grad():\n",
    "        layer.weight.zero_()\n",
    "        if hasattr(layer, 'bias') and hasattr(layer.bias, 'zero_'): layer.bias.zero_()\n",
    "\n",
    "class TinyUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024], # Number of features in each layer\n",
    "        activation=partial(nn.LeakyReLU, negative_slope=0.1), # Activation function\n",
    "        leaky:float=0.1,# Leaky ReLU negative slope\n",
    "        normalization=nn.BatchNorm2d # Normalization function\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # first layer\n",
    "        self.start = ResBlock(n_features[0], n_features[1], kernel_size=3, stride=1, activation=activation, normalization=normalization)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        # encoder downsample receptive field\n",
    "        down = partial(ResBlock, kernel_size=3,  stride=2, activation=activation, normalization=normalization)\n",
    "        for i in range(1, len(n_features) - 1):\n",
    "            self.encoder += [down(n_features[i], n_features[i+1])]\n",
    "\n",
    "        # decoder upsampling receptive field\n",
    "        up = partial(UpBlock, kernel_size=3, activation=activation, normalization=normalization)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(len(n_features) - 1, 1, -1):\n",
    "            self.decoder += [up(n_features[i], n_features[i-1])]\n",
    "        self.decoder += [up(n_features[1], n_features[0])]\n",
    "        self.end = ResBlock(n_features[0], n_features[0], kernel_size=3, stride=2, activation=nn.Identity, normalization=normalization)\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        layers = [] # store the output of each layer\n",
    "        x = self.start(x)\n",
    "        for layer in self.encoder:\n",
    "            layers.append(x)\n",
    "            x = layer(x)\n",
    "        n = len(layers)\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if i != 0:\n",
    "                x += layers[n-i]\n",
    "            x = layer(x)\n",
    "        return self.end(x+layers[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:33:35] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TinyUnet(n_features\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m])\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model(x)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m, in \u001b[0;36mTinyUnet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m layers[n\u001b[38;5;241m-\u001b[39mi]\n\u001b[1;32m     47\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend(x\u001b[38;5;241m+\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "model = TinyUnet(n_features=[3, 16, 32])\n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyUnet(nn.Module):\n",
    "    def __init__(self, act=nn.ReLU, nfs=(32,64,128,256,512,1024), norm=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.start = ResBlock(3, nfs[0], stride=1, activation=act, normalization=norm)\n",
    "        self.dn = nn.ModuleList([ResBlock(nfs[i], nfs[i+1], activation=act, normalization=norm, stride=2)\n",
    "                                 for i in range(len(nfs)-1)])\n",
    "        self.up = nn.ModuleList([UpBlock(nfs[i], nfs[i-1], activation=act, normalization=norm)\n",
    "                                 for i in range(len(nfs)-1,0,-1)])\n",
    "        self.up += [ResBlock(nfs[0], 3, activation=act, normalization=norm)]\n",
    "        self.end = ResBlock(3, 3, activation=nn.Identity, normalization=norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = []\n",
    "        layers.append(x)\n",
    "        x = self.start(x)\n",
    "        for l in self.dn:\n",
    "            layers.append(x)\n",
    "            x = l(x)\n",
    "        n = len(layers)\n",
    "        for i,l in enumerate(self.up):\n",
    "            if i!=0: x += layers[n-i]\n",
    "            x = l(x)\n",
    "        return self.end(x+layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:58:55] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TinyUnet()\n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
