{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_show: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "> Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch_lr_finder/lr_finder.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.models.conv import ConvLayer\n",
    "from nimrod.models.resnet import ResBlock\n",
    "from nimrod.utils import get_device, set_seed\n",
    "\n",
    "from typing import List\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_block(ni, nf, kernel_size=3, norm=None):\n",
    "    return nn.Sequential(\n",
    "        nn.UpsamplingNearest2d(scale_factor=2),\n",
    "        ResBlock(ni, nf, kernel_size=kernel_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:47:36] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 128, 128)\n",
    "up_block(3, 6)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyUnet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int = 3,\n",
    "            n_features: List[int]= [32, 64, 128, 256, 512, 1024],\n",
    "            normalization=nn.BatchNorm2d,\n",
    "            activation=nn.ReLU,\n",
    "            ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Identity())\n",
    "        start = ResBlock(in_channels, n_features[0])\n",
    "        dn = nn.ModuleList(\n",
    "            [ResBlock(n_features[i], n_features[i+1]) for i in range(len(n_features)-1)]\n",
    "            )\n",
    "        self.layers += dn\n",
    "\n",
    "        self.up = nn.ModuleList(\n",
    "            [up_block(n_features[i], n_features[i-1]) for i in range(len(n_features)-1,0,-1)]\n",
    "            )\n",
    "        \n",
    "        self.up += [ResBlock(n_features[0], 3)]\n",
    "        self.end = ResBlock(3, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = len(self.layers)\n",
    "        for i,l in enumerate(self.up):\n",
    "            if i!=0: x += self.layers[n-i]\n",
    "            x = l(x)\n",
    "        return self.end(x+self.layers[0])\n",
    "\n",
    "        \n",
    "        # x = self.start(x)\n",
    "        # for l in self.dn:\n",
    "        #     layers.append(x)\n",
    "        #     x = l(x)\n",
    "        # n = len(layers)\n",
    "        # for i,l in enumerate(self.up):\n",
    "        #     if i!=0: x += layers[n-i]\n",
    "        #     x = l(x)\n",
    "        # return self.end(x+layers[0])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[16:53:36] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [TinyUnet] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TinyUnet()\n\u001b[1;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model(x)\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:351\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [TinyUnet] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "model = TinyUnet()\n",
    "x = torch.randn(1, 3, 128, 128)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
