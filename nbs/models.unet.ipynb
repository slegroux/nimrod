{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net\n",
    "\n",
    "> Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch_lr_finder/lr_finder.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[14:05:22] INFO - PyTorch version 2.3.0 available.\n",
      "Seed set to 42\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Seed set to 42\n",
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.models.conv import ConvBlock, DeconvBlock\n",
    "from nimrod.models.resnet import ResBlock\n",
    "from nimrod.models.core import Regressor\n",
    "from nimrod.utils import get_device, set_seed\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from typing import List, Optional, Callable, Any\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "def init_weights(m, leaky=0.):\n",
    "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)\n",
    "\n",
    "def zero_weights(layer):\n",
    "    with torch.no_grad():\n",
    "        layer.weight.zero_()\n",
    "        if hasattr(layer, 'bias') and hasattr(layer.bias, 'zero_'): layer.bias.zero_()\n",
    "\n",
    "class TinyUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024], # Number of features in each layer\n",
    "        activation=partial(nn.LeakyReLU, negative_slope=0.1), # Activation function\n",
    "        leaky:float=0.1,# Leaky ReLU negative slope\n",
    "        normalization=nn.BatchNorm2d, # Normalization function\n",
    "        pre_activation:bool=False # use Resblock with pre-activation\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if len(n_features) < 3:\n",
    "            raise ValueError(\"n_features must be at least 3\")\n",
    "        # first layer\n",
    "        self.start = ResBlock(\n",
    "            n_features[0],\n",
    "            n_features[1],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            activation=activation,\n",
    "            normalization=normalization,\n",
    "            pre_activation=pre_activation\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        # encoder downsample receptive field\n",
    "        down = partial(\n",
    "            ResBlock,\n",
    "            kernel_size=3, \n",
    "            stride=2,\n",
    "            activation=activation,\n",
    "            normalization=normalization,\n",
    "            pre_activation=pre_activation\n",
    "            )\n",
    "\n",
    "        for i in range(1, len(n_features)-1):\n",
    "            self.encoder.append(down(n_features[i], n_features[i+1]))\n",
    "\n",
    "        # decoder upsampling receptive field\n",
    "        up = partial(DeconvBlock, kernel_size=3, activation=activation, normalization=normalization)\n",
    "\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(len(n_features)-1, 1, -1):\n",
    "            self.decoder.append(up(n_features[i], n_features[i-1]))\n",
    "\n",
    "        self.decoder += [down(n_features[1], n_features[0], stride=1)]\n",
    "\n",
    "        self.end = ResBlock(\n",
    "            n_features[0],\n",
    "            n_features[0],\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            activation=nn.Identity,\n",
    "            normalization=normalization,\n",
    "            pre_activation=pre_activation\n",
    "            )\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        layers = [] # store the output of each layer\n",
    "        layers.append(x)\n",
    "        x = self.start(x)\n",
    "        for layer in self.encoder:\n",
    "            layers.append(x)\n",
    "            x = layer(x)\n",
    "        n = len(layers)\n",
    "        for i, layer in enumerate(self.decoder):\n",
    "            if i != 0:\n",
    "                x += layers[n-i]\n",
    "            x = layer(x)\n",
    "        return self.end(x+layers[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:11:18] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TinyUnet(n_features=[3, 16, 32], pre_activation=True)\n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class TinyUnetX(Regressor):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nnet:TinyUnet, # super res autoencoder neural net\n",
    "        optimizer: Callable[...,torch.optim.Optimizer], # optimizer partial\n",
    "        scheduler: Optional[Callable[...,Any]]=None, # scheduler partial\n",
    "    ):\n",
    "        logger.info(\"SuperResAutoencoderX: init\")\n",
    "        super().__init__(\n",
    "            nnet=nnet,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler\n",
    "            )\n",
    "        self.nnet = nnet\n",
    "        self.register_module('nnet', self.nnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:42] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../config/model/image/unetx.yaml')\n",
    "model = instantiate(cfg.nnet)\n",
    "x = torch.randn(1, 3, 64, 64)\n",
    "model(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
