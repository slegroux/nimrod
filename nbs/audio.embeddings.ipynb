{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp audio.embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from encodec import EncodecModel\n",
    "from encodec.utils import convert_audio\n",
    "\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "from lhotse.features import FeatureExtractor\n",
    "from lhotse.utils import compute_num_frames, Seconds\n",
    "from lhotse import CutSet, Fbank\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "\n",
    "from dataclasses import asdict, dataclass\n",
    "from typing import Any, Dict, List, Optional, Pattern, Union\n",
    "from plum import dispatch\n",
    "\n",
    "from nimrod.audio.utils import plot_waveform\n",
    "from nimrod.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoDec():\n",
    "    def __init__(self, device:str='cpu'):\n",
    "        self.model = EncodecModel.encodec_model_24khz()\n",
    "        self._device = device\n",
    "        self.model.to(self._device)\n",
    "        self.model.set_target_bandwidth(6.0)\n",
    "\n",
    "    @dispatch\n",
    "    def __call__(self, wav:torch.Tensor, sr:int)->torch.Tensor:\n",
    "        # (CxT) -> (CxDxT_frames)\n",
    "        if sr != self.model.sample_rate:\n",
    "            wav = convert_audio(wav, sr, self.model.sample_rate, self.model.channels) # model.sample_rate=24kHz\n",
    "        wav = wav.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = self.model.encode(wav)\n",
    "        codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
    "        return(codes)\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, wav:np.ndarray, sr:int)->torch.Tensor:\n",
    "        wav = torch.from_numpy(wav).float().unsqueeze(0)\n",
    "        if sr != self.model.sample_rate:\n",
    "            wav = convert_audio(wav, sr, self.model.sample_rate, self.model.channels) # model.sample_rate=24kHz\n",
    "        # wav = wav.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = self.model.encode(wav.to(self._device))\n",
    "        codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)\n",
    "        return(codes)\n",
    "\n",
    "    def decode(self, codes:torch.Tensor)->torch.Tensor:\n",
    "        # (CxDxT_frames) -> (CxT)\n",
    "        frames_from_code = [(codes, None)]\n",
    "        return(self.model.decode(encoded_frames=frames_from_code))\n",
    "\n",
    "    @property\n",
    "    def sample_rate(self):\n",
    "        return self.model.sample_rate\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = torchaudio.load(\"../data/audio/obama.wav\")\n",
    "# wav, sr = torch.rand((1, 24000)), 24000\n",
    "# wav, sr = np.random.random((1, 24000)), 24000\n",
    "\n",
    "encodec = EncoDec(device='cpu')\n",
    "codes = encodec(wav,sr)\n",
    "print(f\"wav: {wav.shape}, code: {codes.shape} \")\n",
    "plt.rcParams[\"figure.figsize\"] = (5,5)\n",
    "plt.xlabel('frames')\n",
    "plt.ylabel('quantization')\n",
    "plt.imshow(codes.squeeze().cpu().numpy())\n",
    "decoded = encodec.decode(codes)\n",
    "plot_waveform(decoded.detach().cpu().squeeze(0), encodec.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(codes[0][0])\n",
    "print(codes[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "ipd.Audio(wav.squeeze(0).numpy(), rate=sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lhotse-style Encodec feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# https://lhotse.readthedocs.io/en/v0.6_ba/features.html#creating-custom-feature-extractor\n",
    "@dataclass\n",
    "class EncoDecConfig:\n",
    "    # The encoder produces embeddings at 75 Hz for input waveforms at 24 kHz,\n",
    "    # which is a 320-fold reduction in the sampling rate.\n",
    "    frame_shift: float = 320.0 / 24000\n",
    "    n_q: int = 8\n",
    "\n",
    "class EncoDecExtractor(FeatureExtractor):\n",
    "    name = 'encodec'\n",
    "    config_type = EncoDecConfig\n",
    "    def __init__(self, config=EncoDecConfig()):\n",
    "        super().__init__(config)\n",
    "        self.encodec = EncoDec()\n",
    "\n",
    "    def extract(self, samples:Union[torch.Tensor, np.ndarray], sampling_rate: int) -> np.ndarray:    \n",
    "        codes = self.encodec(samples, sampling_rate)\n",
    "        duration = round(samples.shape[-1] / sampling_rate, ndigits=12)\n",
    "        expected_num_frames = compute_num_frames(\n",
    "            duration=duration,\n",
    "            frame_shift=self.frame_shift,\n",
    "            sampling_rate=sampling_rate,\n",
    "        )\n",
    "        assert abs(codes.shape[-1] - expected_num_frames) <= 1\n",
    "        codes = codes[..., :expected_num_frames]\n",
    "        return codes.cpu().squeeze(0).permute(1, 0).numpy()\n",
    "\n",
    "    @property\n",
    "    def frame_shift(self)->float:\n",
    "        return self.config.frame_shift\n",
    "\n",
    "    def feature_dim(self, sampling_rate: int) -> int:\n",
    "        return self.config.n_q\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodec_extractor = EncoDecExtractor()\n",
    "# cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/first_3.jsonl.gz\")\n",
    "cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\n",
    "print(cuts[0])\n",
    "print(cuts[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(1)\n",
    "# torch.set_num_interop_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feats = cuts.compute_and_store_features(extractor=Fbank(), storage_path=\"../recipes/tts/ljspeech/data/feats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_path = \"../.data/en/LJSpeech-1.1\"\n",
    "# # storage_path = \"../recipes/tts/ljspeech/data/feats\"\n",
    "# # TODO: make it work with num_jobs>1\n",
    "# cuts = cuts.compute_and_store_features(\n",
    "#     extractor=encodec_extractor,\n",
    "#     storage_path=storage_path,\n",
    "#     num_jobs=1,\n",
    "# )\n",
    "# cuts.to_file(\"../recipes/tts/ljspeech/data/cuts_encodec.jsonl.gz\")\n",
    "# print(cuts[0])\n",
    "# cuts[0].plot_features()\n",
    "# print(cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = \"../data/en/LJSpeech-1.1/cuts_encodec.jsonl.gz\"\n",
    "# files = \"../recipes/tts/ljspeech/data/cuts_encodec.jsonl.gz\"\n",
    "cuts = CutSet.from_file(files)\n",
    "print(cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HF\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import EncodecModel, AutoProcessor\n",
    "\n",
    "# dummy dataset, however you can swap this with an dataset on the ðŸ¤— hub or bring your own\n",
    "librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# load the model + processor (for pre-processing the audio)\n",
    "model = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n",
    "librispeech_dummy[0]\n",
    "# cast the audio data to the correct sampling rate for the model\n",
    "librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n",
    "audio_sample = librispeech_dummy[0][\"audio\"][\"array\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
