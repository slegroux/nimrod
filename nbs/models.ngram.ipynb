{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language modeling\n",
    "\n",
    "> \"old school\" language modeling based on counting tokens in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import Counter\n",
    "\n",
    "import kenlm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karpathy/makemore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CharUnigram:\n",
    "    def __init__(self, data:List[str]):\n",
    "        self._count = {}\n",
    "        self.total_count = 0\n",
    "        self.unique_chars = set()\n",
    "        for name in data:\n",
    "            for c in name:\n",
    "                self.unique_chars.update(c)\n",
    "                if c in self._count:\n",
    "                    self._count[c] += 1\n",
    "                else:\n",
    "                    self._count[c] = 1\n",
    "                self.total_count += 1\n",
    "        self._probs = {k:v/self.total_count for k,v in self._count.items()}\n",
    "        self._count = self.sort_dict_by_value(self._count)\n",
    "        self._probs = self.sort_dict_by_value(self._probs)\n",
    "        self.unique_chars = sorted(self.unique_chars)\n",
    "        self._stoi = {v:idx for idx,v in enumerate(self.unique_chars)}\n",
    "        self._itos = {idx:v for idx,v in enumerate(self.unique_chars)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.unique_chars)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_dict_by_value(dict:Dict, reverse:bool=True)->Dict:\n",
    "        return {k:v for k,v in sorted(dict.items(), reverse=reverse, key=lambda x:x[1])}\n",
    "\n",
    "    @property\n",
    "    def counts(self)->Dict:\n",
    "        return(self._count)\n",
    "\n",
    "    @property\n",
    "    def probs(self)->Dict:\n",
    "        return(self._probs)\n",
    "\n",
    "    @property\n",
    "    def chars(self)->List:\n",
    "        return self.unique_chars\n",
    "        \n",
    "    def stoi(self, char:str)->int:\n",
    "        return(self._stoi[char])\n",
    "    \n",
    "    def itos(self, idx:int)->str:\n",
    "        return(self._itos[idx])\n",
    "    \n",
    "    def sample(self)->str:\n",
    "        # get probs for order list of characters to build prob table\n",
    "        prob_distrib = torch.tensor([self._probs[k] for k in self.unique_chars])\n",
    "        idx = int(torch.multinomial(prob_distrib,num_samples=1,replacement=True))\n",
    "        return(self._itos[idx])\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without pandas\n",
    "with open('../data/text/names.txt', 'r') as f:\n",
    "    list_of_words = f.read().splitlines()\n",
    "# with pandas\n",
    "df = pd.read_csv('../data/text/names.txt', names=['name'], header=None)\n",
    "list_of_words = list(df.head().name)\n",
    "\n",
    "unigram = CharUnigram(list_of_words)\n",
    "print(\"sorted counts: \", unigram.counts)\n",
    "print(\"sorted probs: \", unigram.probs)\n",
    "print(len(unigram))\n",
    "print(unigram.chars)\n",
    "print(unigram._stoi)\n",
    "print(unigram.stoi('a'))\n",
    "print(unigram.itos(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(unigram.counts, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(10000):\n",
    "    s = unigram.sample()\n",
    "    samples.append(s)\n",
    "\n",
    "# sampled\n",
    "count = Counter([c for w in samples for c in w])\n",
    "df = pd.DataFrame.from_dict(count, orient='index')\n",
    "df[0].sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharBigram():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "with open('../data/text/names.txt', 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "print(\"first lines of text: \", data[:10])\n",
    "\n",
    "# data = [\"this is a text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigram counts\n",
    "bigrams = {}\n",
    "unique_tokens = set()\n",
    "for name in data:\n",
    "    line = list(name)\n",
    "    unique_tokens.update(line)\n",
    "    line.append('<stop>')\n",
    "    line.insert(0, '<stop>')\n",
    "    for i,v in enumerate(range(len(line)-1)):\n",
    "        bigram = (line[i], line[i+1])\n",
    "        if bigram in bigrams:\n",
    "            bigrams[bigram] += 1\n",
    "        else:\n",
    "            bigrams[bigram] = 1\n",
    "\n",
    "# print(\"unsorted: \", list(bigrams)[:10])\n",
    "# print(\"sorted: \", sort_dict_by_value(bigrams))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(unique_tokens)\n",
    "# use same for start & stop in this case (separate lines of names)\n",
    "# tokens.append('<start>')\n",
    "tokens.append('<stop>')\n",
    "print(tokens)\n",
    "stoi = {v:i for i,v in enumerate(tokens)}\n",
    "itos = {i:v for i, v in enumerate(tokens)}\n",
    "print(stoi, itos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_toks = len(tokens)\n",
    "print(n_toks)\n",
    "N = torch.zeros((n_toks, n_toks)).long()\n",
    "print(N.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bigram, value in bigrams.items():\n",
    "    idx1, idx2 = stoi[bigram[0]], stoi[bigram[1]]\n",
    "    N[idx1, idx2] = value\n",
    "\n",
    "plt.xlabel('char_t+1')\n",
    "plt.ylabel('char_t')\n",
    "i = [i for i, v in itos.items()]\n",
    "v = [v for i,v in itos.items()]\n",
    "plt.xticks(i, v)\n",
    "plt.yticks(i, v)\n",
    "plt.imshow(N, origin='lower')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From counts to probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing avoids having log(0) = inf when computing NLL loss\n",
    "smoothing = 1\n",
    "P = (N.float()+smoothing) / N.sum(1,keepdim=True)\n",
    "plt.imshow(P, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_6 = (N[6,:]/N[6,:].sum())\n",
    "print(row_6)\n",
    "print(row_6.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = P[6, :]\n",
    "print(p.sum(), p.max(), torch.argmax(p))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    res = []\n",
    "    prev = stoi['<stop>']\n",
    "    while True:\n",
    "        # max prob sampling\n",
    "        next = int(torch.argmax(P[prev, :]))\n",
    "        # multinomial sampling\n",
    "        next = int(torch.multinomial(P[prev,:],num_samples=1,replacement=True))\n",
    "        if next == stoi['<stop>']:\n",
    "            print(''.join(res))\n",
    "            break\n",
    "        else:\n",
    "            res.append(itos[next])\n",
    "            prev = next\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log likelihood loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_p = {}\n",
    "for bigram, value in bigrams.items():\n",
    "    idx1, idx2 = stoi[bigram[0]], stoi[bigram[1]]\n",
    "    bigram_p[bigram] = P[idx1,idx2]\n",
    "\n",
    "print(bigram_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_p_sorted = {k: v.float() for k, v in sorted(bigram_p.items(), reverse=True, key=lambda x: x[1])}\n",
    "print(bigram_p_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likelihood of full corpus = product of all bigram prods\n",
    "l = 0\n",
    "for bigram, prob in bigram_p_sorted.items():\n",
    "    l += torch.log(prob)\n",
    "\n",
    "# negative log likelihood loss nll\n",
    "nll = -l /len(bigram_p_sorted)\n",
    "print(nll)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"this\"\n",
    "sample = [(word[i], word[i+1]) for i,c in enumerate(word) if i < len(word)-1]\n",
    "print(list(zip(*sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for word in data:\n",
    "    sample = [(stoi[word[i]], stoi[word[i+1]]) for i,c in enumerate(word) if i < len(word)-1]\n",
    "    x, y = list(zip(*sample)) # inverse of zip\n",
    "    xs.append(torch.tensor(x))\n",
    "    ys.append(torch.tensor(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x:', xs[:3])\n",
    "print('y', ys[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-hot encoded input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = [F.one_hot(x, num_classes=len(tokens)).float() for x in xs]\n",
    "print(enc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = enc[0]\n",
    "print(X.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Neural net' modeling\n",
    "we model the transition probability matrix by neural net activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(27, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = X @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "print(probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KenLM\n",
    "We refer to efficient kenlm implementation for larger n-gram models usable for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class KenLM:\n",
    "    def __init__(self, arpa_path:str, vocab:List):\n",
    "        # TODO: deal with zipped arpa models\n",
    "        self.model = kenlm.LanguageModel(arpa_path)\n",
    "        self.partial_text = []\n",
    "        self.partial_score = 0.0\n",
    "        # init new sentence\n",
    "        self.s1 = kenlm.State()\n",
    "        self.s2 = kenlm.State()\n",
    "        self.model.BeginSentenceWrite(self.s1)\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def init_vocab(self, vocab_path):\n",
    "        with open(vocab_path) as f: self.vocab = f.read().splitlines()\n",
    "    \n",
    "    def new_sentence_init(self):\n",
    "        self.partial_text = []\n",
    "        self.partial_score = 0.0\n",
    "        self.s1 = kenlm.State()\n",
    "        self.s2 = kenlm.State()\n",
    "        self.model.BeginSentenceWrite(self.s1)\n",
    "    \n",
    "    def append(self, word:str):\n",
    "        # add word to beam and update probs\n",
    "        if word == '.':\n",
    "            self.partial_score += self.model.BaseScore(self.s1, word, self.s2)\n",
    "            self.partial_score += self.model.BaseScore(self.s2, '</s>', self.s1)\n",
    "            self.partial_text.append(word)\n",
    "        else:\n",
    "            self.partial_score += self.model.BaseScore(self.s1, word, self.s2)\n",
    "            # input <=> output state\n",
    "            self.s1, self.s2 = self.s2, self.s1\n",
    "            self.partial_text.append(word)\n",
    "    \n",
    "    def peek(self, word:str, log_prob:bool=True)->float:\n",
    "        if log_prob:\n",
    "            # check prob of next word given context without update state\n",
    "            res = self.partial_score+self.model.BaseScore(self.s1, word, self.s2)\n",
    "        else:\n",
    "            res = 10**(self.partial_score+self.model.BaseScore(self.s1, word, self.s2))\n",
    "        return(res)\n",
    "\n",
    "    def sentence_score(self, sentence:str)->float:\n",
    "        return(self.model.score(sentence))\n",
    "\n",
    "    def partial_sentence_score(self, sentence:str)->float:\n",
    "        return(self.model.score(sentence, eos=False))\n",
    "        \n",
    "    def nbest(self, n:int, log_prob:bool=True):\n",
    "        if not self.vocab:\n",
    "            print('need to init vocab')\n",
    "        res = []\n",
    "        for word in self.vocab:\n",
    "            res.append((word, self.peek(word, log_prob)))\n",
    "\n",
    "        return sorted(res, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "    @property\n",
    "    def score(self)->float:\n",
    "        return(self.partial_score)\n",
    "\n",
    "    @property\n",
    "    def text(self)->str:\n",
    "        return(' '.join(self.partial_text))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data into kenlm format\n",
    "tokens separated by space with new sentence at each line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/text/names.txt', header=None, names=['name']) \n",
    "df = df.name.apply(lambda x: list(x)) # str into list of char\n",
    "# df.apply(lambda x: x.append('<eos>')) # if eos needed\n",
    "print(df.head())\n",
    "df_toks = df.str.join(' ') # for kenlm input format tokens are separated by space\n",
    "print(df_toks.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "# for row in df.iterrows():\n",
    "#     print(row)\n",
    "tokens = set()\n",
    "for k,v in df.items():\n",
    "    tokens.update(list(v))\n",
    "\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data to kenlm format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = df.to_csv('../data/text/names.kenlm.txt', header=None, index=None)\n",
    "! bzip2 -kz ../data/text/names.kenlm.txt\n",
    "! bzcat ../data/text/names.kenlm.txt.bz2 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KenLM n-gram model\n",
    "https://lukesalamone.github.io/posts/running-simple-language-model/\n",
    "\n",
    "KenLM requires data to be one sentence per line lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! if [ ! -f \"../data/text/names.2gram.arpa\" ]; then lmplz --discount_fallback -o 2 < ../data/text/names.kenlm.txt.bz2>../data/text/names.2gram.arpa; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! if [ ! -f \"../data/text/names.2gram.kenlm\" ]; then build_binary ../data/text/names.2gram.arpa ../data/text/names.2gram.kenlm; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test original Kenlm python api probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel('../data/text/names.2gram.kenlm')\n",
    "sentence = \"emma\"\n",
    "tokenized = \"e m m a\"\n",
    "# model.score(\"emma\", bos = False, eos = False)\n",
    "words = ['<s>'] + list(sentence) + ['</s>']\n",
    "print(words)\n",
    "final = 0\n",
    "for i, (prob, length, oov) in enumerate(model.full_scores(tokenized)):\n",
    "    print(f'words: {words[i:i+length]} index:{i}, prob:{prob}, length:{length}, oov:{oov}')\n",
    "    final += prob\n",
    "\n",
    "print(final)\n",
    "print(model.score(\"e m m a\"))\n",
    "print(f'prob <s> e: {model.score(\"e\", bos=True, eos=False)}')\n",
    "print(f'prob e: {model.score(\"e\", bos=False, eos=False)}')\n",
    "print(f'prob <s> e m: {model.score(\"e m\", bos=True, eos=False)}')\n",
    "print(f'prob e m: {model.score(\"e m\", bos=False, eos=False)}')\n",
    "state = kenlm.State()\n",
    "state2 = kenlm.State()\n",
    "model.BeginSentenceWrite(state)\n",
    "accum = 0\n",
    "accum += model.BaseScore(state, \"e\", state2)\n",
    "print(f'prob <s> e: {accum}')\n",
    "state, state2 = state2, state\n",
    "accum += model.BaseScore(state, \"m\", state2)\n",
    "print(f'prob <s> e m: {accum}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LM vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add special tokens to vocabulary\n",
    "tokens.add('<s>')\n",
    "tokens.add('</s>')\n",
    "tokens.add('<unk>')\n",
    "print(tokens, len(tokens))\n",
    "vocab = list(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference / Sampling from prob distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = KenLM('../data/text/names.2gram.kenlm', vocab)\n",
    "init_char = '<s> e m m'\n",
    "# probs = lm.nbest(len(vocab), log_prob=False)\n",
    "# print(np.sum([p for char, p in probs]))\n",
    "# res = [init_char]\n",
    "# next = int(torch.multinomial(P[prev,:],num_samples=1,replacement=True))\n",
    "for i in range(50):\n",
    "    lm.new_sentence_init()\n",
    "    lm.append(init_char)\n",
    "    while True:\n",
    "        # nbest probs at current state\n",
    "        probs = lm.nbest(len(vocab), log_prob=False)\n",
    "        # print(probs)\n",
    "        # print(np.sum(probs))\n",
    "        # sample from prob distribution\n",
    "        try:\n",
    "            index_next = int(torch.multinomial(torch.tensor([prob for char, prob in probs]),num_samples=1,replacement=True))\n",
    "        except:\n",
    "            print(\"probs too small\")\n",
    "            break\n",
    "        char_next = probs[index_next][0]\n",
    "        lm.append(char_next)\n",
    "        # print(init_char + '<s>')\n",
    "        if char_next == '</s>' or char_next == '<s>' and lm.text != init_char and (lm.text != init_char+' <s>'):\n",
    "            print(lm.text.replace(' ', ''))\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
