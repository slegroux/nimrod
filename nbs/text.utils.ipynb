{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import Dict\n",
    "import inflect\n",
    "from anyascii import anyascii"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TTSTextNormalizer:\n",
    "    def __init__(self):\n",
    "        # ABBREVIATIONS\n",
    "        self.abbreviations_en = [\n",
    "            (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n",
    "            for x in [\n",
    "                (\"mrs\", \"misess\"),\n",
    "                (\"mr\", \"mister\"),\n",
    "                (\"dr\", \"doctor\"),\n",
    "                (\"st\", \"saint\"),\n",
    "                (\"co\", \"company\"),\n",
    "                (\"jr\", \"junior\"),\n",
    "                (\"maj\", \"major\"),\n",
    "                (\"gen\", \"general\"),\n",
    "                (\"drs\", \"doctors\"),\n",
    "                (\"rev\", \"reverend\"),\n",
    "                (\"lt\", \"lieutenant\"),\n",
    "                (\"hon\", \"honorable\"),\n",
    "                (\"sgt\", \"sergeant\"),\n",
    "                (\"capt\", \"captain\"),\n",
    "                (\"esq\", \"esquire\"),\n",
    "                (\"ltd\", \"limited\"),\n",
    "                (\"col\", \"colonel\"),\n",
    "                (\"ft\", \"fort\"),\n",
    "            ]\n",
    "        ]\n",
    "        # NUMBERS\n",
    "        self._inflect = inflect.engine()\n",
    "        self._comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n",
    "        self._decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n",
    "        self._currency_re = re.compile(r\"(£|\\$|¥)([0-9\\,\\.]*[0-9]+)\")\n",
    "        self._ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n",
    "        self._number_re = re.compile(r\"-?[0-9]+\")\n",
    "        # TIME\n",
    "        self._time_re = re.compile(\n",
    "            r\"\"\"\\b\n",
    "            ((0?[0-9])|(1[0-1])|(1[2-9])|(2[0-3]))  # hours\n",
    "            :\n",
    "            ([0-5][0-9])                            # minutes\n",
    "            \\s*(a\\\\.m\\\\.|am|pm|p\\\\.m\\\\.|a\\\\.m|p\\\\.m)? # am/pm\n",
    "            \\b\"\"\",\n",
    "            re.IGNORECASE | re.X,\n",
    "        )\n",
    "        # LANG CLEAN\n",
    "        self._whitespace_re = re.compile(r\"\\s+\")\n",
    "\n",
    "    # NUMBERS\n",
    "    def _remove_commas(self, m):\n",
    "        return m.group(1).replace(\",\", \"\")\n",
    "\n",
    "    def _expand_decimal_point(self, m):\n",
    "        return m.group(1).replace(\".\", \" point \")\n",
    "\n",
    "    def __expand_currency(self, value: str, inflection: Dict[float, str]) -> str:\n",
    "        parts = value.replace(\",\", \"\").split(\".\")\n",
    "        if len(parts) > 2:\n",
    "            return f\"{value} {inflection[2]}\"  # Unexpected format\n",
    "        text = []\n",
    "        integer = int(parts[0]) if parts[0] else 0\n",
    "        if integer > 0:\n",
    "            integer_unit = inflection.get(integer, inflection[2])\n",
    "            text.append(f\"{integer} {integer_unit}\")\n",
    "        fraction = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n",
    "        if fraction > 0:\n",
    "            fraction_unit = inflection.get(fraction / 100, inflection[0.02])\n",
    "            text.append(f\"{fraction} {fraction_unit}\")\n",
    "        if len(text) == 0:\n",
    "            return f\"zero {inflection[2]}\"\n",
    "        return \" \".join(text)\n",
    "\n",
    "    def _expand_currency(self, m: \"re.Match\") -> str:\n",
    "        currencies = {\n",
    "            \"$\": {\n",
    "                0.01: \"cent\",\n",
    "                0.02: \"cents\",\n",
    "                1: \"dollar\",\n",
    "                2: \"dollars\",\n",
    "            },\n",
    "            \"€\": {\n",
    "                0.01: \"cent\",\n",
    "                0.02: \"cents\",\n",
    "                1: \"euro\",\n",
    "                2: \"euros\",\n",
    "            },\n",
    "            \"£\": {\n",
    "                0.01: \"penny\",\n",
    "                0.02: \"pence\",\n",
    "                1: \"pound sterling\",\n",
    "                2: \"pounds sterling\",\n",
    "            },\n",
    "            \"¥\": {\n",
    "                # TODO rin\n",
    "                0.02: \"sen\",\n",
    "                2: \"yen\",\n",
    "            },\n",
    "        }\n",
    "        unit = m.group(1)\n",
    "        currency = currencies[unit]\n",
    "        value = m.group(2)\n",
    "        return self.__expand_currency(value, currency)\n",
    "\n",
    "    def _expand_ordinal(self, m):\n",
    "        return self._inflect.number_to_words(m.group(0))\n",
    "\n",
    "    def _expand_number(self, m):\n",
    "        num = int(m.group(0))\n",
    "        if 1000 < num < 3000:\n",
    "            if num == 2000:\n",
    "                return \"two thousand\"\n",
    "            if 2000 < num < 2010:\n",
    "                return \"two thousand \" + self._inflect.number_to_words(num % 100)\n",
    "            if num % 100 == 0:\n",
    "                return self._inflect.number_to_words(num // 100) + \" hundred\"\n",
    "            return self._inflect.number_to_words(num, andword=\"\", zero=\"oh\", group=2).replace(\", \", \" \")\n",
    "        return self._inflect.number_to_words(num, andword=\"\")\n",
    "\n",
    "    def en_normalize_numbers(self, text):\n",
    "        text = re.sub(self._comma_number_re, self._remove_commas, text)\n",
    "        text = re.sub(self._currency_re, self._expand_currency, text)\n",
    "        text = re.sub(self._decimal_number_re, self._expand_decimal_point, text)\n",
    "        text = re.sub(self._ordinal_re, self._expand_ordinal, text)\n",
    "        text = re.sub(self._number_re, self._expand_number, text)\n",
    "        return text\n",
    "\n",
    "    # TIME\n",
    "    def _expand_num(self, n: int) -> str:\n",
    "        return self._inflect.number_to_words(n)\n",
    "\n",
    "    def _expand_time_english(self, match: \"re.Match\") -> str:\n",
    "        hour = int(match.group(1))\n",
    "        past_noon = hour >= 12\n",
    "        time = []\n",
    "        if hour > 12:\n",
    "            hour -= 12\n",
    "        elif hour == 0:\n",
    "            hour = 12\n",
    "            past_noon = True\n",
    "        time.append(self._expand_num(hour))\n",
    "\n",
    "        minute = int(match.group(6))\n",
    "        if minute > 0:\n",
    "            if minute < 10:\n",
    "                time.append(\"oh\")\n",
    "            time.append(self._expand_num(minute))\n",
    "        am_pm = match.group(7)\n",
    "        if am_pm is None:\n",
    "            time.append(\"p m\" if past_noon else \"a m\")\n",
    "        else:\n",
    "            time.extend(list(am_pm.replace(\".\", \"\")))\n",
    "        return \" \".join(time)\n",
    "\n",
    "    def expand_time_english(self, text: str) -> str:\n",
    "        return re.sub(self._time_re, self._expand_time_english, text)\n",
    "\n",
    "    # BASIC CLEANING\n",
    "    def expand_abbreviations(self, text, lang=\"en\"):\n",
    "        if lang == \"en\":\n",
    "            _abbreviations = self.abbreviations_en\n",
    "        elif lang == \"fr\":\n",
    "            pass\n",
    "        for regex, replacement in _abbreviations:\n",
    "            text = re.sub(regex, replacement, text)\n",
    "        return text\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def collapse_whitespace(self, text):\n",
    "        return re.sub(self._whitespace_re, \" \", text).strip()\n",
    "\n",
    "    def convert_to_ascii(self, text):\n",
    "        return anyascii(text)\n",
    "\n",
    "    def remove_aux_symbols(self, text):\n",
    "        text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def replace_symbols(self, text, lang=\"en\"):\n",
    "        text = text.replace(\";\", \",\")\n",
    "        text = text.replace(\"-\", \" \")\n",
    "        text = text.replace(\":\", \",\")\n",
    "        if lang == \"en\":\n",
    "            text = text.replace(\"&\", \" and \")\n",
    "        elif lang == \"fr\":\n",
    "            text = text.replace(\"&\", \" et \")\n",
    "        elif lang == \"pt\":\n",
    "            text = text.replace(\"&\", \" e \")\n",
    "        return text\n",
    "    \n",
    "    # LANG CLEANERS\n",
    "\n",
    "    def basic_cleaners(self, text):\n",
    "        \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def transliteration_cleaners(self, text):\n",
    "        \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n",
    "        # text = convert_to_ascii(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def basic_german_cleaners(self, text):\n",
    "        \"\"\"Pipeline for German text\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def basic_turkish_cleaners(self, text):\n",
    "        \"\"\"Pipeline for Turkish text\"\"\"\n",
    "        text = text.replace(\"I\", \"ı\")\n",
    "        text = self.lowercase(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def english_cleaners(self, text):\n",
    "        \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n",
    "        # text = convert_to_ascii(text)\n",
    "        text = self.lowercase(text)\n",
    "        text = self.expand_time_english(text)\n",
    "        text = self.en_normalize_numbers(text)\n",
    "        text = self.expand_abbreviations(text)\n",
    "        text = self.replace_symbols(text)\n",
    "        text = self.remove_aux_symbols(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def phoneme_cleaners(self, text):\n",
    "        \"\"\"Pipeline for phonemes mode, including number and abbreviation expansion.\"\"\"\n",
    "        text = self.en_normalize_numbers(text)\n",
    "        text = self.expand_abbreviations(text)\n",
    "        text = self.replace_symbols(text)\n",
    "        text = self.remove_aux_symbols(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def french_cleaners(self, text):\n",
    "        \"\"\"Pipeline for French text. There is no need to expand numbers, phonemizer already does that\"\"\"\n",
    "        text = self.expand_abbreviations(text, lang=\"fr\")\n",
    "        text = self.lowercase(text)\n",
    "        text = self.replace_symbols(text, lang=\"fr\")\n",
    "        text = self.remove_aux_symbols(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def portuguese_cleaners(self, text):\n",
    "        \"\"\"Basic pipeline for Portuguese text. There is no need to expand abbreviation and\n",
    "        numbers, phonemizer already does that\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.replace_symbols(text, lang=\"pt\")\n",
    "        text = self.remove_aux_symbols(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n",
    "\n",
    "    def multilingual_cleaners(self, text):\n",
    "        \"\"\"Pipeline for multilingual text\"\"\"\n",
    "        text = self.lowercase(text)\n",
    "        text = self.replace_symbols(text, lang=None)\n",
    "        text = self.remove_aux_symbols(text)\n",
    "        text = self.collapse_whitespace(text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three hundred fifty dollars\n",
      "twelve oh five p m\n",
      "oh my dear! this is five dollars too soon... it's one oh four a m!\n"
     ]
    }
   ],
   "source": [
    "cleaner = TTSTextNormalizer()\n",
    "print(cleaner.en_normalize_numbers(\"$350\"))\n",
    "print(cleaner.expand_time_english(\"12:05pm\"))\n",
    "print(cleaner.english_cleaners(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import collections\n",
    "from enum import Enum\n",
    "import six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "_DEF_PUNCS = ';:,.!?¡¿—…\"«»“”'\n",
    "_PUNC_IDX = collections.namedtuple(\"_punc_index\", [\"punc\", \"position\"])\n",
    "\n",
    "class PuncPosition(Enum):\n",
    "    \"\"\"Enum for the punctuations positions\"\"\"\n",
    "\n",
    "    BEGIN = 0\n",
    "    END = 1\n",
    "    MIDDLE = 2\n",
    "    ALONE = 3\n",
    "\n",
    "class Punctuation:\n",
    "    \"\"\"Handle punctuations in text.\n",
    "\n",
    "    Just strip punctuations from text or strip and restore them later.\n",
    "\n",
    "    Args:\n",
    "        puncs (str): The punctuations to be processed. Defaults to `_DEF_PUNCS`.\n",
    "\n",
    "    Example:\n",
    "        >>> punc = Punctuation()\n",
    "        >>> punc.strip(\"This is. example !\")\n",
    "        'This is example'\n",
    "\n",
    "        >>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n",
    "        >>> ' '.join(text_striped)\n",
    "        'This is example'\n",
    "\n",
    "        >>> text_restored = punc.restore(text_striped, punc_map)\n",
    "        >>> text_restored[0]\n",
    "        'This is. example !'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, puncs: str = _DEF_PUNCS):\n",
    "        self.puncs = puncs\n",
    "\n",
    "    @staticmethod\n",
    "    def default_puncs():\n",
    "        \"\"\"Return default set of punctuations.\"\"\"\n",
    "        return _DEF_PUNCS\n",
    "\n",
    "    @property\n",
    "    def puncs(self):\n",
    "        return self._puncs\n",
    "\n",
    "    @puncs.setter\n",
    "    def puncs(self, value):\n",
    "        if not isinstance(value, six.string_types):\n",
    "            raise ValueError(\"[!] Punctuations must be of type str.\")\n",
    "        self._puncs = \"\".join(list(dict.fromkeys(list(value))))  # remove duplicates without changing the oreder\n",
    "        self.puncs_regular_exp = re.compile(rf\"(\\s*[{re.escape(self._puncs)}]+\\s*)+\")\n",
    "\n",
    "    def strip(self, text):\n",
    "        \"\"\"Remove all the punctuations by replacing with `space`.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be processed.\n",
    "\n",
    "        Example::\n",
    "\n",
    "            \"This is. example !\" -> \"This is example \"\n",
    "        \"\"\"\n",
    "        return re.sub(self.puncs_regular_exp, \" \", text).rstrip().lstrip()\n",
    "\n",
    "    def strip_to_restore(self, text):\n",
    "        \"\"\"Remove punctuations from text to restore them later.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be processed.\n",
    "\n",
    "        Examples ::\n",
    "\n",
    "            \"This is. example !\" -> [[\"This is\", \"example\"], [\".\", \"!\"]]\n",
    "\n",
    "        \"\"\"\n",
    "        text, puncs = self._strip_to_restore(text)\n",
    "        return text, puncs\n",
    "\n",
    "    def _strip_to_restore(self, text):\n",
    "        \"\"\"Auxiliary method for Punctuation.preserve()\"\"\"\n",
    "        matches = list(re.finditer(self.puncs_regular_exp, text))\n",
    "        if not matches:\n",
    "            return [text], []\n",
    "        # the text is only punctuations\n",
    "        if len(matches) == 1 and matches[0].group() == text:\n",
    "            return [], [_PUNC_IDX(text, PuncPosition.ALONE)]\n",
    "        # build a punctuation map to be used later to restore punctuations\n",
    "        puncs = []\n",
    "        for match in matches:\n",
    "            position = PuncPosition.MIDDLE\n",
    "            if match == matches[0] and text.startswith(match.group()):\n",
    "                position = PuncPosition.BEGIN\n",
    "            elif match == matches[-1] and text.endswith(match.group()):\n",
    "                position = PuncPosition.END\n",
    "            puncs.append(_PUNC_IDX(match.group(), position))\n",
    "        # convert str text to a List[str], each item is separated by a punctuation\n",
    "        splitted_text = []\n",
    "        for idx, punc in enumerate(puncs):\n",
    "            split = text.split(punc.punc)\n",
    "            prefix, suffix = split[0], punc.punc.join(split[1:])\n",
    "            splitted_text.append(prefix)\n",
    "            # if the text does not end with a punctuation, add it to the last item\n",
    "            if idx == len(puncs) - 1 and len(suffix) > 0:\n",
    "                splitted_text.append(suffix)\n",
    "            text = suffix\n",
    "        return splitted_text, puncs\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, text, puncs):\n",
    "        \"\"\"Restore punctuation in a text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be processed.\n",
    "            puncs (List[str]): The list of punctuations map to be used for restoring.\n",
    "\n",
    "        Examples ::\n",
    "\n",
    "            ['This is', 'example'], ['.', '!'] -> \"This is. example!\"\n",
    "\n",
    "        \"\"\"\n",
    "        return cls._restore(text, puncs, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def _restore(cls, text, puncs, num):  # pylint: disable=too-many-return-statements\n",
    "        \"\"\"Auxiliary method for Punctuation.restore()\"\"\"\n",
    "        if not puncs:\n",
    "            return text\n",
    "\n",
    "        # nothing have been phonemized, returns the puncs alone\n",
    "        if not text:\n",
    "            return [\"\".join(m.mark for m in puncs)]\n",
    "\n",
    "        current = puncs[0]\n",
    "\n",
    "        if current.position == PuncPosition.BEGIN:\n",
    "            return cls._restore([current.punc + text[0]] + text[1:], puncs[1:], num)\n",
    "\n",
    "        if current.position == PuncPosition.END:\n",
    "            return [text[0] + current.punc] + cls._restore(text[1:], puncs[1:], num + 1)\n",
    "\n",
    "        if current.position == PuncPosition.ALONE:\n",
    "            return [current.mark] + cls._restore(text, puncs[1:], num + 1)\n",
    "\n",
    "        # POSITION == MIDDLE\n",
    "        if len(text) == 1:  # pragma: nocover\n",
    "            # a corner case where the final part of an intermediate\n",
    "            # mark (I) has not been phonemized\n",
    "            return cls._restore([text[0] + current.punc], puncs[1:], num)\n",
    "\n",
    "        return cls._restore([text[0] + current.punc + text[1]] + text[2:], puncs[1:], num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is This is example\n",
      "['This is', 'This is', 'example']  ----  [_punc_index(punc='. ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc=', ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc='!', position=<PuncPosition.END: 1>)]\n",
      "['This is. This is, example!']\n"
     ]
    }
   ],
   "source": [
    "punc = Punctuation()\n",
    "text = \"This is. This is, example!\"\n",
    "print(punc.strip(text))\n",
    "split_text, puncs = punc.strip_to_restore(text)\n",
    "print(split_text, \" ---- \", puncs)\n",
    "restored_text = punc.restore(split_text, puncs)\n",
    "print(restored_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimrod",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
