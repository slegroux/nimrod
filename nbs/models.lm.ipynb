{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import lightning as L\n",
    "from lightning import Trainer, LightningModule\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# plt.set_loglevel('INFO')\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pprint\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from typing import List\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from nimrod.text.datasets import CharDataset, Vocab\n",
    "from nimrod.utils import set_seed, get_device\n",
    "from nimrod.models.core import Classifier\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Nimrod Language Models (LM) module provides a comprehensive framework for developing, training, and deploying advanced natural language processing models, with a focus on flexibility, performance, and cutting-edge research.\n",
    "\n",
    "## Key Features\n",
    "- üî§ Advanced Language Model Architectures\n",
    "- üß† Transformer-based Models\n",
    "- üöÄ High-Performance NLP Utilities\n",
    "- üîß Configurable Model Components\n",
    "- üåê Multi-Language Support\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### Language Model Architectures\n",
    "- Transformer-based Models\n",
    "- Sequence-to-Sequence Models\n",
    "- Encoder-Decoder Architectures\n",
    "- Causal Language Models\n",
    "- Masked Language Models\n",
    "\n",
    "### Key Capabilities\n",
    "- Tokenization\n",
    "- Embedding Strategies\n",
    "- Attention Mechanisms\n",
    "- Transfer Learning\n",
    "- Few-Shot Learning\n",
    "- Prompt Engineering\n",
    "\n",
    "## Supported Model Types\n",
    "- BERT-like Models\n",
    "- GPT-style Architectures\n",
    "- T5 Variants\n",
    "- BART\n",
    "- RoBERTa\n",
    "- XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_EPOCHS for training debuggging\n",
    "ITER_MAX = 1\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading with pandas\n",
    "df = pd.read_csv('../data/text/names.txt', header=None, names=['name'])\n",
    "data = list(df.name)\n",
    "print(\"names: \", data[:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting\n",
    "given last n tokens we predict token n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = list(\"alexandra\")\n",
    "print(s)\n",
    "bigram = [(x,y) for x, y in zip(s, s[1:])]\n",
    "print(bigram)\n",
    "trigram = [ (x,y,z) for x, y, z in zip(s, s[1:], s[2:])]\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny shakespeare LM char dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading directly in plain python\n",
    "lines = []\n",
    "with open('../data/text/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip():\n",
    "            # only append non blank lines\n",
    "            lines.append(line)\n",
    "\n",
    "# add sentence tokens\n",
    "# data = [['<bos>'] +list(line.strip()) + ['<eos>'] for line in lines]\n",
    "# data = [list(line.strip()) for line in lines]\n",
    "data = [list(line) for line in lines]\n",
    "print(\"data: \", data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset(\n",
    "        words:List[str], # data is a list of sentences which are a list of words\n",
    "        v:Vocab,# vocabulary class for mapping words to indices\n",
    "        verbose:bool=False, # print debug info\n",
    "        context_length=3 # number of words/tokens to use as context\n",
    "        ):\n",
    "    X = []\n",
    "    y = []\n",
    "    for word in words:\n",
    "        s = list(word)\n",
    "        if verbose:\n",
    "            print('row: ', s)\n",
    "        # init prefix with padding while len < context_length\n",
    "        for i in range(context_length-1):\n",
    "            sequence = v.stoi(s[:i+1])\n",
    "            pad_len = context_length - len(sequence)\n",
    "            pad = [v.stoi(\"<pad>\")] * pad_len\n",
    "            X.append(pad + sequence)\n",
    "            y.append(v.stoi(s[i+1]))\n",
    "\n",
    "            if verbose:\n",
    "                print([\"<pad>\"]+ s[:i+1], s[i+1])\n",
    "\n",
    "        # for length seq = context_length\n",
    "        i = 0\n",
    "        while i < (len(s) - context_length):\n",
    "            X.append(v.stoi(s[i:context_length+i]))\n",
    "            y.append(v.stoi(s[i+context_length]))\n",
    "            if verbose:\n",
    "                print(s[i:context_length+i], s[i+context_length])\n",
    "            i += 1\n",
    "    return torch.tensor(X),torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each row in the dataset we expand all the combinations of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['<unk>','<pad>'])\n",
    "print(\"vocabulary: \", v.vocabulary)\n",
    "print(\"vocabulary size: \", len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_LEN = 3\n",
    "X, y = make_dataset(data[:80], v, verbose=True, context_length=CONTEXT_LEN)\n",
    "print(\"X: \", X.shape, \"y:\", y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP LM Model\n",
    "https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F.one_hot(torch.tensor(5), num_classes=n_vocab).float()@C # == C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class NNLMConfig:\n",
    "    n_vocab:int = 30\n",
    "    n_emb:int = 10\n",
    "    n_context:int = 3\n",
    "    n_h:int = 100\n",
    "\n",
    "class NNLM(nn.Module):\n",
    "    def __init__(self,\n",
    "                n_vocab:int = 30, # vocabulary size \n",
    "                n_emb:int = 10, # embedding dimension\n",
    "                n_context:int = 3, # context size bigram/trigram, etc.\n",
    "                n_h:int = 100 # hidden layer size\n",
    "                ):\n",
    "\n",
    "        logger.info(f\"NNLM: Init\")\n",
    "        super().__init__()\n",
    "        # to each token id from n_vocab in sequence T coresponds a embedding of size n_emb (C)\n",
    "        self.embedder = nn.Embedding(n_vocab, n_emb) # (B,T)->(B,T,C)\n",
    "        self.n_emb = n_emb\n",
    "        self.n_context = n_context\n",
    "        # we concatenate input of [n_context length, n_emb] into linear layer (T*C):\n",
    "        self.l1 = nn.Linear(n_context * n_emb, n_h) \n",
    "        self.l2 = nn.Linear(n_h, n_vocab)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        # input: (B,T)\n",
    "        embedding = self.embedder(x) # ->(B,T,C)\n",
    "        # we concatenate input of n_context length * n_emb (T*C) into linear layer:\n",
    "        h = self.l1(embedding.view(-1,self.n_context * self.n_emb))\n",
    "        h = torch.tanh(h)\n",
    "        logits = self.l2(h)\n",
    "        return(logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prompt:str, vocab:Vocab, max_new_tokens:int=50, temperature:float=1.0):\n",
    "\n",
    "        for _ in range(max_new_tokens):            \n",
    "            # limit prompt to context size\n",
    "            context = prompt[-self.n_context:]\n",
    "            context = vocab.stoi(list(context))\n",
    "\n",
    "            logits = self(torch.tensor(context))\n",
    "            logits = logits / temperature\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1).item()\n",
    "            prompt += vocab.itos(ix)\n",
    "        return(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config model\n",
    "conf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\n",
    "lm = NNLM(**asdict(conf))\n",
    "\n",
    "# test data\n",
    "bs = 25\n",
    "x = torch.randint(conf.n_vocab, (bs, conf.n_context)) # (B, T) with values between 0 and n_vocab\n",
    "print(\"X (B, T):\", x.shape)\n",
    "\n",
    "# prediction\n",
    "y = lm(x)\n",
    "print(\"Y_hat logits (B, n_vocab):\", y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handmade dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr = make_dataset(data[:80], v, context_length=CONTEXT_LEN)\n",
    "Xdev, Ydev = make_dataset(data[80:90], v)\n",
    "Xte, Yte = make_dataset(data[90:100], v)\n",
    "print(\"Xtr (B, T): \", Xtr.shape, \"Ytr (B): \", Ytr.shape, \"data:\", len(data[:80]))\n",
    "print(\"len Xtr: \", len(Xtr))\n",
    "print(\"CONTEXT_LEN: \", CONTEXT_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfit on subset of 80 first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "%time\n",
    "device = get_device()\n",
    "device = 'cpu'\n",
    "# lm.to(device)\n",
    "\n",
    "# overfit on one big batch\n",
    "optim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\n",
    "train_loss = []\n",
    "ITER_MAX = 1000\n",
    "for i in tqdm(range(ITER_MAX)):\n",
    "    # for batch in dm.train_dataloader():\n",
    "        # Xtr, Ytr = batch\n",
    "        # Ytr = Ytr[:, -1]\n",
    "        Xtr = Xtr.to(device)\n",
    "        Ytr = Ytr.to(device)\n",
    "        optim.zero_grad()\n",
    "        logits = lm(Xtr)\n",
    "        loss = F.cross_entropy(logits, Ytr)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss.append(loss.item())\n",
    "        if not(i%250):\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on CPU\n",
    "lm.to('cpu')\n",
    "prompt = \"The country of \"\n",
    "sequences = lm.sample(prompt, v, max_new_tokens=250, temperature=0.6)\n",
    "print(sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\n",
    "# use <unk> and <pad> to be consistent with manual data preprocessing and have smae vocabulary size\n",
    "v = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['<unk>','<pad>'])\n",
    "print(\"vocabulary: \", v.vocabulary)\n",
    "print(\"vocabulary size: \", len(v))\n",
    "print(cfg)\n",
    "cfg.train_val_test_split = [0.8, 0.1, 0.1]\n",
    "# by default data_path is relative to the recipe folder so need to update for nbs\n",
    "cfg.data_path = \"../data/text/tiny_shakespeare.txt\"\n",
    "cfg.context_size = CONTEXT_LEN\n",
    "cfg.batch_size = 2700 # large batch to mimic manual data order of magnitude\n",
    "cfg.random_split = False\n",
    "cfg.specials=['<unk>', '<pad>']\n",
    "cfg.add_sentence_tokens = False\n",
    "print(cfg)\n",
    "dm = instantiate(cfg)\n",
    "dm.setup()\n",
    "print(\"vocab size: \", dm.vocab_size)\n",
    "# setup large batch to overfit / test model\n",
    "Xtr, Ytr= next(iter(dm.train_dataloader()))\n",
    "# target is last token in sequence\n",
    "Ytr = Ytr[:, -1]\n",
    "print(\"Xtr (B, T): \", Xtr.shape, \"Ytr (B): \", Ytr.shape)\n",
    "X, Y = dm.data_train[0]\n",
    "print(dm.ds.from_tokens(X), dm.ds.from_tokens(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\n",
    "print(len(v), CONTEXT_LEN)\n",
    "lm = NNLM(**asdict(conf))\n",
    "bs = 10\n",
    "x = torch.randint(conf.n_vocab, (bs, conf.n_context)) # (B, T) with values between 0 and n_vocab\n",
    "print(\"X (B, T):\", x.shape)\n",
    "lm(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking data tokens are between 0 and vocab size\n",
    "print(Xtr.min(),  Xtr.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overfit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "%time\n",
    "# device = get_device()\n",
    "device = 'cpu'\n",
    "lm.to(device)\n",
    "\n",
    "# overfit on one big batch\n",
    "optim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\n",
    "train_loss = []\n",
    "ITER_MAX = 1000\n",
    "for i in tqdm(range(ITER_MAX)):\n",
    "    # for batch in dm.train_dataloader():\n",
    "        # Xtr, Ytr = batch\n",
    "        # Ytr = Ytr[:, -1]\n",
    "        Xtr = Xtr.to(device)\n",
    "        Ytr = Ytr.to(device)\n",
    "        optim.zero_grad()\n",
    "        logits = lm(Xtr)\n",
    "        loss = F.cross_entropy(logits, Ytr)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss.append(loss.item())\n",
    "        if not(i%250):\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on CPU\n",
    "lm.to('cpu')\n",
    "prompt = \"The country of \"\n",
    "sequences = lm.sample(prompt, v, max_new_tokens=250, temperature=0.6)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching with dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batch gradient descent with datamodule\n",
    "cfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\n",
    "cfg.train_val_test_split = [0.8, 0.1, 0.1]\n",
    "cfg.data_path = \"../data/text/tiny_shakespeare.txt\"\n",
    "cfg.context_size = CONTEXT_LEN\n",
    "cfg.batch_size = 2048\n",
    "cfg.random_split = False\n",
    "cfg.specials=['<unk>', '<pad>']\n",
    "cfg.add_sentence_tokens = False\n",
    "dm = instantiate(cfg)\n",
    "dm.setup()\n",
    "\n",
    "conf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\n",
    "lm = NNLM(**asdict(conf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "%time\n",
    "\n",
    "optim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\n",
    "train_loss = []\n",
    "# device = get_device()\n",
    "device = 'cpu'\n",
    "lm.to(device)\n",
    "i = 0\n",
    "EPOCHS = 1\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    for batch in tqdm(dm.train_dataloader()):\n",
    "        Xtr, Ytr = batch\n",
    "        # target is last token in sequence\n",
    "        Ytr = Ytr[:, -1] # BxT\n",
    "        Xtr = Xtr.to(device)\n",
    "        Ytr = Ytr.to(device)\n",
    "        \n",
    "        logits = lm(Xtr)\n",
    "        loss = F.cross_entropy(logits, Ytr)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "        if not(i%1000):\n",
    "            print(loss.item())\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on CPU\n",
    "lm.to('cpu')\n",
    "prompt = \"The country of \"\n",
    "sequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP LM X Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class NNLM_X(Classifier, LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet: NNLM,\n",
    "            num_classes:int,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            scheduler: torch.optim.lr_scheduler,\n",
    "            ):\n",
    "\n",
    "        logger.info(\"NNLM_X: Init\")\n",
    "        super().__init__(\n",
    "            num_classes,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            )\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        # required attribute for lr finder\n",
    "        self.lr = optimizer.keywords['lr']\n",
    "        self.nnet = nnet\n",
    "    \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = y[:, -1]\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y = y[:, -1]  # Get the last token as target\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)\n",
    "    \n",
    "    def sample(self, prompt:str, vocab:Vocab, max_new_tokens:int=50, temperature:float=1.0):\n",
    "        return self.nnet.sample(prompt, vocab, max_new_tokens, temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omegaconf\n",
    "cfg = OmegaConf.load(\"../config/text/model/nnlm.yaml\")\n",
    "cfg.num_classes = len(v)\n",
    "print(len(v))\n",
    "# have to convert omegaconf dict to dict for pprint\n",
    "opt = instantiate(cfg.optimizer)\n",
    "print(opt.keywords['lr'])\n",
    "\n",
    "pprint.pprint(dict(cfg))\n",
    "lm  = instantiate(cfg)\n",
    "print(lm.hparams.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 25\n",
    "x = torch.randint(conf.n_vocab, (n_samples, cfg.nnet.n_context))\n",
    "print(\"X:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lm(x)\n",
    "print(\"Y_hat logits:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['<unk>','<pad>'])\n",
    "lm.sample(\"The country of \", v, max_new_tokens=500, temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab\n",
    "print(len(v))\n",
    "\n",
    "# data\n",
    "cfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\n",
    "cfg.context_size = CONTEXT_LEN\n",
    "cfg.specials: [\"<pad>\", \"<unk>\"]\n",
    "cfg.batch_size = 2048\n",
    "cfg.random_split = False\n",
    "dm = instantiate(cfg)\n",
    "dm.setup()\n",
    "\n",
    "# model\n",
    "cfg = OmegaConf.load(\"../config/text/model/nnlm.yaml\")\n",
    "lm  = instantiate(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model can be easily trained with L trainer (c.f. recipes/text/ for examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=1,\n",
    "    logger=CSVLogger(\"logs\", name=\"nnlm\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "trainer.fit(lm, dm.train_dataloader(), dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "csv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\n",
    "metrics = pd.read_csv(csv_path)\n",
    "metrics.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "plt.plot(metrics['step'], metrics['train/loss_step'],'b.-')\n",
    "plt.plot(metrics['step'], metrics['val/loss'],'r.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "trainer.test(lm, dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on CPU\n",
    "lm.to('cpu')\n",
    "prompt = \"The country of \"\n",
    "sequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=1,\n",
    ")\n",
    "tuner = Tuner(trainer)\n",
    "lr_finder = tuner.lr_find(\n",
    "    lm,\n",
    "    datamodule=dm,\n",
    "    min_lr=1e-6,\n",
    "    max_lr=1.0,\n",
    "    num_training=100,  # number of iterations\n",
    "    # attr_name=\"optimizer.lr\",\n",
    ")\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "plt.show()\n",
    "print(f\"Suggested learning rate: {lr_finder.suggestion()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-train with new lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "new_lr = lr_finder.suggestion()\n",
    "lm.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=1,\n",
    "    logger=CSVLogger(\"logs\", name=\"nnlm\"),\n",
    ")\n",
    "trainer.fit(lm, dm.train_dataloader(), dm.val_dataloader())\n",
    "trainer.test(lm, dm.test_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "csv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\n",
    "metrics = pd.read_csv(csv_path)\n",
    "plt.plot(metrics['step'], metrics['train/loss_step'],'.-')\n",
    "# plt.figure()\n",
    "# plot_classifier_metrics_from_csv(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer on CPU\n",
    "lm.to('cpu')\n",
    "prompt = \"The country of \"\n",
    "sequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class NNBigram(nn.Module):\n",
    "    def __init__(self, vocab_size:int) -> None:\n",
    "        super().__init__()\n",
    "        self._vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, x:torch.tensor) -> torch.tensor:\n",
    "        logits = self.emb(x) # B,T,C\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self(idx)\n",
    "            logits = logits[:,-1,:] # last time step\n",
    "            probs = F.softmax(logits, dim=-1) #(B,C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self)->int:\n",
    "        return self._vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B, T, C = 32, 8, 65\n",
    "vocab_size = C\n",
    "model = NNBigram(vocab_size)\n",
    "print(\"vocab size: \",  model.vocab_size)\n",
    "X = torch.randint(0,C,(B,T))\n",
    "Y = torch.randint(0,C,(B,T))\n",
    "batch = (X,Y)\n",
    "logits = model(X) # (B, T, C)\n",
    "print(\"X: \", X.shape, \"Y: \", Y.shape, \"logits: \", logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "model.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #| export\n",
    "# class NNBigramL(ModelModule):\n",
    "#     def __init__(self, vocab_size:int, lr:float=1e-3):\n",
    "#         model = NNBigram(vocab_size)\n",
    "#         super().__init__(model, lr)\n",
    "#         self.accuracy = Accuracy(task='multiclass', num_classes=model.vocab_size)\n",
    "\n",
    "#     def _step(self, batch:torch.tensor, batch_idx:int):\n",
    "#         x, y = batch\n",
    "#         logits = self.model(x) # (B,T,C)\n",
    "#         B, T, C = logits.shape\n",
    "#         logits = logits.view(B*T, C)\n",
    "#         y = y.view(B*T)\n",
    "#         loss = self.loss(logits, y)\n",
    "#         acc = self.accuracy(logits, y)\n",
    "#         return loss, acc\n",
    "    \n",
    "#     def predict(self,idx:torch.IntTensor, max_new_tokens:int):\n",
    "#         return self.model.predict(idx, max_new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pl = NNBigramL(vocab_size)\n",
    "# logits = model_pl(X) # (B, T, C)\n",
    "# print(logits.shape)\n",
    "# model_pl.training_step(batch, 0)\n",
    "# model_pl._step(batch, 0)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/text/tiny_shakespeare.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "block_size = 8\n",
    "ds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size)\n",
    "X,Y = ds[0]\n",
    "print(\"x:\",  ds.from_tokens(X), \"\\ny:\", ds.from_tokens(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "dl = DataLoader(ds, batch_size=32, num_workers=0)\n",
    "X, Y = next(iter(dl))\n",
    "print(\"x:\", X.shape, \"\\ny:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNBigram(ds.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "%time\n",
    "ITER_MAX = 1000\n",
    "train_loss = []\n",
    "for epoch in tqdm(range(ITER_MAX)):\n",
    "    model.train()\n",
    "    X = X.to(device) # (B,T)\n",
    "    Y = Y.to(device) # (B,T)\n",
    "    logits = model(X)\n",
    "    B, T, C = logits.shape\n",
    "    loss = criterion(logits.view(B*T, C), Y.view(B*T))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "    if not(epoch % 1000):\n",
    "        print(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        logits = model(X).view(B*T,C) \n",
    "        # _, predicted = torch.max(logits.data, 1)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # print(\"probs: \", probs.shape)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        # print(\"pred:\", preds.shape)\n",
    "        # print(\"Y:\", Y.shape)\n",
    "        # print(predicted)\n",
    "        # total += Y.size(0)\n",
    "        # correct += (predicted == Y).sum()\n",
    "        # print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.from_tokens(model.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training from module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# n_epochs = 1\n",
    "# train_loss = []\n",
    "# for epoch in range(n_epochs):\n",
    "#     model_pl.model.train()\n",
    "#     loss = model_pl.training_step(batch, None)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     train_loss.append(loss.item())\n",
    "#     if not(epoch % 100):\n",
    "#         print(loss.item())\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ds.from_tokens(model_pl.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
