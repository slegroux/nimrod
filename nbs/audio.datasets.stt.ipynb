{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech to Text Datasets\n",
    "\n",
    "> Speech to text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp audio.datasets.stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from lightning import LightningDataModule, LightningModule\n",
    "\n",
    "\n",
    "from lhotse.dataset import BucketingSampler, OnTheFlyFeatures\n",
    "from lhotse.dataset.collation import TokenCollater\n",
    "from lhotse.recipes import download_librispeech, prepare_librispeech\n",
    "from lhotse.dataset.vis import plot_batch\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, Fbank, FbankConfig\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List\n",
    "\n",
    "from nimrod.text.tokenizers import CharTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lhotse-Based Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class STTDataset(Dataset):\n",
    "    def __init__(self,\n",
    "        tokenizer:TokenCollater, # text tokenizer\n",
    "        num_mel_bins:int=80 # number of mel spectrogram bins\n",
    "        ):\n",
    "        self.extractor = OnTheFlyFeatures(Fbank(FbankConfig(num_mel_bins=num_mel_bins)))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, cuts: CutSet) -> dict:\n",
    "        cuts = cuts.sort_by_duration()\n",
    "        feats, feat_lens = self.extractor(cuts)\n",
    "        tokens, token_lens = self.tokenizer(cuts)\n",
    "        return {\"feats_pad\": feats, \"ilens\": feat_lens, \"tokens_pad\": tokens}\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LibriSpeech DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LibriSpeechDataModule(LightningDataModule):\n",
    "    def __init__(self,\n",
    "        target_dir=\"../data/en\", # where data will be saved / retrieved\n",
    "        dataset_parts=\"mini_librispeech\", # either full librispeech or mini subset\n",
    "        output_dir=\"../recipes/stt/librispeech/data\", # where to save manifest\n",
    "        num_jobs=1 # num_jobs depending on number of cpus available\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.libri = {}\n",
    "\n",
    "    def prepare_data(self,) -> None:\n",
    "        download_librispeech(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.libri = prepare_librispeech(Path(self.hparams.target_dir) / \"LibriSpeech\", dataset_parts=self.hparams.dataset_parts, output_dir=self.hparams.output_dir, num_jobs=self.hparams.num_jobs)\n",
    "        if stage == \"fit\" or stage == None:\n",
    "            self.cuts_train = CutSet.from_manifests(**self.libri[\"train-clean-5\"])\n",
    "            self.cuts_test = CutSet.from_manifests(**self.libri[\"dev-clean-2\"])\n",
    "            self.tokenizer = TokenCollater(self.cuts_train)\n",
    "            self.tokenizer(self.cuts_test.subset(first=2))\n",
    "            self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "        if stage == \"test\":\n",
    "            self.cuts_test = CutSet.from_manifests(**self.libri[\"dev-clean-2\"])\n",
    "            self.tokenizer = TokenCollater(self.cuts_test)\n",
    "            self.tokenizer(self.cuts_test.subset(first=2))\n",
    "            self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True) #, bucket_method=\"equal_duration\")\n",
    "        return DataLoader(STTDataset(self.tokenizer), sampler=train_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False) #, bucket_method=\"equal_duration\")\n",
    "        return DataLoader(STTDataset(self.tokenizer), sampler=test_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {\n",
    "            \"odim\": len(self.tokenizer.idx2token),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LibriSpeechDataModule(\n",
    "    target_dir=\"../data/en\", \n",
    "    dataset_parts=\"mini_librispeech\",\n",
    "    output_dir=\"../data/en/LibriSpeech\",\n",
    "    num_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f53cb16b4045678c150fb034e4da17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading LibriSpeech parts:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# skip this at export time to not waste time\n",
    "# download\n",
    "dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libri = prepare_librispeech(\"../data/en/LibriSpeech\", dataset_parts='mini_librispeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "#! rm ../data/en/LibriSpeech/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# dm.setup(stage='test')\n",
    "# dm.cuts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089 68\n"
     ]
    }
   ],
   "source": [
    "recs = RecordingSet.from_file(\"../data/en/LibriSpeech/librispeech_recordings_dev-clean-2.jsonl.gz\")\n",
    "sup = SupervisionSet(\"../data/en/LibriSpeech/librispeech_supervisions_dev-clean-2.jsonl.gz\")\n",
    "print(len(recs),len(sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# test_dl = dm.test_dataloader()\n",
    "# b = next(iter(test_dl))\n",
    "# print(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\n",
    "# plt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n",
    "\n",
    "# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n",
    "# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# print(dm.cuts_test)\n",
    "# cut = dm.cuts_test[0]\n",
    "# # pprint(cut.to_dict())\n",
    "# cut.plot_audio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniLibriSpeech Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniLibriSpeechDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root:str,\n",
    "        subset:str,\n",
    "        audio_transform:torchaudio.transforms = None,\n",
    "        text_transform = None\n",
    "        ):\n",
    "\n",
    "        self.data = [] # audiopath, text\n",
    "        \n",
    "        self.audio_transform = audio_transform\n",
    "        self.text_transform = text_transform\n",
    "\n",
    "        path = Path(root) / subset\n",
    "        for speaker_dir in path.iterdir():\n",
    "            if not speaker_dir.is_dir():\n",
    "                continue\n",
    "            for chapter_dir in speaker_dir.iterdir():\n",
    "                if not chapter_dir.is_dir():\n",
    "                    continue\n",
    "                trn_path = chapter_dir / f\"{speaker_dir.name}-{chapter_dir.name}.trans.txt\"\n",
    "                if not trn_path.exists():\n",
    "                    continue\n",
    "                with open(trn_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "\n",
    "                with open(trn_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                transcripts = {\n",
    "                    line.split()[0]: ' '.join(line.strip().split()[1:])\n",
    "                    for line in lines\n",
    "                }\n",
    "\n",
    "                for utt_id, text in transcripts.items():\n",
    "                    audio_path = chapter_dir / f\"{utt_id}.flac\"\n",
    "                    if audio_path.exists():\n",
    "                        self.data.append((audio_path, text))\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        audio_path, trn =  self.data[idx]\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if self.audio_transform:\n",
    "            feats = self.audio_transform(waveform)\n",
    "        else:\n",
    "            feats = waveform\n",
    "        \n",
    "        if self.text_transform:\n",
    "            label = self.text_transform(trn.lower())\n",
    "        else:\n",
    "            label = trn.lower()\n",
    "        \n",
    "        return feats.squeeze(0), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio & text transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089 torch.Size([186560]) if the reader will excuse me i will say nothing of my antecedents nor of the circumstances which led me to leave my native country the narrative would be tedious to him and painful to myself\n",
      "torch.Size([13, 933]) if the reader will excuse me i will say nothing of my antecedents nor of the circumstances which led me to leave my native country the narrative would be tedious to him and painful to myself\n"
     ]
    }
   ],
   "source": [
    "ds = MiniLibriSpeechDataset('../data/en/LibriSpeech', 'dev-clean-2')\n",
    "print(len(ds), ds[0][0].shape, ds[0][1])\n",
    "\n",
    "mfcc_transform = torchaudio.transforms.MFCC(sample_rate=16000, n_mfcc=13)\n",
    "ds = MiniLibriSpeechDataset(\n",
    "    \"../data/en/LibriSpeech\", \"dev-clean-2\",\n",
    "    audio_transform=mfcc_transform\n",
    ")\n",
    "print(ds[0][0].shape, ds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello!\n",
      "torch.Size([13, 933]) if the reader will excuse me i will say nothing of my antecedents nor of the circumstances which led me to leave my native country the narrative would be tedious to him and painful to myself\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "vocab = list(string.ascii_lowercase + string.digits) + [' ', '.', ',', '?', '!', '-', \"'\"]\n",
    "tok = CharTokenizer(vocab)\n",
    "encoded = tok.encode('hello!')\n",
    "decoded = tok.decode(encoded)\n",
    "print(decoded)\n",
    "\n",
    "ds = MiniLibriSpeechDataset(\n",
    "    \"../data/en/LibriSpeech\", \"dev-clean-2\",\n",
    "    audio_transform=mfcc_transform,\n",
    "    text_transform=tok.encode\n",
    ")\n",
    "# print(ds[0][0].shape, ds[0][1])\n",
    "print(ds[0][0].shape, tok.decode(ds[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
