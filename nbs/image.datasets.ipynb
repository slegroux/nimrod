{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Datasets\n",
    "\n",
    "> Image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp image.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from lightning import LightningDataModule\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "from nimrod.data.core import DataModule\n",
    "from nimrod.utils import set_seed\n",
    "\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "set_seed(42)\n",
    "logger = logging.getLogger(__name__)\n",
    "plt.set_loglevel('INFO')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageDataset base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ImageDataset(Dataset):\n",
    "    \" Base class for image datasets providing visualization of (image, label) samples\"\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"ImageDataset: init\")\n",
    "        super().__init__()\n",
    "\n",
    "    def show_idx(self,\n",
    "            index:int # Index of the (image,label) sample to visualize\n",
    "        ):\n",
    "        \"display image from data point index of a image dataset\"\n",
    "        X, y = self.__getitem__(index)\n",
    "        plt.figure(figsize = (1, 1))\n",
    "        plt.imshow(X.numpy().reshape(28,28),cmap='gray')\n",
    "        plt.title(f\"Label: {int(y)}\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def show_grid(\n",
    "            imgs: List[torch.Tensor], # python list of images dim (C,H,W)\n",
    "            save_path=None, # path where image can be saved\n",
    "            dims:Tuple[int,int] = (28,28)\n",
    "        ):\n",
    "        \"display list of mnist-like images (C,H,W)\"\n",
    "        if not isinstance(imgs, list):\n",
    "            imgs = [imgs]\n",
    "        fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "        for i, img in enumerate(imgs):\n",
    "            img = img.detach()\n",
    "            axs[0, i].imshow(img.numpy().reshape(dims[0],dims[1]))\n",
    "            axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "\n",
    "    def show_random(\n",
    "            self,\n",
    "            n:int=3, # number of images to display\n",
    "            dims:Tuple[int,int] = (28,28)\n",
    "        ):\n",
    "        \"display grid of random images\"\n",
    "        indices = torch.randint(0,len(self), (n,))\n",
    "        images = []\n",
    "        for index in indices:\n",
    "            X, y = self.__getitem__(index)\n",
    "            X = X.reshape(dims[0],dims[1])\n",
    "            images.append(X)\n",
    "        self.show_grid(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ImageDataset.show_idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class MNISTDataset(ImageDataset):\n",
    "    \"MNIST digit dataset\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir:str='../data/image', # path where data is saved\n",
    "        train = True, # train or test dataset\n",
    "        transform:torchvision.transforms.transforms=transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.1307,), (0.3081,))\n",
    "            ])\n",
    "        # TODO: add noramlization?\n",
    "        # torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize(0.1307,), (0.3081,))])\n",
    "\n",
    "    ):\n",
    "        os.makedirs(data_dir, exist_ok=True)\n",
    "        super().__init__()\n",
    "        logger.info(\"MNISTDataset: init\")\n",
    "\n",
    "        self.ds = MNIST(\n",
    "            data_dir,\n",
    "            train = train,\n",
    "            transform=transform, \n",
    "            download=True\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int: # length of dataset\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx # index into the dataset\n",
    "                    ) -> tuple[torch.FloatTensor, int]: # Y image data, x digit number\n",
    "        x = self.ds[idx][0]\n",
    "        y = self.ds[idx][1]\n",
    "        return x, y\n",
    "    \n",
    "    def train_dev_split(\n",
    "            self,\n",
    "            ratio:float, # percentage of train/dev split,\n",
    "        ) -> tuple[torchvision.datasets.MNIST, torchvision.datasets.MNIST]: # train and set mnnist datasets\n",
    "\n",
    "        train_set_size = int(len(self.ds) * ratio)\n",
    "        valid_set_size = len(self.ds) - train_set_size\n",
    "\n",
    "        # split the train set into two\n",
    "        train_set, valid_set = data.random_split(self.ds, [train_set_size, valid_set_size])\n",
    "        # TODO: cast to ImageDataset to allow for drawing\n",
    "        # train_set, valid_set = Dataset(train_set),j Dataset(valid_set)\n",
    "        return train_set, valid_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MNISTDataset.train_dev_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "Setup MNIST dataset. Download data if not found in specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test set (train=False)\n",
    "test = MNISTDataset('../data/image', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# output ( (C,H,W), int)\n",
    "print(test.ds, test.ds[0][0].dtype, type(test.ds[0][1]))\n",
    "print(f\"Number of samples in the dataset: {len(test)}\")\n",
    "\n",
    "# get item helper\n",
    "X, y = test[0]\n",
    "print(X.shape, y, X.type(), type(y))\n",
    "\n",
    "# display each digit\n",
    "test.show_idx(0)\n",
    "\n",
    "# split data\n",
    "train, dev = test.train_dev_split(0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate from config file\n",
    "It is convenient to keep setup of specific dataset for an experiment in a config file for reproductibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset from yaml config file\n",
    "cfg = OmegaConf.load(\"../config/image/data/mnist.yaml\")\n",
    "print(cfg.dataset)\n",
    "test = instantiate(cfg.dataset)\n",
    "type(test)\n",
    "\n",
    "# output ( (B,C, H,W), int)\n",
    "print(test.ds, test.ds[0][0].dtype, type(test.ds[0][1]))\n",
    "print(f\"Number of samples in the dataset: {len(test)}\")\n",
    "\n",
    "# get item helper\n",
    "X, y = test[0]\n",
    "print(X.shape, y, X.type(), type(y))\n",
    "\n",
    "# display each digit\n",
    "test.show_idx(0)\n",
    "\n",
    "# split data\n",
    "train, dev = test.train_dev_split(0.8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MNISTDataModule(DataModule, LightningDataModule):\n",
    "    def __init__(self,\n",
    "                 data_dir: str | os.PathLike = \"~/Data/\", # path to source data dir\n",
    "                 train_val_test_split:List[float] = [0.8, 0.1, 0.1], # train val test %\n",
    "                 batch_size: int = 64, # size of compute batch\n",
    "                 num_workers: int = 0, # num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow\n",
    "                 pin_memory: bool = False, # If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer\n",
    "                 persistent_workers: bool = False\n",
    "                 ):\n",
    "\n",
    "        logger.info(\"Init MNIST DataModule\")\n",
    "        super().__init__(train_val_test_split, batch_size, num_workers, pin_memory, persistent_workers)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int: # num of classes in dataset\n",
    "        return 10\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Download data if needed + format with MNISTDataset\n",
    "        \"\"\"\n",
    "        # train set\n",
    "        MNISTDataset(self.hparams.data_dir, train=True)\n",
    "        # test set\n",
    "        MNISTDataset(self.hparams.data_dir, train=False)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # stage: {fit,validate,test,predict}\\n\",\n",
    "        # concat train & test mnist dataset and randomly generate train, eval, test sets\n",
    "        if not self.data_train or not self.data_val or not self.data_test:\n",
    "            # ((B, H, W), int)\n",
    "            trainset = MNISTDataset(self.hparams.data_dir, train=True, transform=self.transforms)\n",
    "            testset = MNISTDataset(self.hparams.data_dir, train=False, transform=self.transforms)\n",
    "            dataset = ConcatDataset(datasets=[trainset, testset])\n",
    "            # TODO: keep test set untouched\n",
    "            lengths = [int(split * len(dataset)) for split in self.hparams.train_val_test_split]\n",
    "            self.data_train, self.data_val, self.data_test = random_split(dataset=dataset, lengths=lengths)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# check for abstract methods\n",
    "pprint([(name, getattr(method,\"__isabstractmethod__\", False)) for (name, method) in DataModule.__dict__.items()])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "dm = MNISTDataModule(\n",
    "    data_dir=\"../data/image\",\n",
    "    train_val_test_split=[0.8, 0.1, 0.1],\n",
    "    batch_size = 64,\n",
    "    num_workers = 0, # main process\n",
    "    pin_memory= False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "# download or reference data from dir\n",
    "dm.prepare_data()\n",
    "\n",
    "# define train, eval, test subsets\n",
    "dm.setup()\n",
    "\n",
    "# len of splits\n",
    "print(len(dm.data_train), len(dm.data_val), len(dm.data_test))\n",
    "\n",
    "# access data batches via dataloader\n",
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X dim(B,C,W,H): \", X.shape, \"Y: dim(B)\", Y.shape)\n",
    "\n",
    "# access data points directly by index\n",
    "print(len(dm.data_test[0]), print(dm.data_test[0][0].shape))\n",
    "imgs = [dm.data_test[i][0] for i in range(5)]\n",
    "\n",
    "# display image samples\n",
    "ImageDataset.show_grid(imgs)\n",
    "\n",
    "# labels are ints\n",
    "lbls = [dm.data_test[i][1] for i in range(5)]\n",
    "print(lbls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../config/image/data/mnist.yaml\")\n",
    "print(cfg.datamodule)\n",
    "dm = instantiate(cfg.datamodule)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "test_dl = dm.test_dataloader()\n",
    "len(dm.data_test[0])\n",
    "imgs = [dm.data_test[i][0] for i in range(5)]\n",
    "ImageDataset.show_grid(imgs)\n",
    "print(type(dm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
