{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLIP: Bootstrapping Language-Image Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://github.com/salesforce/BLIP\n",
    "\n",
    " * Copyright (c) 2022, salesforce.com, inc.\n",
    " * All rights reserved.\n",
    " * SPDX-License-Identifier: BSD-3-Clause\n",
    " * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n",
    " * By Junnan Li\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp image.blip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.image.vit import VisionTransformer, interpolate_pos_embed\n",
    "from nimrod.image.med import BertConfig, BertModel, BertLMHeadModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from timm.models.hub import download_cached_file\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "MED_CFG = \"../config/data/image/med_config.json\"\n",
    "\n",
    "class BLIP_Base(nn.Module):\n",
    "    def __init__(self,                 \n",
    "                 med_config = MED_CFG,  \n",
    "                 image_size = 224,\n",
    "                 vit = 'base',\n",
    "                 vit_grad_ckpt = False,\n",
    "                 vit_ckpt_layer = 0,                 \n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "            image_size (int): input image size\n",
    "            vit (str): model size of vision transformer\n",
    "        \"\"\"               \n",
    "        super().__init__()\n",
    "        \n",
    "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "        self.tokenizer = init_tokenizer()   \n",
    "        med_config = BertConfig.from_json_file(med_config)\n",
    "        med_config.encoder_width = vision_width\n",
    "        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n",
    "\n",
    "        \n",
    "    def forward(self, image, caption, mode):\n",
    "        \n",
    "        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\").to(image.device) \n",
    "        \n",
    "        if mode=='image':    \n",
    "            # return image features\n",
    "            image_embeds = self.visual_encoder(image)             \n",
    "            return image_embeds\n",
    "        \n",
    "        elif mode=='text':\n",
    "            # return text features\n",
    "            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n",
    "                                            return_dict = True, mode = 'text')  \n",
    "            return text_output.last_hidden_state\n",
    "        \n",
    "        elif mode=='multimodal':\n",
    "            # return multimodel features\n",
    "            image_embeds = self.visual_encoder(image)    \n",
    "            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      \n",
    "            \n",
    "            text.input_ids[:,0] = self.tokenizer.enc_token_id\n",
    "            output = self.text_encoder(text.input_ids,\n",
    "                                       attention_mask = text.attention_mask,\n",
    "                                       encoder_hidden_states = image_embeds,\n",
    "                                       encoder_attention_mask = image_atts,      \n",
    "                                       return_dict = True,\n",
    "                                      )              \n",
    "            return output.last_hidden_state\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "        \n",
    "class BLIP_Decoder(nn.Module):\n",
    "    def __init__(self,                 \n",
    "                 med_config = MED_CFG,  \n",
    "                 image_size = 384,\n",
    "                 vit = 'base',\n",
    "                 vit_grad_ckpt = False,\n",
    "                 vit_ckpt_layer = 0,\n",
    "                 prompt = 'a picture of ',\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "            image_size (int): input image size\n",
    "            vit (str): model size of vision transformer\n",
    "        \"\"\"            \n",
    "        super().__init__()\n",
    "        \n",
    "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "        self.tokenizer = init_tokenizer()   \n",
    "        med_config = BertConfig.from_json_file(med_config)\n",
    "        med_config.encoder_width = vision_width\n",
    "        self.text_decoder = BertLMHeadModel(config=med_config)    \n",
    "        \n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1\n",
    "\n",
    "        \n",
    "    def forward(self, image, caption):\n",
    "        \n",
    "        image_embeds = self.visual_encoder(image) \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        \n",
    "        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(image.device) \n",
    "        \n",
    "        text.input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "        \n",
    "        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n",
    "        decoder_targets[:,:self.prompt_length] = -100\n",
    "     \n",
    "        decoder_output = self.text_decoder(text.input_ids, \n",
    "                                           attention_mask = text.attention_mask, \n",
    "                                           encoder_hidden_states = image_embeds,\n",
    "                                           encoder_attention_mask = image_atts,                  \n",
    "                                           labels = decoder_targets,\n",
    "                                           return_dict = True,   \n",
    "                                          )   \n",
    "        loss_lm = decoder_output.loss\n",
    "        \n",
    "        return loss_lm\n",
    "        \n",
    "    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
    "        image_embeds = self.visual_encoder(image)\n",
    "\n",
    "        if not sample:\n",
    "            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n",
    "            \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n",
    "        \n",
    "        prompt = [self.prompt] * image.size(0)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \n",
    "        input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "        input_ids = input_ids[:, :-1] \n",
    "\n",
    "        if sample:\n",
    "            #nucleus sampling\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=top_p,\n",
    "                                                  num_return_sequences=1,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id, \n",
    "                                                  repetition_penalty=1.1,                                            \n",
    "                                                  **model_kwargs)\n",
    "        else:\n",
    "            #beam search\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  num_beams=num_beams,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id,     \n",
    "                                                  repetition_penalty=repetition_penalty,\n",
    "                                                  **model_kwargs)            \n",
    "            \n",
    "        captions = []    \n",
    "        for output in outputs:\n",
    "            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n",
    "            captions.append(caption[len(self.prompt):])\n",
    "        return captions\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def blip_decoder(pretrained='',**kwargs):\n",
    "    model = BLIP_Decoder(**kwargs)\n",
    "    if pretrained:\n",
    "        model,msg = load_checkpoint(model,pretrained)\n",
    "        assert(len(msg.missing_keys)==0)\n",
    "    return model    \n",
    "    \n",
    "def blip_feature_extractor(pretrained='',**kwargs):\n",
    "    model = BLIP_Base(**kwargs)\n",
    "    if pretrained:\n",
    "        model,msg = load_checkpoint(model,pretrained)\n",
    "        assert(len(msg.missing_keys)==0)\n",
    "    return model        \n",
    "\n",
    "def init_tokenizer():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       \n",
    "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n",
    "        \n",
    "    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n",
    "    if vit=='base':\n",
    "        vision_width = 768\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n",
    "                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0 or drop_path_rate\n",
    "                                          )   \n",
    "    elif vit=='large':\n",
    "        vision_width = 1024\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n",
    "                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0.1 or drop_path_rate\n",
    "                                          )   \n",
    "    return visual_encoder, vision_width\n",
    "\n",
    "def is_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "def load_checkpoint(model,url_or_filename):\n",
    "    if is_url(url_or_filename):\n",
    "        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n",
    "        checkpoint = torch.load(cached_file, map_location='cpu') \n",
    "    elif os.path.isfile(url_or_filename):        \n",
    "        checkpoint = torch.load(url_or_filename, map_location='cpu') \n",
    "    else:\n",
    "        raise RuntimeError('checkpoint url or path is invalid')\n",
    "        \n",
    "    state_dict = checkpoint['model']\n",
    "    \n",
    "    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n",
    "    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n",
    "        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n",
    "                                                                         model.visual_encoder_m)    \n",
    "    for key in model.state_dict().keys():\n",
    "        if key in state_dict.keys():\n",
    "            if state_dict[key].shape!=model.state_dict()[key].shape:\n",
    "                del state_dict[key]\n",
    "    \n",
    "    msg = model.load_state_dict(state_dict,strict=False)\n",
    "    print('load checkpoint from %s'%url_or_filename)  \n",
    "    return model,msg\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(\"DEVICE: \", device)\n",
    "\n",
    "def load_demo_image(image_size,device):\n",
    "    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')   \n",
    "\n",
    "    w,h = raw_image.size\n",
    "    display(raw_image.resize((w//5,h//5)))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "    image = transform(raw_image).unsqueeze(0).to(device)   \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from nimrod.image.blip import blip_decoder\n",
    "# image_size = 384\n",
    "# image = load_demo_image(image_size=image_size, device=device)\n",
    "\n",
    "# model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n",
    "    \n",
    "# model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n",
    "# model.eval()\n",
    "# model = model.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # beam search\n",
    "#     caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) \n",
    "#     # nucleus sampling\n",
    "#     # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n",
    "#     print('caption: '+caption[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF Transformer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "# image = Image.open(\"path_to_your_image.jpg\")\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(image, text=text, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unConditional image captioning\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "# image = Image.open(\"path_to_your_image.jpg\")\n",
    "\n",
    "# img_url = \"https://images.artnet.com/aoa_lot_images/141282/shepard-fairey-kurt-cobain-endless-nameless-prints-and-multiples-zoom_374_500.jpg\"\n",
    "img_url = \"https://images.artnet.com/aoa_lot_images/141287/hiroshi-sugimoto-gulf-of-bothnia-photographs.jpeg\"\n",
    "image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# Conditional image captioning\n",
    "text = \"a painting of\"\n",
    "inputs = processor(image, text=text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Caption:\", caption)\n",
    "display(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
