{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGPT LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from nimrod.text.tokenizers import CharTokenizer\n",
    "from nimrod.text.datasets import SimpleCharDataset\n",
    "from nimrod.models.transformer import TransformerBlock\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we \n",
      "vocabulary size: 65\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "length of data str: 1115384\n"
     ]
    }
   ],
   "source": [
    "text = Path('../data/text/tiny_shakespeare.txt').read_text()\n",
    "print(text[:25])\n",
    "tok = CharTokenizer.from_text(text)\n",
    "print(f\"vocabulary size: {len(tok)}\")\n",
    "encoded = tok.encode(text)\n",
    "print(encoded[:10])\n",
    "ds = SimpleCharDataset(encoded, 10)\n",
    "print(f\"length of data str: {len(ds)}\")\n",
    "n_train = int(0.8 * len(ds))\n",
    "n_eval = int(0.1 * len(ds))\n",
    "n_test = len(ds) - n_train - n_eval\n",
    "train_ds, eval_ds, test_ds = random_split(ds, lengths=(n_train, n_eval, n_test))\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "eval_dl = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
    "test_dl = DataLoader(train_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        embed_dim:int,\n",
    "        n_head:int,\n",
    "        context_length:int,\n",
    "        dropout:float,\n",
    "        n_blocks:int\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(context_length, embed_dim)\n",
    "        block_params = [embed_dim, n_head, context_length, dropout]\n",
    "        self.blocks = [TransformerBlock(*block_params) for _ in range(n_blocks)]\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x:torch.LongTensor # (B,T) list of token IDS\n",
    "        ):\n",
    "        B,T = x.shape\n",
    "        tok_embed = self.tok_embed(x) # B, T, C\n",
    "        pos_embed = self.pos_embed(torch.arange(T, device=x.device)) # B, T, C\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        x:torch.LongTensor, # tok ids (B, T)\n",
    "        max_tokens: int # max size of sequence gen\n",
    "    ):  \n",
    "        self.eval()\n",
    "        for i in range(max_tokens):\n",
    "            if x.size(1) >= self.context_length:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(x) # (B, T, n_classes)\n",
    "                # look at last time step only\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                pred = torch.multinomial(probs, num_samples=1)\n",
    "                x = torch.cat([x, pred], dim=1) #(T+1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size:int\n",
    "    embed_dim:int = 768\n",
    "    n_head:int = 4\n",
    "    context_length:int = 32\n",
    "    n_blocks:int = 4\n",
    "    dropout:float = 0.1\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.embed_dim % self.n_head == 0, \"embed_dim must be divisible by n_head\"\n",
    "        assert self.dropout >= 0 and self.dropout <= 1, \"dropout must be between 0 and 1\"\n",
    "\n",
    "cfg = GPTConfig(vocab_size=len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGPT(\n",
      "  (tok_embed): Embedding(65, 768)\n",
      "  (pos_embed): Embedding(32, 768)\n",
      "  (blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x AttentionHead(\n",
      "            (key): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (query): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (value): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x AttentionHead(\n",
      "            (key): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (query): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (value): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x AttentionHead(\n",
      "            (key): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (query): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (value): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (ln1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-3): 4 x AttentionHead(\n",
      "            (key): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (query): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (value): Linear(in_features=768, out_features=192, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=768, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = MiniGPT(**asdict(cfg))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32, 65])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "B, T, C = 5, cfg.context_length, cfg.embed_dim\n",
    "x = torch.randint(0, cfg.vocab_size, (B,T), dtype=torch.long).to(device)\n",
    "m = m.to(device)\n",
    "y = m(x) # B,T, n_classes\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46, 43, 50, 50, 53,  2]])\n",
      "tensor([[46, 43, 50, 50, 53,  2, 59, 11, 41, 34,  7,  4,  8, 63, 16, 30, 18, 32,\n",
      "         34, 58, 51, 54, 61, 40, 26, 45,  6, 22, 16, 64, 26]])\n",
      "hello!u;cV-&.yDRFTVtmpwbNg,JDzN\n"
     ]
    }
   ],
   "source": [
    "prompt = \"hello!\"\n",
    "encoded = tok.encode(prompt).unsqueeze(dim=0)\n",
    "print(encoded)\n",
    "res = m.generate(encoded, max_tokens=25)\n",
    "print(res)\n",
    "print(tok.decode(res.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
