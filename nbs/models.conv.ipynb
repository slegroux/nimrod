{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import Accuracy\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nimrod.image.datasets import MNISTDataModule\n",
    "from nimrod.utils import get_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Layer\n",
    "\n",
    "Using a convolution with a stride of 2 instead of max pooling essentially achieves the same goal of downsampling an image by reducing its spatial dimensions, but with the key difference that the convolution layer can learn more complex feature combinations from overlapping regions, while max pooling only selects the maximum value within a window, potentially losing information about the finer details within that region; making the convolution with stride approach often preferred for preserving more spatial information in a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels:int, # input channels\n",
    "                out_channels:int, # output channels\n",
    "                kernel_size:int=3, # kernel size\n",
    "                activation:bool=True\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        # use stride 2 for downsampling instead of max or average pooling with stride 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, 2, kernel_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.activation:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3136])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 64, 1, 28, 28\n",
    "X = torch.rand(B, C, H, W)\n",
    "c = ConvLayer(1, 16, 3)\n",
    "# flatten all dims except batch\n",
    "Y = torch.flatten(c(X), 1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnet\n",
    "Simple convolution network for image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels:int=1, out_channels:int=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ConvLayer(in_channels, 8, kernel_size=5), #14x14\n",
    "            nn.BatchNorm2d(8),\n",
    "            ConvLayer(8, 16), #7x7\n",
    "            nn.BatchNorm2d(16),\n",
    "            ConvLayer(16, 32), #4x4\n",
    "            nn.BatchNorm2d(32),\n",
    "            ConvLayer(32, 64), #2x2\n",
    "            nn.BatchNorm2d(64),\n",
    "            ConvLayer(64, 10, activation=False), #1x1\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Flatten()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor # input image tensor of dimension (B, C, W, H)\n",
    "                ) -> torch.Tensor: # output probs (B, N_classes)\n",
    "\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, C, H, W = 64, 1, 28, 28\n",
    "X = torch.rand(B, C, H, W)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (net): Sequential(\n",
      "    (0): ConvLayer(\n",
      "      (conv): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ConvLayer(\n",
      "      (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ConvLayer(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvLayer(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ConvLayer(\n",
      "      (conv): Conv2d(64, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (9): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# model instantiation\n",
    "convnet = ConvNet()\n",
    "print(convnet)\n",
    "out = convnet(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (C,H,W):  torch.Size([1, 28, 28]) y:  0\n",
      "XX (B,C,H,W):  torch.Size([512, 1, 28, 28]) YY:  torch.Size([512])\n",
      "110\n",
      "56000\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# data module config\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "cfg.datamodule.batch_size = 512\n",
    "cfg.datamodule.pin_memory = True\n",
    "cfg.num_workers = 1\n",
    "\n",
    "# data module instantiation\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "# one data point \n",
    "X,y = datamodule.data_test[0]\n",
    "print(\"X (C,H,W): \", X.shape, \"y: \", y)\n",
    "\n",
    "# a batch of data via dataloader\n",
    "XX,YY = next(iter(datamodule.test_dataloader()))\n",
    "print(\"XX (B,C,H,W): \", XX.shape, \"YY: \", YY.shape)\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "print(len(train_loader))\n",
    "print(len(datamodule.data_train))\n",
    "print(len(datamodule.data_train)//cfg.datamodule.batch_size)\n",
    "val_loader = datamodule.val_dataloader()\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model & hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = ConvNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "steps_per_epochs = len(datamodule.data_train)//cfg.datamodule.batch_size\n",
    "print(len(train_loader))\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epochs, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 0.7540, Accuracy = 88.07%\n",
      "CPU times: user 3.55 s, sys: 145 ms, total: 3.7 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            # model expects input (B,H*W)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Pass the input through the model\n",
    "            outputs = model(images)\n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"Epoch {epoch + 1}: Loss {loss.item():.4f}, Accuracy = {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimrod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
