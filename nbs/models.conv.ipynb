{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.models.core import Classifier\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Layer\n",
    "\n",
    "Using a convolution with a stride of 2 instead of max pooling essentially achieves the same goal of downsampling an image by reducing its spatial dimensions, but with the key difference that the convolution layer can learn more complex feature combinations from overlapping regions, while max pooling only selects the maximum value within a window, potentially losing information about the finer details within that region; making the convolution with stride approach often preferred for preserving more spatial information in a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels:int=3, # input channels\n",
    "                out_channels:int=16, # output channels\n",
    "                kernel_size:int=3, # kernel size\n",
    "                activation:bool=True\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        # use stride 2 for downsampling instead of max or average pooling with stride 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, 2, kernel_size//2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.activation:\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3136])\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 64, 1, 28, 28\n",
    "X = torch.rand(B, C, H, W)\n",
    "c = ConvLayer(1, 16, 3, True)\n",
    "# flatten all dims except batch\n",
    "Y = torch.flatten(c(X), 1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convnet Model\n",
    "Simple convolution network for image recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int=1, # input channels\n",
    "            out_channels:int=10 # num_classes\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            ConvLayer(in_channels, 8, kernel_size=5), #14x14\n",
    "            nn.BatchNorm2d(8),\n",
    "            ConvLayer(8, 16), #7x7\n",
    "            nn.BatchNorm2d(16),\n",
    "            ConvLayer(16, 32), #4x4\n",
    "            nn.BatchNorm2d(32),\n",
    "            ConvLayer(32, 64), #2x2\n",
    "            nn.BatchNorm2d(64),\n",
    "            ConvLayer(64, 10, activation=False), #1x1\n",
    "            nn.BatchNorm2d(10),\n",
    "            nn.Flatten()\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x:torch.Tensor # input image tensor of dimension (B, C, W, H)\n",
    "                ) -> torch.Tensor: # output probs (B, N_classes)\n",
    "\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, C, H, W = 64, 1, 28, 28\n",
    "X = torch.rand(B, C, H, W)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "{'_target_': 'nimrod.models.conv.ConvNet', 'in_channels': 1, 'out_channels': '${num_classes}'}\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# model instantiation\n",
    "convnet = ConvNet()\n",
    "out = convnet(X)\n",
    "print(out.shape)\n",
    "\n",
    "# from config\n",
    "cfg = OmegaConf.load('../config/image/model/conv.yaml')\n",
    "print(cfg.nnet)\n",
    "convnet = instantiate(cfg.nnet)\n",
    "print(convnet(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18:04:01] INFO - Init MNIST DataModule\n",
      "[18:04:01] INFO - MNISTDataset: init\n",
      "[18:04:01] INFO - ImageDataset: init\n",
      "[18:04:08] INFO - MNISTDataset: init\n",
      "[18:04:08] INFO - ImageDataset: init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (C,H,W):  torch.Size([1, 28, 28]) y:  0\n",
      "XX (B,C,H,W):  torch.Size([64, 1, 28, 28]) YY:  torch.Size([64])\n",
      "56000\n",
      "875\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "# data module config\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "# datamodule.prepare_data()\n",
    "datamodule.batch_size = 2048\n",
    "datamodule.setup()\n",
    "\n",
    "# one data point \n",
    "X,y = datamodule.data_test[0]\n",
    "print(\"X (C,H,W): \", X.shape, \"y: \", y)\n",
    "\n",
    "# a batch of data via dataloader\n",
    "XX,YY = next(iter(datamodule.test_dataloader()))\n",
    "print(\"XX (B,C,H,W): \", XX.shape, \"YY: \", YY.shape)\n",
    "\n",
    "print(len(datamodule.data_train))\n",
    "print(len(datamodule.data_train)//cfg.datamodule.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model & hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(device)\n",
    "model = ConvNet()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LR finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    \n",
    "# Initialize LR Finder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "    \n",
    "# Run LR range test\n",
    "lr_finder.range_test(\n",
    "    datamodule.train_dataloader(),\n",
    "    start_lr=1e-6,      # Extremely small starting learning rate\n",
    "    end_lr=10,          # Large ending learning rate\n",
    "    num_iter=100,   # Number of iterations to test\n",
    "    smooth_f=0.05,   # Smoothing factor for the loss\n",
    "    diverge_th=5, \n",
    ")\n",
    "    \n",
    "# Plot the learning rate vs loss\n",
    "_, lr_found = lr_finder.plot(log_lr=True)\n",
    "print('Suggested lr:', lr_found)\n",
    "    \n",
    "lr_finder.reset()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1-cycle training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "# data module config\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "cfg.datamodule.batch_size = 2048\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "# datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "print(device)\n",
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "steps_per_epoch = len(datamodule.data_train) // cfg.datamodule.batch_size\n",
    "total_steps = steps_per_epoch* N_EPOCHS\n",
    "print(len(datamodule.data_train), cfg.datamodule.batch_size, steps_per_epoch, total_steps)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epochs, epochs=1)\n",
    "N_EPOCHS = 10\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr_found,  # Peak learning rate\n",
    "        # total_steps=len(datamodule.data_train) * N_EPOCHS,  # Total training iterations\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=N_EPOCHS,\n",
    "        pct_start=0.3,  # 30% of training increasing LR, 70% decreasing\n",
    "        anneal_strategy='cos',  # Cosine annealing\n",
    "        div_factor=10,  # Initial lr = max_lr / div_factor\n",
    "        # final_div_factor=1e4,\n",
    "        three_phase=False  # Two phase LR schedule (increase then decrease)\n",
    "    )\n",
    "\n",
    "%time\n",
    "losses = []\n",
    "lrs = []\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    i = 0\n",
    "    model.train()\n",
    "    for images, labels in datamodule.train_dataloader():\n",
    "        if current_step >= total_steps:\n",
    "            print(f\"Reached total steps: {current_step}/{total_steps}\")\n",
    "            break\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()    \n",
    "        current_step += 1\n",
    "    \n",
    "        losses.append(loss.item())\n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lrs.append(current_lr)\n",
    "        if not (i % 100):\n",
    "            print(f\"Loss {loss.item():.4f}, Current LR: {current_lr:.10f}, Step: {current_step}/{total_steps}\")\n",
    "        i += 1\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in datamodule.test_dataloader():\n",
    "            # model expects input (B,H*W)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Pass the input through the model\n",
    "            outputs = model(images)\n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        # Print the accuracy\n",
    "    print(f\"Epoch {epoch + 1}: Loss {loss.item():.4f}, Accuracy = {100 * correct / total:.2f}%\")\n",
    "    # print(f'Current LR: {optimizer.param_groups[0][\"lr\"]:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.plot(losses)\n",
    "plt.subplot(212)\n",
    "plt.ylabel('lr')\n",
    "plt.xlabel('step')\n",
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNetX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ConvNetX(Classifier, LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet:ConvNet,\n",
    "            num_classes:int,\n",
    "            optimizer:torch.optim.Optimizer,\n",
    "            scheduler:torch.optim.lr_scheduler,\n",
    "            ):\n",
    "        logger.info(\"ConvNetX: init\")\n",
    "        super().__init__(num_classes, optimizer, scheduler)\n",
    "        self.save_hyperparameters(logger=False, ignore=['nnet'])\n",
    "        self.lr = optimizer.keywords['lr'] # for lr finder\n",
    "        self.nnet = nnet\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        return loss, preds, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../config/image/model/conv.yaml')\n",
    "model = instantiate(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 64, 1, 28, 28\n",
    "X = torch.rand(B, C, H, W)\n",
    "X.shape\n",
    "print(model(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nimrod training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cfg = OmegaConf.load('../config/image/model/conv.yaml')\n",
    "model = instantiate(cfg)\n",
    "\n",
    "# data module config\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "cfg.datamodule.batch_size = 2048\n",
    "cfg.datamodule.num_workers = 0\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "# datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=3,\n",
    "    logger=CSVLogger(\"logs\", name=\"mnist_convnet\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "csv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\n",
    "metrics = pd.read_csv(csv_path)\n",
    "metrics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.figure()\n",
    "plt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\n",
    "plt.plot(metrics['step'], metrics['val/loss'],'r.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
