{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio TTS Datasets\n",
    "\n",
    "> TTS datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp audio.datasets.tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pytorch_lightning import LightningDataModule, LightningModule\n",
    "from matplotlib import pyplot as plt\n",
    "from lhotse.dataset import BucketingSampler, OnTheFlyFeatures\n",
    "from lhotse.dataset.collation import TokenCollater\n",
    "from lhotse.recipes import download_librispeech, prepare_librispeech\n",
    "from lhotse.dataset.vis import plot_batch\n",
    "from lhotse import CutSet, RecordingSet, SupervisionSet, Fbank, FbankConfig\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-To-Speech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TTSDataset(Dataset):\n",
    "    def __init__(self,\n",
    "        tokenizer:TokenCollater, # text tokenizer\n",
    "        num_mel_bins:int=80 # number of mel spectrogram bins\n",
    "        ):\n",
    "        self.extractor = OnTheFlyFeatures(Fbank(FbankConfig(num_mel_bins=num_mel_bins)))\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, cuts: CutSet) -> dict:\n",
    "        cuts = cuts.sort_by_duration()\n",
    "        feats, feat_lens = self.extractor(cuts)\n",
    "        tokens, token_lens = self.tokenizer(cuts)\n",
    "        return {\"feats_pad\": feats, \"ilens\": feat_lens, \"tokens_pad\": tokens}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJSpeech DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class LJSpeechDataModule(LightningDataModule):\n",
    "#     def __init__(self,\n",
    "#         target_dir=\"/data/en\", # where data will be saved / retrieved\n",
    "#         dataset_parts=\"mini_librispeech\", # either full librispeech or mini subset\n",
    "#         output_dir=\"../recipes/tts/ljspeech/data\" # where to save manifest\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(logger=False)\n",
    "\n",
    "#     def prepare_data(self,) -> None:\n",
    "#         download_librispeech(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)\n",
    "\n",
    "#     def setup(self, stage = None):\n",
    "#         libri = prepare_librispeech(corpus_dir=Path(self.hparams.target_dir) / \"LibriSpeech\", output_dir=self.hparams.output_dir)\n",
    "#         self.cuts_train = CutSet.from_manifests(**libri[\"train-clean-5\"])\n",
    "#         self.cuts_test = CutSet.from_manifests(**libri[\"dev-clean-2\"])\n",
    "#         self.tokenizer = TokenCollater(self.cuts_train)\n",
    "#         self.tokenizer(self.cuts_test.subset(first=2))\n",
    "#         self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\n",
    "#         return DataLoader(STTDataset(self.tokenizer), sampler=train_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False, bucket_method=\"equal_duration\")\n",
    "#         return DataLoader(STTDataset(self.tokenizer), sampler=test_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "#     @property\n",
    "#     def model_kwargs(self):\n",
    "#         return {\n",
    "#             \"odim\": len(self.tokenizer.idx2token),\n",
    "#         }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibriTTS DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from lhotse.recipes import download_libritts, prepare_libritts\n",
    "from nimrod.text.tokenizers import Tokenizer\n",
    "from nimrod.audio.embedding import EncoDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LibriTTSDataModule(LightningDataModule):\n",
    "    def __init__(self,\n",
    "        target_dir=\"/data/en/libriTTS\", # where data will be saved / retrieved\n",
    "        dataset_parts=[\"dev-clean\", \"test-clean\"], # either full libritts or subset\n",
    "        output_dir=\"/home/syl20/slg/nimrod/recipes/libritts/data\", # where to save manifest\n",
    "        num_jobs=1 # num_jobs depending on number of cpus available\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "    def prepare_data(self,) -> None:\n",
    "        # takes a while to download from openslr mirror (~15 min each for test/dev-clean)\n",
    "        download_libritts(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.libri = prepare_libritts(corpus_dir=Path(self.hparams.target_dir) / \"LibriTTS\", output_dir=self.hparams.output_dir, num_jobs=self.hparams.num_jobs)\n",
    "        if stage == 'fit' or stage == None:\n",
    "            self.cuts_train = CutSet.from_manifests(**self.libri[\"dev-clean\"])\n",
    "            self.cuts_test = CutSet.from_manifests(**self.libri[\"test-clean\"])\n",
    "            self.tokenizer = TokenCollater(self.cuts_train)\n",
    "            self.tokenizer(self.cuts_test.subset(first=2))\n",
    "            self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "        if stage == \"test\":\n",
    "            self.cuts_test = CutSet.from_manifests(**self.libri[\"test-clean\"])\n",
    "            self.tokenizer = TokenCollater(self.cuts_test)\n",
    "            self.tokenizer(self.cuts_test.subset(first=2))\n",
    "            self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\n",
    "        return DataLoader(TTSDataset(self.tokenizer), sampler=train_sampler, batch_size=None, num_workers=100)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False, bucket_method=\"equal_duration\")\n",
    "        return DataLoader(TTSDataset(self.tokenizer), sampler=test_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "    @property\n",
    "    def model_kwargs(self):\n",
    "        return {\n",
    "            \"odim\": len(self.tokenizer.idx2token),\n",
    "        }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LibriTTSDataModule(\n",
    "    target_dir=\"../data/en\", \n",
    "    dataset_parts=\"test-clean\",\n",
    "    output_dir=\"../data/en/LibriTTS/test-clean\",\n",
    "    num_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip download and use local data folder\n",
    "# dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 95it [00:00, 4875.90it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]00:00<00:00, 48.20it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Preparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 50.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dm.setup(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
