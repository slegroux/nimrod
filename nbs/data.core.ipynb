{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Core Utils\n",
    "> core classes & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# python\n",
    "from typing import List, Optional, Dict, Any, Tuple, Sequence\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, Subset\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "The Data Core module provides fundamental data handling, transformation, and preprocessing utilities across different machine learning domains.\n",
    "\n",
    "## Key Features\n",
    "- Flexible dataset loading and preprocessing\n",
    "- Standardized data transformation interfaces\n",
    "- Support for various data types and formats\n",
    "- Configurable data augmentation\n",
    "- Seamless integration with PyTorch and other ML frameworks\n",
    "\n",
    "## Main Components\n",
    "- Abstract base classes for datasets\n",
    "- Transformation utilities\n",
    "- Data loading strategies\n",
    "- Preprocessing pipelines\n",
    "- Compatibility with hydra configuration\n",
    "\n",
    "## Design Principles\n",
    "- Modularity: Easily extensible and composable\n",
    "- Type safety: Strong typing and runtime checks\n",
    "- Performance: Optimized data loading and preprocessing\n",
    "- Flexibility: Support for multiple data sources and formats\n",
    "\n",
    "## Use Cases\n",
    "- Computer Vision\n",
    "- Natural Language Processing\n",
    "- Time Series Analysis\n",
    "- Multi-modal Machine Learning\n",
    "\n",
    "## Configuration\n",
    "Supports configuration via:\n",
    "- YAML files\n",
    "- Hydra configuration\n",
    "- Runtime parameter overrides\n",
    "\n",
    "## Dependencies\n",
    "- PyTorch\n",
    "- torchvision\n",
    "- hydra-core\n",
    "- typing extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataModule(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # data_dir: str = \"~/Data/\", # path to source data dir\n",
    "        # train_val_split:Tuple[float, float] = [0.8, 0.2], # train val test %\n",
    "        batch_size: int = 64, # size of compute batch\n",
    "        num_workers: int = 0, # num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow\n",
    "        pin_memory: bool = False, # If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer\n",
    "        persistent_workers: bool = False,\n",
    "        shuffle: bool = True # shuffle training data\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # can access inputs with self.hparams\n",
    "        self.transforms = transforms.Compose([transforms.ToTensor()])\n",
    "        self.train_ds: Optional[Dataset] = None\n",
    "        self.val_ds: Optional[Dataset] = None\n",
    "        self.test_ds: Optional[Dataset] = None\n",
    "\n",
    "        # if sum(train_val_split) != 1.0:\n",
    "        #     raise Exception('split percentages should sum up to 1.0')\n",
    "    \n",
    "    @abstractmethod\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Download data if needed.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def setup(self, stage: Optional[str]=None)->None:\n",
    "        \"\"\"Split data into train, val, test.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.train_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.val_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return DataLoader(\n",
    "            dataset=self.test_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Clean up after fit or test.\"\"\"\n",
    "        return NotImplemented\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"Extra things to save to checkpoint.\"\"\"\n",
    "        # return {}\n",
    "        return NotImplemented\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
    "        \"\"\"Things to do when loading checkpoint.\"\"\"\n",
    "        return NotImplemented\n",
    "\n",
    "    def _sequential_split(self, dataset: Dataset, split_ratios: Sequence[float]) -> List[Subset]:\n",
    "        \"\"\"Split a dataset into train, validation and test sets sequentially (not randomly).\n",
    "        This is useful for time series or sequence data where order matters.\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset to split\n",
    "            split_ratios: Sequence of ratios that sum to 1.0 (e.g. [0.8, 0.1, 0.1])\n",
    "            \n",
    "        Returns:\n",
    "            List of dataset subsets\n",
    "        \"\"\"\n",
    "        if not isinstance(dataset, Dataset):\n",
    "            raise TypeError(\"dataset must be a Dataset instance\")\n",
    "        if abs(sum(split_ratios) - 1.0) > 1e-6:\n",
    "            raise ValueError(\"split ratios must sum to 1\")\n",
    "        logger.info(\"Split dataset into train/val/test. Keep sequence order.\")\n",
    "        # Calculate lengths\n",
    "        total_length = len(dataset)\n",
    "        lengths = [int(ratio * total_length) for ratio in split_ratios[:-1]]\n",
    "        # Ensure we use all samples by assigning remainder to last split\n",
    "        lengths.append(total_length - sum(lengths))\n",
    "        \n",
    "        # Create cumulative indices\n",
    "        cum_lengths = [0] + [sum(lengths[:i+1]) for i in range(len(lengths))]\n",
    "        \n",
    "        # Create subsets with sequential indices\n",
    "        subsets = []\n",
    "        for start_idx, end_idx in zip(cum_lengths[:-1], cum_lengths[1:]):\n",
    "            indices = range(start_idx, end_idx)\n",
    "            subset = Subset(dataset, indices)\n",
    "            subsets.append(subset)\n",
    "            \n",
    "        return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def split_train_valid_test(dataset:Dataset, splits:List[float]):\n",
    "    lengths = [int(split * len(dataset)) for split in splits]\n",
    "    # if rounding ends up getting rid of some datapoints and total length != length dataset\n",
    "    remain = len(dataset) - sum(lengths)\n",
    "    lengths[-1] += remain\n",
    "    split_datasets = random_split(\n",
    "                dataset=dataset,\n",
    "                lengths=lengths,\n",
    "                # generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "    return split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
