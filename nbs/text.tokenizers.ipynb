{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from phonemizer.backend.espeak.language_switch import LanguageSwitch\n",
    "from phonemizer.backend.espeak.words_mismatch import WordMismatch\n",
    "from phonemizer.punctuation import Punctuation\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader\n",
    "from multipledispatch import dispatch\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Phonemizer():\n",
    "    def __init__(self,\n",
    "        separator=Separator(word=\" \", syllable=\"|\", phone=None), # separator\n",
    "        language='en-us', # language\n",
    "        backend='espeak', # phonemization backend (espeak)\n",
    "        strip=True, # strip\n",
    "        preserve_punctuation=True # preserve punctuation\n",
    "        ):\n",
    "        self.separator = separator\n",
    "        self.language = language\n",
    "        self.backend = backend\n",
    "        self.strip = strip\n",
    "        self.preserve_punctuation = preserve_punctuation\n",
    "    \n",
    "    @dispatch(str)\n",
    "    def __call__(self, text:str, n_jobs=1)->str:\n",
    "        return(\n",
    "            phonemize(\n",
    "                text,\n",
    "                language=self.language,\n",
    "                backend=self.backend,\n",
    "                separator=self.separator,\n",
    "                strip=self.strip,\n",
    "                preserve_punctuation=self.preserve_punctuation,\n",
    "                njobs=n_jobs\n",
    "                )\n",
    "        )\n",
    "\n",
    "    @dispatch(list)\n",
    "    def __call__(self, texts:List[str], n_jobs=1)->List[str]:\n",
    "        return(\n",
    "            [phonemize(\n",
    "                text,\n",
    "                language=self.language,\n",
    "                backend=self.backend,\n",
    "                separator=self.separator,\n",
    "                strip=self.strip,\n",
    "                preserve_punctuation=self.preserve_punctuation,\n",
    "                njobs=n_jobs\n",
    "                )\n",
    "        for text in texts])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oʊ dɪɹ! ðɪs sʌk...\\nwiːl biː faɪn!', 'ðɪs ɪz ɪt']\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = [\"Oh Dear! This suck...\\n We'll be fine!\", \"this is it\"]\n",
    "print(p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Oh Dear This suck We'll be fine\n",
      "words: {'o', 't', 'e', 'a', 'r', 'f', 'i', 'd', 'k', 'w', \"'\", 'l', 'c', 'h', 'n', 's', 'b', 'u'}\n",
      "lexicon:  {'o': 'oʊ', 't': 't iː', 'e': 'iː', 'a': 'eɪ', 'r': 'ɑːɹ', 'f': 'ɛ f', 'i': 'aɪ', 'd': 'd iː', 'k': 'k eɪ', 'w': 'd ʌ b əl j uː', \"'\": '', 'l': 'ɛ l', 'c': 's iː', 'h': 'eɪ tʃ', 'n': 'ɛ n', 's': 'ɛ s', 'b': 'b iː', 'u': 'j uː'}\n",
      "oʊ dɪɹ ðɪs sʌk wiːl biː faɪn\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "text = Punctuation(';:,.!\"?()-').remove(text)\n",
    "print(\"text:\", text)\n",
    "words = {w.lower() for line in text for w in line.strip().split(' ') if w}\n",
    "print(\"words:\", words)\n",
    "# initialize the espeak backend for English\n",
    "backend = EspeakBackend('en-us')\n",
    "\n",
    "# separate phones by a space and ignoring words boundaries\n",
    "separator = Separator(phone=' ', word=None)\n",
    "# build the lexicon by phonemizing each word one by one. The backend.phonemize\n",
    "# function expect a list as input and outputs a list.\n",
    "lexicon = {\n",
    "    word: backend.phonemize([word], separator=separator, strip=True)[0]\n",
    "    for word in words}\n",
    "print(\"lexicon: \", lexicon)\n",
    "separator=Separator(word=\" \", syllable=\"|\", phone=None)\n",
    "\n",
    "phn = phonemize(\n",
    "    text,\n",
    "    language='en-us',\n",
    "    backend='espeak',\n",
    "    separator=separator,\n",
    "    strip=True,\n",
    "    preserve_punctuation=True,\n",
    "    njobs=4)\n",
    "print(phn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires download of spacy specific language e.g. `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from typing import Iterable, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, backend='spacy', language='en'):\n",
    "        if language == 'en':\n",
    "            language = 'en_core_web_sm'\n",
    "        self.tokenizer = get_tokenizer(backend, language=language)\n",
    "\n",
    "    @dispatch(str)\n",
    "    def __call__(self, text:str)->str:\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    @dispatch(object) # to replace Iterable\n",
    "    # works with agnews type of dataset [(index, text)]\n",
    "    def __call__(self, data_iter:Iterable)->Iterable:\n",
    "        for _, text in data_iter:\n",
    "            yield self.tokenizer(text)\n",
    "    \n",
    "    @dispatch(list)\n",
    "    def __call__(self, texts:List[str])->List[str]:\n",
    "        return [self.tokenizer(text) for text in texts]\n",
    "\n",
    "    def inverse(self, tokens:List[int]):\n",
    "        # TODO: take care of white spaces\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, yeah\n",
      " I don't know dude...\n",
      "['Oh', ',', 'yeah', '\\n ', 'I', 'do', \"n't\", 'know', 'dude', '...']\n",
      "Oh , yeah \n",
      "  I do n't know dude ...\n",
      "[['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...'], ['this', 'is', 'a', 'test']]\n",
      "(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n",
      "[['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['The', 'Race', 'is', 'On', ':', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'TORONTO', ',', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', ' ', '#', '36;10', 'million', 'Ansari', 'X', 'Prize', ',', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight', ',', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket', '.']]\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "# string\n",
    "s = \"Oh, yeah\\n I don't know dude...\"\n",
    "tokenized = tok(s)\n",
    "print(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "\n",
    "# list of strings\n",
    "s = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\n",
    "tokenized = tok(s)\n",
    "print(tokenized)\n",
    "# iterable \n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "print(sample)\n",
    "it = tok(ds)\n",
    "tokens = [token for token in it]\n",
    "print(tokens[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: add more special characters\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokenizer:Tokenizer, data_iter:Iterable, specials=[\"<unk>\"]):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._vocab = self.build_map_from_iter(data_iter, specials=specials)\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials = [\"<unk>\"]):\n",
    "        self._vocab = build_vocab_from_iterator(self._tokenizer.tokenize_iter(data_iter), specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab\n",
    "\n",
    "    @dispatch(list, type=torch.LongTensor)\n",
    "    def __call__(self, texts:List[str], type=torch.LongTensor)->List[List[int]]:\n",
    "        return [type(self._vocab(self._tokenizer(text))) for text in texts]\n",
    "        \n",
    "    @dispatch(str)\n",
    "    def __call__(self, text:str)->List[int]:\n",
    "        return self._vocab(self._tokenizer(text))\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return(self._vocab)\n",
    "    \n",
    "    def inverse(self, indices:List[int]):\n",
    "        return self._tokenizer.inverse([self._vocab.get_itos()[i] for i in indices])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "num = Numericalizer(tok, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 531, 1037,  307,    3,    0]), tensor([31, 37, 98, 64])]\n",
      "[55, 24, 31]\n",
      "1\n",
      "the\n",
      "this is it\n"
     ]
    }
   ],
   "source": [
    "print(num([\"here we go. asdflkj\", \"it was time...\"]))\n",
    "print(num(\"this is it\"))\n",
    "vocab = num.vocab\n",
    "print(vocab.get_stoi()['the'])\n",
    "print(vocab.get_itos()[1])\n",
    "print(num.inverse([55, 24, 31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextCollater:\n",
    "    def __init__(self,\n",
    "                 numericalizer,\n",
    "                 padding_value:int= -1\n",
    "                 ):\n",
    "        self._numericalizer = numericalizer\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def collate_list(self, texts:List[str])->Tuple[torch.Tensor, torch.Tensor]:\n",
    "        tokens = self._numericalizer(texts)\n",
    "        text_lens = torch.LongTensor([token.shape[0] for token in tokens])\n",
    "        text_pad = pad_sequence(tokens, batch_first=True, padding_value=self.padding_value)\n",
    "        return text_pad, text_lens\n",
    "\n",
    "    def collate_agnews(self, batch)->Tuple[torch.Tensor, torch.Tensor]:\n",
    "        texts = [row[1] for row in batch]\n",
    "        tokens = self._numericalizer(texts)\n",
    "        text_lens = torch.LongTensor([token.shape[0] for token in tokens])\n",
    "        text_pad = pad_sequence(tokens, batch_first=True, padding_value=self.padding_value)\n",
    "        return text_pad, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  55,   24,   31,   64,   -1,   -1],\n",
      "        [  55,   24,    1,   92, 3711,    3]]), tensor([4, 6]))\n"
     ]
    }
   ],
   "source": [
    "collater = TextCollater(num)\n",
    "texts = [\"this is it...\", \"this is the second sentence.\"]\n",
    "print(collater.collate_list(texts))\n",
    "dl = DataLoader(dataset=ds, batch_size=2, shuffle=True, collate_fn=collater.collate_agnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  161, 12780,   514,  1898,  6079,  3465,   325,    22,   161, 12780,\n",
      "           514,    24,  3666,     1,    47,  6079,   279,   650, 10158,    10,\n",
      "            54,     3,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [  467,     2,  4583,     2,  5184,     4,   346,  5020,     5,  4389,\n",
      "          7554,  8169,     8,   312,    90,   198,    64,   197,    27,   962,\n",
      "            14, 16492,    15,     5,  3503,    17,    46,  6261,    65,     7,\n",
      "             1,   100,     5,  1143,  8544,   808,  6039,     2,     1,  5020,\n",
      "             5,  4389,  7554,     2,   246,     4,   487,  8169,    17,   154,\n",
      "            10,  1669,  3447,     7,     1, 10313,    20,   257,    20,   312,\n",
      "             2,     1,  9571,  9410, 21801,   204,     2,   893,  2883,   917,\n",
      "             3]]), tensor([22, 71]))\n",
      "tensor([  161, 12780,   514,  1898,  6079,  3465,   325,    22,   161, 12780,\n",
      "          514,    24,  3666,     1,    47,  6079,   279,   650, 10158,    10,\n",
      "           54,     3])\n",
      "10 O'Clock News goes interactive BBC One 's 10 O'Clock News is launching the first interactive news television bulletin on Tuesday .\n",
      "tensor([  467,     2,  4583,     2,  5184,     4,   346,  5020,     5,  4389,\n",
      "         7554,  8169,     8,   312,    90,   198,    64,   197,    27,   962,\n",
      "           14, 16492,    15,     5,  3503,    17,    46,  6261,    65,     7,\n",
      "            1,   100,     5,  1143,  8544,   808,  6039,     2,     1,  5020,\n",
      "            5,  4389,  7554,     2,   246,     4,   487,  8169,    17,   154,\n",
      "           10,  1669,  3447,     7,     1, 10313,    20,   257,    20,   312,\n",
      "            2,     1,  9571,  9410, 21801,   204,     2,   893,  2883,   917,\n",
      "            3])\n",
      "Sony , Matsushita , Sharp to launch Blu - ray Disc camcorders in 2005 & lt;b&gt; ... &lt;/b&gt ; TOKYO ( AFX ) - Firms that are supporting one of the next - generation optical DVD formats , the Blu - ray Disc , plan to release camcorders that record on smaller versions of the discs as early as 2005 , the Nihon Keizai Shimbun reported , without citing sources .\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "print(b)\n",
    "tokens, lens = b[0], b[1]\n",
    "for token, len in zip(tokens, lens):\n",
    "    print(token[:len])\n",
    "    print(num.inverse(token[:len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
