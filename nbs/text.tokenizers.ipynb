{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# skip_exec: true\n",
    "# skip_showdoc: true\n",
    "# ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import SGD\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set, Iterable\n",
    "from collections import Counter, OrderedDict\n",
    "from dataclasses import dataclass, asdict\n",
    "from plum import dispatch\n",
    "from pathlib import Path\n",
    "\n",
    "# nimrod\n",
    "# from nimrod.models.lm import Vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # torchtext\n",
    "# import torchtext\n",
    "# from torchtext.vocab import vocab\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "# from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, vocabulary:List[str]):\n",
    "        self.ctoi = {char: tok_id for tok_id, char in enumerate(vocabulary)}\n",
    "        self.itoc = {tok_id: char for tok_id, char in enumerate(vocabulary)}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text:str):\n",
    "        vocabulary = set(text)\n",
    "        return cls(sorted(list(vocabulary)))\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        ids = [self.ctoi[c] for c in text]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, ids:torch.tensor):\n",
    "        chars = [self.itoc[id] for id in ids.tolist()]\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ctoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we \n",
      "65\n",
      "39\n",
      "a\n",
      "tensor([58, 46, 47, 57,  1, 47, 57,  1, 57, 61, 43, 50, 50])\n",
      "this is swell\n"
     ]
    }
   ],
   "source": [
    "text = Path('../data/text/tiny_shakespeare.txt').read_text()\n",
    "print(text[:25])\n",
    "\n",
    "tokenizer = CharTokenizer.from_text(text)\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.ctoi['a'])\n",
    "print(tokenizer.itoc[39])\n",
    "enc = tokenizer.encode(\"this is swell\")\n",
    "print(enc)\n",
    "print(tokenizer.decode(enc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext\n",
    "https://pytorch.org/text/0.16.0/data_utils.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self,\n",
    "                backend:str='spacy', # backend tokenizer default to spacy\n",
    "                language:str='en', # language on which tokenization is applied\n",
    "                bos:bool=False, # add beginning of sentence tag <bos>\n",
    "                eos:bool=False, # add end of sentence tag <eos>\n",
    "                ):\n",
    "        if backend == 'spacy' and language == 'en':\n",
    "            language = 'en_core_web_sm'\n",
    "        if backend== 'character_based':\n",
    "            self.tokenizer = self.character_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = get_tokenizer(backend, language=language)\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.backend = backend\n",
    "        print(f\"# Tokenizer uses {self.backend} backend\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def character_tokenizer(text:str)->List[str]:\n",
    "        return [c for c in text]\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, text:str)->List[str]:\n",
    "        res = self.tokenizer(text)\n",
    "        if self.bos:\n",
    "            res = ['<bos>'] + res\n",
    "        if self.eos:\n",
    "            res = res + ['<eos>']\n",
    "        return(res)\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[str])->List[List[str]]:\n",
    "        return [self(text) for text in texts]\n",
    "    \n",
    "    @dispatch # to replace Iterable\n",
    "    # works with agnews type of dataset [(index, text)]\n",
    "    def __call__(self, data_iter:Iterable)->Iterable:\n",
    "        for _, text in data_iter:\n",
    "            yield self(text)\n",
    "\n",
    "    @dispatch    \n",
    "    def inverse(self, tokens:List[str])->str:\n",
    "        if self.backend == 'character_based':\n",
    "            return ''.join(tokens)\n",
    "        # TODO: take care of white spaces\n",
    "        else:\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "    @dispatch\n",
    "    def inverse(self, list_of_tokens:List[List[str]])->List[str]:\n",
    "        s = []\n",
    "        for tokens in list_of_tokens:\n",
    "            s.append(self.inverse(tokens))\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokenizer uses character_based backend\n",
      "original sentence:  Oh, yeah I'm not sure...\n",
      "tokenized:  ['<bos>', 'O', 'h', ',', ' ', 'y', 'e', 'a', 'h', ' ', 'I', \"'\", 'm', ' ', 'n', 'o', 't', ' ', 's', 'u', 'r', 'e', '.', '.', '.', '<eos>']\n",
      "un-tokenized:  <bos>Oh, yeah I'm not sure...<eos>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized: \u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenized)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mun-tokenized: \u001b[39m\u001b[38;5;124m\"\u001b[39m, tok\u001b[38;5;241m.\u001b[39minverse(tokenized))\n\u001b[0;32m---> 11\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# str -> List[str]\u001b[39;00m\n\u001b[1;32m     13\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOh, yeah I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm not sure...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self, backend, language, bos, eos)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharacter_tokenizer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m(backend, language\u001b[38;5;241m=\u001b[39mlanguage)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos \u001b[38;5;241m=\u001b[39m bos\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos \u001b[38;5;241m=\u001b[39m eos\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "tok = Tokenizer(backend='character_based', bos=True, eos=True)\n",
    "# str -> List[str]\n",
    "s = \"Oh, yeah I'm not sure...\"\n",
    "tokenized = tok(s)\n",
    "print(\"original sentence: \", s)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"un-tokenized: \", tok.inverse(tokenized))\n",
    "\n",
    "tok = Tokenizer(backend='spacy', bos=True, eos=True)\n",
    "# str -> List[str]\n",
    "s = \"Oh, yeah I'm not sure...\"\n",
    "tokenized = tok(s)\n",
    "print(\"original sentence: \", s)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"un-tokenized: \", tok.inverse(tokenized))\n",
    "\n",
    "tok = Tokenizer(backend='basic_english', bos=True, eos=True)\n",
    "# str -> List[str]\n",
    "s = \"Oh, yeah I'm not sure...\"\n",
    "tokenized = tok(s)\n",
    "print(\"original sentence: \", s)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"un-tokenized: \", tok.inverse(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# List[str]->List[List[str]]\n",
    "s = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\n",
    "tokenized = tok(s)\n",
    "print(\"original sentence: \", s)\n",
    "print(\"tokenized: \", tokenized)\n",
    "print(\"un-tokenized: \", tok.inverse(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# Iterable -> Iterable\n",
    "tok = Tokenizer()\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "print(sample)\n",
    "it = tok(ds)\n",
    "tokens = [token for token in it]\n",
    "print(len(tokens))\n",
    "print(tokens[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numericalizer\n",
    "https://pytorch.org/text/stable/vocab.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# TODO: add more special characters\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokens_iter:Iterable, specials=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]):\n",
    "        self._vocab = self.build_map_from_iter(tokens_iter, specials)\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials=None):\n",
    "        self._vocab = torchtext.vocab.build_vocab_from_iterator(data_iter, specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab\n",
    "\n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[str])->List[List[int]]:\n",
    "        # TODO: check self._vocab has been built\n",
    "        return [self._vocab[text] for text in texts]\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[List[str]]):\n",
    "        # TODO: use nested list comprehension\n",
    "        res = []\n",
    "        for row in texts:\n",
    "            res.append([self._vocab[text] for text in row])\n",
    "        return res\n",
    "        \n",
    "    @dispatch\n",
    "    def __call__(self, text:str)->int:\n",
    "        return self._vocab[text]\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return(self._vocab)\n",
    "    \n",
    "    @dispatch\n",
    "    def inverse(self, idx:int)->str:\n",
    "        return self._vocab.get_itos()[idx]\n",
    "\n",
    "    @dispatch\n",
    "    def inverse(self, indices:List[int])->List[str]:\n",
    "        return [self._vocab.get_itos()[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tok \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# In the case of agnews, dataset is: [(index, text)]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoken_iterator\u001b[39m(data_iter:Iterable)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39mIterable:\n",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m, in \u001b[0;36mTokenizer.__init__\u001b[0;34m(self, backend, language, bos, eos)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharacter_tokenizer\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer\u001b[49m(backend, language\u001b[38;5;241m=\u001b[39mlanguage)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbos \u001b[38;5;241m=\u001b[39m bos\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meos \u001b[38;5;241m=\u001b[39m eos\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "tok = Tokenizer()\n",
    "# In the case of agnews, dataset is: [(index, text)]\n",
    "def token_iterator(data_iter:Iterable)->Iterable:\n",
    "    for _, text in data_iter:\n",
    "        yield tok(text)\n",
    "tok_it= token_iterator(ds)\n",
    "# initialize numericalizer based on token iterator\n",
    "num = Numericalizer(tok_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(num('<pad>'), num('<unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(num.vocab['the'])\n",
    "# print(num('the'))\n",
    "# print(num(['<bos>', '<pad>', '<unk>', 'a', 'this', 'the', 'lkjsdf']))\n",
    "# print(num.inverse(0))\n",
    "# print(num.inverse([6,55]))\n",
    "# print(num([['<bos>', '<pad>'], ['<unk>', 'a', 'this', 'the', 'lkjsdf']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tok([\"here we go. asdflkj\", \"it was time...\"])\n",
    "# print(tokens)\n",
    "# print([num(tok) for tok in tokens])\n",
    "# print(num(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face tokenizers\n",
    "https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
