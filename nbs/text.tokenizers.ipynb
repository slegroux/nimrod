{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from phonemizer.backend.espeak.language_switch import LanguageSwitch\n",
    "from phonemizer.backend.espeak.words_mismatch import WordMismatch\n",
    "from phonemizer.punctuation import Punctuation\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader\n",
    "from multipledispatch import dispatch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Phonemizer():\n",
    "    def __init__(self,\n",
    "        separator=Separator(word=\" \", syllable=\"|\", phone=None), # separator\n",
    "        language='en-us', # language\n",
    "        backend='espeak', # phonemization backend (espeak)\n",
    "        strip=True, # strip\n",
    "        preserve_punctuation=True # preserve punctuation\n",
    "        ):\n",
    "        self.separator = separator\n",
    "        self.language = language\n",
    "        self.backend = backend\n",
    "        self.strip = strip\n",
    "        self.preserve_punctuation = preserve_punctuation\n",
    "    \n",
    "    def __call__(self, text, n_jobs=1):\n",
    "        return(\n",
    "            phonemize(\n",
    "                text,\n",
    "                language=self.language,\n",
    "                backend=self.backend,\n",
    "                separator=self.separator,\n",
    "                strip=self.strip,\n",
    "                preserve_punctuation=self.preserve_punctuation,\n",
    "                njobs=n_jobs\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oʊ dɪɹ! ðɪs sʌk...\n",
      "wiːl biː faɪn!\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "print(p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Oh Dear This suck We'll be fine\n",
      "words: {'b', 'd', 't', 'o', 'w', 'h', 'e', 'u', 'l', 'n', \"'\", 'c', 'r', 'i', 'a', 'k', 'f', 's'}\n",
      "lexicon:  {'b': 'b iː', 'd': 'd iː', 't': 't iː', 'o': 'oʊ', 'w': 'd ʌ b əl j uː', 'h': 'eɪ tʃ', 'e': 'iː', 'u': 'j uː', 'l': 'ɛ l', 'n': 'ɛ n', \"'\": '', 'c': 's iː', 'r': 'ɑːɹ', 'i': 'aɪ', 'a': 'eɪ', 'k': 'k eɪ', 'f': 'ɛ f', 's': 'ɛ s'}\n",
      "oʊ dɪɹ ðɪs sʌk wiːl biː faɪn\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "text = Punctuation(';:,.!\"?()-').remove(text)\n",
    "print(\"text:\", text)\n",
    "words = {w.lower() for line in text for w in line.strip().split(' ') if w}\n",
    "print(\"words:\", words)\n",
    "# initialize the espeak backend for English\n",
    "backend = EspeakBackend('en-us')\n",
    "\n",
    "# separate phones by a space and ignoring words boundaries\n",
    "separator = Separator(phone=' ', word=None)\n",
    "# build the lexicon by phonemizing each word one by one. The backend.phonemize\n",
    "# function expect a list as input and outputs a list.\n",
    "lexicon = {\n",
    "    word: backend.phonemize([word], separator=separator, strip=True)[0]\n",
    "    for word in words}\n",
    "print(\"lexicon: \", lexicon)\n",
    "separator=Separator(word=\" \", syllable=\"|\", phone=None)\n",
    "\n",
    "phn = phonemize(\n",
    "    text,\n",
    "    language='en-us',\n",
    "    backend='espeak',\n",
    "    separator=separator,\n",
    "    strip=True,\n",
    "    preserve_punctuation=True,\n",
    "    njobs=4)\n",
    "print(phn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires download of spacy specific language e.g. `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, backend='spacy', language='en'):\n",
    "        if language == 'en':\n",
    "            language = 'en_core_web_sm'\n",
    "        self.tokenizer = get_tokenizer(backend, language=language)\n",
    "\n",
    "    def __call__(self, text:str):\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    def tokenize_iter(self, data_iter:Iterable):\n",
    "        for _, text in data_iter:\n",
    "            yield self.tokenizer(text)\n",
    "\n",
    "    def inverse(self, tokens:List[int]):\n",
    "        # TODO: take care of white spaces\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, yeah\n",
      " I don't know dude...\n",
      "['Oh', ',', 'yeah', '\\n ', 'I', 'do', \"n't\", 'know', 'dude', '...']\n",
      "Oh , yeah \n",
      "  I do n't know dude ...\n",
      "(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n",
      "['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "s = \"Oh, yeah\\n I don't know dude...\"\n",
    "tokenized = tok(s)\n",
    "print(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "print(sample)\n",
    "tokenized_ds = tok.tokenize_iter(ds)\n",
    "sample = next(iter(tokenized_ds))\n",
    "print(sample)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: add more special characters\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokenizer:Tokenizer, data_iter:Iterable, specials=[\"<unk>\"]):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._vocab = self.build_map_from_iter(data_iter, specials=specials)\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials = [\"<unk>\"]):\n",
    "        self._vocab = build_vocab_from_iterator(self._tokenizer.tokenize_iter(data_iter), specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab\n",
    "\n",
    "    @dispatch(list, type=torch.LongTensor)\n",
    "    def __call__(self, texts:List[str], type=torch.LongTensor)->List[List[int]]:\n",
    "        return [type(self._vocab(self._tokenizer(text))) for text in texts]\n",
    "        \n",
    "    @dispatch(str)\n",
    "    def __call__(self, text:str)->List[int]:\n",
    "        return self._vocab(self._tokenizer(text))\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return(self._vocab)\n",
    "    \n",
    "    def inverse(self, indices:List[int]):\n",
    "        return self._tokenizer.inverse([self._vocab.get_itos()[i] for i in indices])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "num = Numericalizer(tok, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 531, 1037,  307,    3,    0]), tensor([31, 37, 98, 64])]\n",
      "[55, 24, 31]\n",
      "1\n",
      "the\n",
      "this is it\n"
     ]
    }
   ],
   "source": [
    "print(num([\"here we go. asdflkj\", \"it was time...\"]))\n",
    "print(num(\"this is it\"))\n",
    "vocab = num.vocab\n",
    "print(vocab.get_stoi()['the'])\n",
    "print(vocab.get_itos()[1])\n",
    "print(num.inverse([55, 24, 31]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextCollater:\n",
    "    def __init__(self,\n",
    "                 numericalizer,\n",
    "                 padding_value:int= -1\n",
    "                 ):\n",
    "        self._numericalizer = numericalizer\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts = [row[1] for row in batch]\n",
    "        tokens = self._numericalizer(texts)\n",
    "        text_lens = torch.LongTensor([token.shape[0] for token in tokens])\n",
    "        text_pad = pad_sequence(tokens, batch_first=True, padding_value=self.padding_value)\n",
    "        return text_pad, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collater = TextCollater(num)\n",
    "dl = DataLoader(dataset=ds, batch_size=2, shuffle=True, collate_fn=collater.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 2635,  3278,  9765, 12976,  1101,  1066,    14,    32,    15,    32,\n",
      "             5,    19,  2142,     9,  1183,  2487,    28,  1028,     6, 11123,\n",
      "             5, 23940,  3181,  5429,     4, 15088,   254, 10522,    17,    20,\n",
      "           332,    20, 15957,   109,    94,   785,   516,    17,   129,    42,\n",
      "          1728,     4,  1101,     2,     1,  3181,    22,  2930,    25,    52,\n",
      "             3],\n",
      "        [ 2534, 21995,   207,     5, 12021,  6549,   774,  5396,  9394,  6549,\n",
      "            37,   250,  2201,     5,  2722,     8,   543,    20,    48,  4773,\n",
      "          4857, 10362,    17,     6,   586,   210,   169,  2607,   268,    34,\n",
      "          2022, 10415,   475,     3,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1]]), tensor([51, 34]))\n",
      "tensor([ 2635,  3278,  9765, 12976,  1101,  1066,    14,    32,    15,    32,\n",
      "            5,    19,  2142,     9,  1183,  2487,    28,  1028,     6, 11123,\n",
      "            5, 23940,  3181,  5429,     4, 15088,   254, 10522,    17,    20,\n",
      "          332,    20, 15957,   109,    94,   785,   516,    17,   129,    42,\n",
      "         1728,     4,  1101,     2,     1,  3181,    22,  2930,    25,    52,\n",
      "            3])\n",
      "FDA OKs Scientist Publishing Vioxx Data ( AP ) AP - The Food and Drug Administration has given a whistle - blower scientist permission to publish data indicating that as many as 139,000 people had heart attacks that may be linked to Vioxx , the scientist 's lawyer said Monday .\n",
      "tensor([ 2534, 21995,   207,     5, 12021,  6549,   774,  5396,  9394,  6549,\n",
      "           37,   250,  2201,     5,  2722,     8,   543,    20,    48,  4773,\n",
      "         4857, 10362,    17,     6,   586,   210,   169,  2607,   268,    34,\n",
      "         2022, 10415,   475,     3])\n",
      "Wenger Spares Red - Faced Lehmann Arsenal keeper Jens Lehmann was left red - faced in Athens as two costly mistakes ensured that a Champions League victory slipped through his sides fingers again .\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "print(b)\n",
    "tokens, lens = b[0], b[1]\n",
    "for token, len in zip(tokens, lens):\n",
    "    print(token[:len])\n",
    "    print(num.inverse(token[:len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
