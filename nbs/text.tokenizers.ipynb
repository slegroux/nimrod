{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#| export\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m EspeakBackend\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage_switch\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageSwitch\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwords_mismatch\u001b[39;00m \u001b[39mimport\u001b[39;00m WordMismatch\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/__init__.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m     sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m     34\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPartial import of phonemizer during the build process.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mphonemize\u001b[39;00m \u001b[39mimport\u001b[39;00m phonemize  \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/phonemize.py:30\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Union, List, Pattern\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Literal\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m BACKENDS\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseBackend\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage_switch\u001b[39;00m \u001b[39mimport\u001b[39;00m LanguageSwitch\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/backend/__init__.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Multilingual text to phonemes converter\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m \u001b[39mimport\u001b[39;00m EspeakBackend\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmbrola\u001b[39;00m \u001b[39mimport\u001b[39;00m EspeakMbrolaBackend\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfestival\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfestival\u001b[39;00m \u001b[39mimport\u001b[39;00m FestivalBackend\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/backend/espeak/espeak.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Tuple, List, Union, Pattern\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEspeakBackend\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlanguage_switch\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     get_language_switch_processor, LanguageSwitch, BaseLanguageSwitch)\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwords_mismatch\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     get_words_mismatch_processor, WordMismatch, BaseWordsMismatch)\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/backend/espeak/base.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Union, Pattern\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseBackend\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mespeak\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m EspeakWrapper\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger\n",
      "File \u001b[0;32m~/anaconda3/envs/nimrod/lib/python3.9/site-packages/phonemizer/backend/base.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlogging\u001b[39;00m \u001b[39mimport\u001b[39;00m Logger\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, List, Any, Dict, Tuple, Union, Pattern\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mphonemizer\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpunctuation\u001b[39;00m \u001b[39mimport\u001b[39;00m Punctuation\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from phonemizer.backend.espeak.language_switch import LanguageSwitch\n",
    "from phonemizer.backend.espeak.words_mismatch import WordMismatch\n",
    "from phonemizer.punctuation import Punctuation\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer import phonemize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Phonemizer():\n",
    "    def __init__(self,\n",
    "        separator=Separator(word=\" \", syllable=\"|\", phone=None), # separator\n",
    "        language='en-us', # language\n",
    "        backend='espeak', # phonemization backend (espeak)\n",
    "        strip=True, # strip\n",
    "        preserve_punctuation=True # preserve punctuation\n",
    "        ):\n",
    "        self.separator = separator\n",
    "        self.language = language\n",
    "        self.backend = backend\n",
    "        self.strip = strip\n",
    "        self.preserve_punctuation = preserve_punctuation\n",
    "    \n",
    "    def __call__(self, text, n_jobs=1):\n",
    "        return(\n",
    "            phonemize(\n",
    "                text,\n",
    "                language=self.language,\n",
    "                backend=self.backend,\n",
    "                separator=self.separator,\n",
    "                strip=self.strip,\n",
    "                preserve_punctuation=self.preserve_punctuation,\n",
    "                njobs=n_jobs\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oʊ dɪɹ! ðɪs sʌk...\n",
      "wiːl biː faɪn!\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "print(p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Oh Dear This suck We'll be fine\n",
      "words: {'i', 'o', 'f', 'd', 'h', \"'\", 'w', 'n', 'c', 'b', 't', 'r', 'e', 's', 'l', 'u', 'k', 'a'}\n",
      "lexicon:  {'i': 'aɪ', 'o': 'oʊ', 'f': 'ɛ f', 'd': 'd iː', 'h': 'eɪ tʃ', \"'\": '', 'w': 'd ʌ b əl j uː', 'n': 'ɛ n', 'c': 's iː', 'b': 'b iː', 't': 't iː', 'r': 'ɑːɹ', 'e': 'iː', 's': 'ɛ s', 'l': 'ɛ l', 'u': 'j uː', 'k': 'k eɪ', 'a': 'eɪ'}\n",
      "oʊ dɪɹ ðɪs sʌk wiːl biː faɪn\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "text = Punctuation(';:,.!\"?()-').remove(text)\n",
    "print(\"text:\", text)\n",
    "words = {w.lower() for line in text for w in line.strip().split(' ') if w}\n",
    "print(\"words:\", words)\n",
    "# initialize the espeak backend for English\n",
    "backend = EspeakBackend('en-us')\n",
    "\n",
    "# separate phones by a space and ignoring words boundaries\n",
    "separator = Separator(phone=' ', word=None)\n",
    "# build the lexicon by phonemizing each word one by one. The backend.phonemize\n",
    "# function expect a list as input and outputs a list.\n",
    "lexicon = {\n",
    "    word: backend.phonemize([word], separator=separator, strip=True)[0]\n",
    "    for word in words}\n",
    "print(\"lexicon: \", lexicon)\n",
    "separator=Separator(word=\" \", syllable=\"|\", phone=None)\n",
    "\n",
    "phn = phonemize(\n",
    "    text,\n",
    "    language='en-us',\n",
    "    backend='espeak',\n",
    "    separator=separator,\n",
    "    strip=True,\n",
    "    preserve_punctuation=True,\n",
    "    njobs=4)\n",
    "print(phn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requires download of spacy specific lang `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torchtext\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import spacy\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from typing import Iterable\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, backend='spacy', language='en'):\n",
    "        self.tokenizer = get_tokenizer(backend, language=language)\n",
    "        self.counter = Counter()\n",
    "        self._vocab = None\n",
    "\n",
    "    def __call__(self, text:str):\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    def tokenize_iter(self, data_iter:Iterable):\n",
    "        for _, text in data_iter:\n",
    "            yield self.tokenizer(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokenizer:Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._vocab = None\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials = [\"<unk>\"]):\n",
    "        self._vocab = build_vocab_from_iterator(self.tokenizer.tokenize_iter(data_iter), specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: collate text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syl20/anaconda3/envs/nimrod/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n",
      "['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tokenized = tok(\"Oh, yeah\\n I don't know dude...\")\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds))\n",
    "print(sample)\n",
    "tokenized_ds = tok.tokenize_iter(ds)\n",
    "sample = next(iter(tokenized_ds))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[531, 1037, 307, 3, 0]\n",
      "[7808, 2, 0, 0, 296, 378, 255, 1324, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "num = Numericalizer(tok)\n",
    "mapper = num.build_map_from_iter(ds)\n",
    "print(mapper[\"<unk>\"])\n",
    "print(mapper(tok(\"here we go. asdflkj\")))\n",
    "# text_pipeline = lambda x: voc(tokenizer(x))\n",
    "print(mapper(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 531, 1037,  307,    3,    0,    0,    0,    0,    0,    0],\n",
      "        [7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64]])\n"
     ]
    }
   ],
   "source": [
    "a = mapper(tok(\"here we go. asdflkj\"))\n",
    "# print(a.shape)\n",
    "b = mapper(tok(\"Oh, yeah\\n I don't know dude...\"))\n",
    "mini_batch = [a, b]\n",
    "x = [torch.LongTensor(x_i) for x_i in mini_batch]\n",
    "x_padded = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "print(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_collate(batch):\n",
    "    xx = batch\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n",
    "    return xx_pad, x_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 531, 1037,  307,    3,    0,    0,    0,    0,    0,    0],\n",
       "         [7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64]]),\n",
       " [5, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_collate(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
