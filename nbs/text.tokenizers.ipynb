{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import SGD\n",
    "\n",
    "# torchtext\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set, Iterable\n",
    "from collections import Counter, OrderedDict\n",
    "from dataclasses import dataclass, asdict\n",
    "from plum import dispatch\n",
    "\n",
    "# nimrod\n",
    "from nimrod.models.lm import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, backend='spacy', language='en'):\n",
    "        if language == 'en':\n",
    "            language = 'en_core_web_sm'\n",
    "        self.tokenizer = get_tokenizer(backend, language=language)\n",
    "\n",
    "    @dispatch\n",
    "    def __call__(self, text:str)->List[str]:\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[str])->List[List[str]]:\n",
    "        return [self.tokenizer(text) for text in texts]\n",
    "    \n",
    "    @dispatch # to replace Iterable\n",
    "    # works with agnews type of dataset [(index, text)]\n",
    "    def __call__(self, data_iter:Iterable)->Iterable:\n",
    "        for _, text in data_iter:\n",
    "            yield self.tokenizer(text)\n",
    "\n",
    "    @dispatch    \n",
    "    def inverse(self, tokens:List[str])->str:\n",
    "        # TODO: take care of white spaces\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    @dispatch\n",
    "    def inverse(self, list_of_tokens:List[List[str]])->List[str]:\n",
    "        s = []\n",
    "        for tokens in list_of_tokens:\n",
    "            s.append(' '.join(tokens)) \n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, yeah I don't know dude...\n",
      "['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...']\n",
      "Oh , yeah I do n't know dude ...\n",
      "[['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...'], ['this', 'is', 'a', 'test']]\n",
      "[\"Oh , yeah I do n't know dude ...\", 'this is a test']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syl20/mambaforge/envs/nimrod/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['The', 'Race', 'is', 'On', ':', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'TORONTO', ',', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', ' ', '#', '36;10', 'million', 'Ansari', 'X', 'Prize', ',', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight', ',', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket', '.']]\n"
     ]
    }
   ],
   "source": [
    "# str -> List[str]\n",
    "s = \"Oh, yeah I don't know dude...\"\n",
    "tokenized = tok(s)\n",
    "print(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "\n",
    "# List[str]->List[List[str]]\n",
    "s = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\n",
    "tokenized = tok(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "\n",
    "# Iterable -> Iterable\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "# print(sample)\n",
    "it = tok(ds)\n",
    "tokens = [token for token in it]\n",
    "print(tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# TODO: add more special characters\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokens_iter:Iterable, specials=[\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]):\n",
    "        self._vocab = self.build_map_from_iter(tokens_iter, specials)\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials=None):\n",
    "        self._vocab = torchtext.vocab.build_vocab_from_iterator(data_iter, specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab\n",
    "\n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[str])->List[List[int]]:\n",
    "        # TODO: check self._vocab has been built\n",
    "        return [self._vocab[text] for text in texts]\n",
    "    \n",
    "    @dispatch\n",
    "    def __call__(self, texts:List[List[str]]):\n",
    "        # TODO: use nested list comprehension\n",
    "        res = []\n",
    "        for row in texts:\n",
    "            res.append([self._vocab[text] for text in row])\n",
    "        return res\n",
    "        \n",
    "    @dispatch\n",
    "    def __call__(self, text:str)->int:\n",
    "        return self._vocab[text]\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return(self._vocab)\n",
    "    \n",
    "    @dispatch\n",
    "    def inverse(self, idx:int)->str:\n",
    "        return self._vocab.get_itos()[idx]\n",
    "\n",
    "    @dispatch\n",
    "    def inverse(self, indices:List[int])->List[str]:\n",
    "        return [self._vocab.get_itos()[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "# In the case of agnews, dataset is: [(index, text)]\n",
    "def token_iterator(data_iter:Iterable)->Iterable:\n",
    "    for _, text in data_iter:\n",
    "        yield tok(text)\n",
    "tok_it= token_iterator(ds)\n",
    "# initialize numericalizer based on token iterator\n",
    "num = Numericalizer(tok_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "print(num('<pad>'), num('<unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "[2, 0, 1, 9, 58, 4, 1]\n",
      "<pad>\n",
      "['.', 'Monday']\n",
      "[[2, 0], [1, 9, 58, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(num.vocab['the'])\n",
    "print(num('the'))\n",
    "print(num(['<bos>', '<pad>', '<unk>', 'a', 'this', 'the', 'lkjsdf']))\n",
    "print(num.inverse(0))\n",
    "print(num.inverse([6,55]))\n",
    "print(num([['<bos>', '<pad>'], ['<unk>', 'a', 'this', 'the', 'lkjsdf']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'we', 'go', '.', 'asdflkj'], ['it', 'was', 'time', '...']]\n",
      "[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n",
      "[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n"
     ]
    }
   ],
   "source": [
    "tokens = tok([\"here we go. asdflkj\", \"it was time...\"])\n",
    "print(tokens)\n",
    "print([num(tok) for tok in tokens])\n",
    "print(num(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
