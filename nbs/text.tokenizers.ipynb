{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from phonemizer.backend import EspeakBackend\n",
    "from phonemizer.backend.espeak.language_switch import LanguageSwitch\n",
    "from phonemizer.backend.espeak.words_mismatch import WordMismatch\n",
    "from phonemizer.punctuation import Punctuation\n",
    "from phonemizer.separator import Separator\n",
    "from phonemizer import phonemize\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Phonemizer():\n",
    "    def __init__(self,\n",
    "        separator=Separator(word=\" \", syllable=\"|\", phone=None), # separator\n",
    "        language='en-us', # language\n",
    "        backend='espeak', # phonemization backend (espeak)\n",
    "        strip=True, # strip\n",
    "        preserve_punctuation=True # preserve punctuation\n",
    "        ):\n",
    "        self.separator = separator\n",
    "        self.language = language\n",
    "        self.backend = backend\n",
    "        self.strip = strip\n",
    "        self.preserve_punctuation = preserve_punctuation\n",
    "    \n",
    "    def __call__(self, text, n_jobs=1):\n",
    "        return(\n",
    "            phonemize(\n",
    "                text,\n",
    "                language=self.language,\n",
    "                backend=self.backend,\n",
    "                separator=self.separator,\n",
    "                strip=self.strip,\n",
    "                preserve_punctuation=self.preserve_punctuation,\n",
    "                njobs=n_jobs\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oʊ dɪɹ! ðɪs sʌk...\n",
      "wiːl biː faɪn!\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "print(p(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Oh Dear This suck We'll be fine\n",
      "words: {'i', 'o', 'f', 'd', 'h', \"'\", 'w', 'n', 'c', 'b', 't', 'r', 'e', 's', 'l', 'u', 'k', 'a'}\n",
      "lexicon:  {'i': 'aɪ', 'o': 'oʊ', 'f': 'ɛ f', 'd': 'd iː', 'h': 'eɪ tʃ', \"'\": '', 'w': 'd ʌ b əl j uː', 'n': 'ɛ n', 'c': 's iː', 'b': 'b iː', 't': 't iː', 'r': 'ɑːɹ', 'e': 'iː', 's': 'ɛ s', 'l': 'ɛ l', 'u': 'j uː', 'k': 'k eɪ', 'a': 'eɪ'}\n",
      "oʊ dɪɹ ðɪs sʌk wiːl biː faɪn\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "text = Punctuation(';:,.!\"?()-').remove(text)\n",
    "print(\"text:\", text)\n",
    "words = {w.lower() for line in text for w in line.strip().split(' ') if w}\n",
    "print(\"words:\", words)\n",
    "# initialize the espeak backend for English\n",
    "backend = EspeakBackend('en-us')\n",
    "\n",
    "# separate phones by a space and ignoring words boundaries\n",
    "separator = Separator(phone=' ', word=None)\n",
    "# build the lexicon by phonemizing each word one by one. The backend.phonemize\n",
    "# function expect a list as input and outputs a list.\n",
    "lexicon = {\n",
    "    word: backend.phonemize([word], separator=separator, strip=True)[0]\n",
    "    for word in words}\n",
    "print(\"lexicon: \", lexicon)\n",
    "separator=Separator(word=\" \", syllable=\"|\", phone=None)\n",
    "\n",
    "phn = phonemize(\n",
    "    text,\n",
    "    language='en-us',\n",
    "    backend='espeak',\n",
    "    separator=separator,\n",
    "    strip=True,\n",
    "    preserve_punctuation=True,\n",
    "    njobs=4)\n",
    "print(phn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires download of spacy specific language e.g. `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from collections import Counter\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from typing import Iterable\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, backend='spacy', language='en'):\n",
    "        self.tokenizer = get_tokenizer(backend, language=language)\n",
    "        self.counter = Counter()\n",
    "        self._vocab = None\n",
    "\n",
    "    def __call__(self, text:str):\n",
    "        return self.tokenizer(text)\n",
    "    \n",
    "    def tokenize_iter(self, data_iter:Iterable):\n",
    "        for _, text in data_iter:\n",
    "            yield self.tokenizer(text)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Numericalizer():\n",
    "    def __init__(self, tokenizer:Tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self._vocab = None\n",
    "    \n",
    "    def build_map_from_iter(self,data_iter:Iterable, specials = [\"<unk>\"]):\n",
    "        self._vocab = build_vocab_from_iterator(self.tokenizer.tokenize_iter(data_iter), specials=specials)\n",
    "        if \"<unk>\" in specials:\n",
    "            self._vocab.set_default_index(self._vocab[\"<unk>\"])\n",
    "        return self._vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: collate text + add special characters & 0 != unk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syl20/anaconda3/envs/nimrod/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n",
      "['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tokenized = tok(\"Oh, yeah\\n I don't know dude...\")\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "print(sample)\n",
    "tokenized_ds = tok.tokenize_iter(ds)\n",
    "sample = next(iter(tokenized_ds))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[531, 1037, 307, 3, 0]\n",
      "[7808, 2, 0, 0, 296, 378, 255, 1324, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "num = Numericalizer(tok)\n",
    "mapper = num.build_map_from_iter(ds)\n",
    "print(mapper[\"<unk>\"])\n",
    "print(mapper(tok(\"here we go. asdflkj\")))\n",
    "# text_pipeline = lambda x: voc(tokenizer(x))\n",
    "print(mapper(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "[tensor([ 531, 1037,  307,    3,    0]), tensor([7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64])]\n",
      "tensor([[ 531, 1037,  307,    3,    0,    0,    0,    0,    0,    0],\n",
      "        [7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64]])\n"
     ]
    }
   ],
   "source": [
    "a = mapper(tok(\"here we go. asdflkj\"))\n",
    "print(len(a))\n",
    "b = mapper(tok(\"Oh, yeah\\n I don't know dude...\"))\n",
    "print(len(b))\n",
    "mini_batch = [a, b]\n",
    "x = [torch.LongTensor(x_i) for x_i in mini_batch]\n",
    "print(x)\n",
    "x_padded = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "print(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_collate(batch):\n",
    "    # batch: [(label, text), ]\n",
    "    # from ipdb import set_trace; set_trace()\n",
    "    texts = [row[1] for row in batch]\n",
    "    tokens = [torch.LongTensor(mapper(tok(row))) for row in texts]\n",
    "    text_lens = [len(token) for token in tokens]\n",
    "    text_pad = pad_sequence(tokens, batch_first=True, padding_value=-1)\n",
    "    return text_pad, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset=ds, batch_size=5, shuffle=True, collate_fn=text_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[11991, 29434, 24121, 10187,  2896,   147,     6,   238,    17,    28,\n",
      "            81,  4305,    20,    12,    13, 16249,    13,    76,    27,    26,\n",
      "             1,  2065,  3871,     7, 17562,     2, 11991,  1048,    24,     4,\n",
      "           885,     1, 11025,     7, 10187,    20,     6,  5517,     3,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [23259,    11,     6, 17457,  1195, 12240,     6,  9439, 13543,     1,\n",
      "          2814,  2657,     7,  6523,  4490,    22, 15031,    82,  1383,    21,\n",
      "         21972,    22,     2, 14020, 23182,    11,     1,  4490,   907,     2,\n",
      "            56,    72,  1076,     1,  2196,     5,  2173, 10377,     3,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [ 3891,  1535,   165,    18,   151,  3243,  1111,  3891,    28,  1295,\n",
      "           139,    11,   161,    43,   145,     5,  1143,  4333,    11,  1545,\n",
      "             4,   206,  7221,    50,    23,  1592,   383,   919,     3,  3891,\n",
      "         22706,     3,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [ 9843,   922, 11719,  3058,   464,     5,   274,    14,    32,    15,\n",
      "            32,     5,   922,  2454,  5026,  5014,   214,    34,    47,   703,\n",
      "          1902,     2,     6,    48,     5,   203,   502,     2,     9,  2692,\n",
      "            65,   203,     8,   413,  1651,     4,   644,   687,     8,     1,\n",
      "          5529,     7,     1,  1773,  2040,     5,  1235,  5943,    18,    35,\n",
      "           464,     5,   274,  4055,     7,     1,  2136,  3058,    10,    52,\n",
      "             3],\n",
      "        [19542,  4688,  1316,    50,   661, 19541, 10660,  5047,  4688,  1813,\n",
      "            63,    87,    10,    34,   173,  1315,     4,  2205,    34,  3847,\n",
      "          1308,  1235,     3,  4688,     2,    73,  2182,     1,   173,   311,\n",
      "             7,     1,  3012,   776,  2775,    11,  4983,     9, 14848,     6,\n",
      "         29237,  1717,     4,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1]]), [39, 39, 32, 61, 43])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.Tensor([17564, 24659, 29258, 26399,    13, 16230,  1350,  1321,     4,     6,\n",
    "          1649,    20,  1471,     7,   386, 10675,     6,   784,   648,     8,\n",
    "          1734,    58,     1,   351,  4033,     7,   315,   217,  4224,  2494,\n",
    "            13,    16,  2880,     5,   363,   105,     3,    -1,    -1,    -1,\n",
    "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "            -1,    -1,    -1,    -1,    -1,    -1])!= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
