{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "> Simple feedforward Multilayer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from lightning import LightningModule\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.image.datasets import ImageDataset, MNISTDataModule\n",
    "from nimrod.utils import logger\n",
    "from nimrod.models.core import Classifier\n",
    "# torch.set_num_interop_threads(1)\n",
    "\n",
    "# from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/image/datasets.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MNISTDataModule\n",
       "\n",
       ">      MNISTDataModule (data_dir:str='~/Data/',\n",
       ">                       train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n",
       ">                       batch_size:int=64, num_workers:int=0,\n",
       ">                       pin_memory:bool=False, persistent_workers:bool=False)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_dir | str | ~/Data/ | path to source data dir |\n",
       "| train_val_test_split | List | [0.8, 0.1, 0.1] | train val test % |\n",
       "| batch_size | int | 64 | size of compute batch |\n",
       "| num_workers | int | 0 | num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow |\n",
       "| pin_memory | bool | False | If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer |\n",
       "| persistent_workers | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/image/datasets.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MNISTDataModule\n",
       "\n",
       ">      MNISTDataModule (data_dir:str='~/Data/',\n",
       ">                       train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n",
       ">                       batch_size:int=64, num_workers:int=0,\n",
       ">                       pin_memory:bool=False, persistent_workers:bool=False)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_dir | str | ~/Data/ | path to source data dir |\n",
       "| train_val_test_split | List | [0.8, 0.1, 0.1] | train val test % |\n",
       "| batch_size | int | 64 | size of compute batch |\n",
       "| num_workers | int | 0 | num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow |\n",
       "| pin_memory | bool | False | If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer |\n",
       "| persistent_workers | bool | False |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MNISTDataModule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                n_in:int=784, # input dimension e.g. (H,W) for image\n",
    "                n_h:int=64, # hidden dimension\n",
    "                n_out:int=10, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2,\n",
    "                **kwargs\n",
    "                ) -> None:\n",
    "        logger.info(\"MLP initi: n_in: {}, n_h: {}, n_out: {}, dropout: {}\".format(n_in, n_h, n_out, dropout))\n",
    "        super().__init__(**kwargs)\n",
    "        l1 = nn.Linear(n_in, n_h)\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        relu = nn.ReLU()\n",
    "        l2 = nn.Linear(n_h, n_out)\n",
    "        self.layers = nn.Sequential(l1, dropout, relu, l2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor # dim (B, H*W)\n",
    "                ) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/models/mlp.py#L30){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP\n",
       "\n",
       ">      MLP (n_in:int=784, n_h:int=64, n_out:int=10, dropout:float=0.2, **kwargs)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_in | int | 784 | input dimension e.g. (H,W) for image |\n",
       "| n_h | int | 64 | hidden dimension |\n",
       "| n_out | int | 10 | output dimension (= number of classes for classification) |\n",
       "| dropout | float | 0.2 |  |\n",
       "| kwargs |  |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/models/mlp.py#L30){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MLP\n",
       "\n",
       ">      MLP (n_in:int=784, n_h:int=64, n_out:int=10, dropout:float=0.2, **kwargs)\n",
       "\n",
       "*Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| n_in | int | 784 | input dimension e.g. (H,W) for image |\n",
       "| n_h | int | 64 | hidden dimension |\n",
       "| n_out | int | 10 | output dimension (= number of classes for classification) |\n",
       "| dropout | float | 0.2 |  |\n",
       "| kwargs |  |  |  |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:27:34,175 - INFO - MLP initi: n_in: 784, n_h: 64, n_out: 10, dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand((5, 28*28))\n",
    "mlp = MLP(n_in=28*28, n_h=64, n_out=10)\n",
    "out = mlp(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic training\n",
    "#### Data Module\n",
    "Data module\n",
    "c.f. recipes/image/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cat ../config/image/data/mnist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'nimrod.image.datasets.MNISTDataModule', 'data_dir': '../data/image', 'train_val_test_split': [0.8, 0.1, 0.1], 'batch_size': 64, 'num_workers': 1, 'pin_memory': False, 'persistent_workers': False}\n",
      "7000\n",
      "original shape (C,H,W):  torch.Size([1, 28, 28])\n",
      "reshape (C,HxW):  torch.Size([1, 784])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# load from config file\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "print(cfg.datamodule)\n",
    "# cfg.datamodule.num_workers = 1\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "x = datamodule.data_test[0][0] # (C, H, W)\n",
    "print(len(datamodule.data_test))\n",
    "label = datamodule.data_test[0][1] #(int)\n",
    "print(\"original shape (C,H,W): \", x.shape)\n",
    "print(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default Pytorch datasets\n",
    "train_dataset = MNIST(\"../data/image\", train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(\"../data/image\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# using nimrod datamodule\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datamodule.data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardware acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\" # for CI on cpu instance\n",
    "device = torch.device(device)\n",
    "model = mlp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss & optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%time\n",
    "# n_epochs = 1\n",
    "# for epoch in range(n_epochs):\n",
    "#     model.train()\n",
    "#     for images, labels in train_loader:\n",
    "#         images = images.view(-1, 28*28)\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         correct = 0\n",
    "#         total = 0\n",
    "#         for images, labels in test_loader:\n",
    "#             # model expects input (B,H*W)\n",
    "#             images = images.view(-1, 28*28).to(device)\n",
    "#             images = images.to(device)\n",
    "#             labels = labels.to(device)\n",
    "#             # Pass the input through the model\n",
    "#             outputs = model(images)\n",
    "#             # Get the predicted labels\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#             # Update the total and correct counts\n",
    "#             total += labels.size(0)\n",
    "#             correct += (predicted == labels).sum()\n",
    "\n",
    "#         # Print the accuracy\n",
    "#         print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated model + training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MLP_X(Classifier, MLP, LightningModule):\n",
    "    def __init__(self,\n",
    "                n_in:int, # input dimension e.g. (H,W) for image\n",
    "                n_h:int, # hidden dimension\n",
    "                n_out:int, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2, # dropout\n",
    "                lr:float=1e-3 # learning rate\n",
    "        ):\n",
    "        \n",
    "        logger.info(\"MLP_PL init: n_in: {}, n_h: {}, n_out: {}, dropout: {}, lr: {}\".format(n_in, n_h, n_out, dropout, lr))        \n",
    "        super().__init__(num_classes=n_out, lr=lr, n_in=n_in, n_h=n_h, n_out=n_out, dropout=dropout)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:27:34,505 - INFO - MLP_PL init: n_in: 784, n_h: 128, n_out: 10, dropout: 0.1, lr: 0.001\n",
      "2024-12-13 09:27:34,505 - INFO - Classifier init: num_classes: 10, lr: 0.001\n",
      "2024-12-13 09:27:34,506 - INFO - MLP initi: n_in: 784, n_h: 128, n_out: 10, dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[__main__.MLP_X,\n",
       " nimrod.core.Classifier,\n",
       " abc.ABC,\n",
       " __main__.MLP,\n",
       " lightning.pytorch.core.module.LightningModule,\n",
       " lightning.fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin,\n",
       " lightning.pytorch.core.mixins.hparams_mixin.HyperparametersMixin,\n",
       " lightning.pytorch.core.hooks.ModelHooks,\n",
       " lightning.pytorch.core.hooks.DataHooks,\n",
       " lightning.pytorch.core.hooks.CheckpointHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pl = MLP_X(n_in=28*28, n_h=128, n_out=10, dropout=0.1, lr=1e-3)\n",
    "MLP_X.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "\n",
    "# class MLP_PL(MLP, LightningModule):\n",
    "#     def __init__(self,\n",
    "#                 n_in:int, # input dimension e.g. (H,W) for image\n",
    "#                 n_h:int, # hidden dimension\n",
    "#                 n_out:int, # output dimension (= number of classes for classification)\n",
    "#                 dropout:float=0.2, # dropout factor\n",
    "#                 lr:float=1e-3, # learning rate\n",
    "#                 ):\n",
    "\n",
    "#         super().__init__(n_in, n_h, n_out, dropout)\n",
    "\n",
    "#         self.save_hyperparameters()\n",
    "#         self.loss = nn.CrossEntropyLoss()\n",
    "#         self.accuracy = Accuracy(task=\"multiclass\", num_classes=n_out)\n",
    "#         self.lr = lr\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "#         return optimizer\n",
    "    \n",
    "#     def _step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         y_hat = self.forward(x)\n",
    "#         loss = self.loss(y_hat, y)\n",
    "#         acc = self.accuracy(y_hat, y)\n",
    "#         return loss, acc\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         loss, acc = self._step(batch, batch_idx)\n",
    "#         metrics = {\"train/loss\": loss, \"train/acc\": acc}\n",
    "#         self.log_dict(metrics, on_epoch=True)\n",
    "#         return loss\n",
    "    \n",
    "#     def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "#         loss, acc = self._step(batch, batch_idx)\n",
    "#         metrics = {\"val/loss\":loss, \"val/acc\": acc}\n",
    "#         self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "#     def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "#         loss, acc = self._step(batch, batch_idx)\n",
    "#         metrics = {\"test/loss\":loss, \"test/acc\": acc}\n",
    "#         self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "\n",
    "#     def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "#         x, y = batch\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         y_hat = self.forward(x)\n",
    "#         return y_hat.argmax(dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 09:27:34,558 - INFO - MLP_PL init: n_in: 784, n_h: 64, n_out: 10, dropout: 0.2, lr: 0.001\n",
      "2024-12-13 09:27:34,559 - INFO - Classifier init: num_classes: 10, lr: 0.001\n",
      "2024-12-13 09:27:34,559 - INFO - MLP initi: n_in: 784, n_h: 64, n_out: 10, dropout: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "mlp_pl = MLP_X(28*28, 64, 10, dropout=0.2, lr=1e-3)\n",
    "# print(mlp_pl.training_step)\n",
    "# from pprint import pprint \n",
    "# pprint(mlp_pl.__dict__)\n",
    "b = torch.rand((5,1, 28*28))\n",
    "print(mlp_pl(b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "# move model and data to hardware\n",
    "mlp_pl = mlp_pl.to(device)\n",
    "\n",
    "b = b.to(device)\n",
    "y_hat = mlp_pl(b)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py:222: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608632396/work/aten/src/ATen/ParallelNative.cpp:228.)\n",
      "  torch.set_num_threads(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "tensor([7, 2, 8, 2, 2, 7, 2, 2, 2, 2, 2, 2, 2, 3, 5, 9, 5, 2, 2, 2, 4, 2, 2, 8,\n",
      "        4, 2, 6, 2, 7, 2, 4, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 5, 6, 2, 7, 2, 8,\n",
      "        2, 8, 2, 2, 2, 2, 4, 6, 7, 2, 2, 7, 3, 2, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# real data\n",
    "batch = next(iter(test_loader))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "print(mlp_pl.predict_step(batch, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(mlp_pl.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = Trainer(accelerator='mps', devices = 1, max_epochs=1)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scripts with config file \n",
    "\n",
    "To check an example script leveraging model training with configurable yaml files check recipes folder\n",
    "\n",
    "```bash\n",
    "cd recipes/image/mnist\n",
    "python train.py trainer.max_epochs 20 trainer.accelerator='mps' datamodule.num_workers=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
