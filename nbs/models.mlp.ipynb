{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "> Simple feedforward Multilayer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import Accuracy\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.image.datasets import ImageDataset, MNISTDataModule\n",
    "\n",
    "# from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/image/datasets.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MNISTDataModule\n",
       "\n",
       ">      MNISTDataModule (data_dir:str='~/Data/',\n",
       ">                       train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n",
       ">                       batch_size:int=64, num_workers:int=0,\n",
       ">                       pin_memory:bool=False, persistent_workers:bool=False)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_dir | str | ~/Data/ | path to source data dir |\n",
       "| train_val_test_split | List | [0.8, 0.1, 0.1] | train val test % |\n",
       "| batch_size | int | 64 | size of compute batch |\n",
       "| num_workers | int | 0 | num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow |\n",
       "| pin_memory | bool | False | If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer |\n",
       "| persistent_workers | bool | False |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/image/datasets.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### MNISTDataModule\n",
       "\n",
       ">      MNISTDataModule (data_dir:str='~/Data/',\n",
       ">                       train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n",
       ">                       batch_size:int=64, num_workers:int=0,\n",
       ">                       pin_memory:bool=False, persistent_workers:bool=False)\n",
       "\n",
       "*A DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is\n",
       "consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    import lightning.pytorch as L\n",
       "    import torch.utils.data as data\n",
       "    from pytorch_lightning.demos.boring_classes import RandomDataset\n",
       "\n",
       "    class MyDataModule(L.LightningDataModule):\n",
       "        def prepare_data(self):\n",
       "            # download, IO, etc. Useful with shared filesystems\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "            ...\n",
       "\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "            dataset = RandomDataset(1, 100)\n",
       "            self.train, self.val, self.test = data.random_split(\n",
       "                dataset, [80, 10, 10], generator=torch.Generator().manual_seed(42)\n",
       "            )\n",
       "\n",
       "        def train_dataloader(self):\n",
       "            return data.DataLoader(self.train)\n",
       "\n",
       "        def val_dataloader(self):\n",
       "            return data.DataLoader(self.val)\n",
       "\n",
       "        def test_dataloader(self):\n",
       "            return data.DataLoader(self.test)\n",
       "\n",
       "        def on_exception(self, exception):\n",
       "            # clean up state after the trainer faced an exception\n",
       "            ...\n",
       "\n",
       "        def teardown(self):\n",
       "            # clean up state after the trainer stops, delete files...\n",
       "            # called on every process in DDP\n",
       "            ...*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| data_dir | str | ~/Data/ | path to source data dir |\n",
       "| train_val_test_split | List | [0.8, 0.1, 0.1] | train val test % |\n",
       "| batch_size | int | 64 | size of compute batch |\n",
       "| num_workers | int | 0 | num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow |\n",
       "| pin_memory | bool | False | If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer |\n",
       "| persistent_workers | bool | False |  |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MNISTDataModule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                n_in:int, # input dimension e.g. (H,W) for image\n",
    "                n_h:int, # hidden dimension\n",
    "                n_out:int, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        l1 = nn.Linear(n_in, n_h)\n",
    "        l2 = nn.Linear(n_h, n_out)\n",
    "        relu = nn.ReLU()\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.Sequential(l1, dropout, relu, l2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor # dim (B, H*W)\n",
    "                ) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand((5, 28*28))\n",
    "mlp = MLP(n_in=28*28, n_h=64, n_out=10)\n",
    "out = mlp(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic training\n",
    "#### Data Module\n",
    "Data module\n",
    "c.f. recipes/image/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cat ../config/image/data/mnist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'nimrod.image.datasets.MNISTDataModule', 'data_dir': '../data/image', 'train_val_test_split': [0.8, 0.1, 0.1], 'batch_size': 64, 'num_workers': 0, 'pin_memory': False, 'persistent_workers': False}\n",
      "7000\n",
      "original shape (C,H,W):  torch.Size([1, 28, 28])\n",
      "reshape (C,HxW):  torch.Size([1, 784])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# load from config file\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "print(cfg.datamodule)\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "x = datamodule.data_test[0][0] # (C, H, W)\n",
    "print(len(datamodule.data_test))\n",
    "label = datamodule.data_test[0][1] #(int)\n",
    "print(\"original shape (C,H,W): \", x.shape)\n",
    "print(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default Pytorch datasets\n",
    "train_dataset = MNIST(\"../data/image\", train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(\"../data/image\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# using nimrod datamodule\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datamodule.data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardware acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\" # for CI on cpu instance\n",
    "device = torch.device(device)\n",
    "model = mlp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss & optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Accuracy = 73.59%\n",
      "CPU times: user 2.55 s, sys: 739 ms, total: 3.28 s\n",
      "Wall time: 2.7 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            # model expects input (B,H*W)\n",
    "            images = images.view(-1, 28*28).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Pass the input through the model\n",
    "            outputs = model(images)\n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated model + training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP_PL(MLP, LightningModule):\n",
    "    def __init__(self,\n",
    "                n_in:int, # input dimension e.g. (H,W) for image\n",
    "                n_h:int, # hidden dimension\n",
    "                n_out:int, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2, # dropout factor\n",
    "                lr:float=1e-3 # learning rate\n",
    "                ):\n",
    "\n",
    "        super(MLP_PL, self).__init__(n_in, n_h, n_out, dropout)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self._step(batch, batch_idx)\n",
    "        metrics = {\"train/loss\": loss, \"train/acc\": acc}\n",
    "        self.log_dict(metrics, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, acc = self._step(batch, batch_idx)\n",
    "        metrics = {\"val/loss\":loss, \"val/acc\": acc}\n",
    "        self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, acc = self._step(batch, batch_idx)\n",
    "        metrics = {\"test/loss\":loss, \"test/acc\": acc}\n",
    "        self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[__main__.MLP_PL,\n",
       " __main__.MLP,\n",
       " pytorch_lightning.core.module.LightningModule,\n",
       " lightning_fabric.utilities.device_dtype_mixin._DeviceDtypeModuleMixin,\n",
       " pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin,\n",
       " pytorch_lightning.core.hooks.ModelHooks,\n",
       " pytorch_lightning.core.hooks.DataHooks,\n",
       " pytorch_lightning.core.hooks.CheckpointHooks,\n",
       " torch.nn.modules.module.Module,\n",
       " object]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_PL.mro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 10])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "tensor([1, 3, 3, 7, 3, 0, 5, 3, 7, 3, 3, 7, 1, 9, 3, 3, 3, 5, 3, 3, 1, 9, 1, 3,\n",
      "        7, 7, 1, 1, 3, 3, 1, 1, 5, 1, 7, 3, 1, 3, 3, 3, 0, 3, 3, 3, 3, 3, 9, 3,\n",
      "        1, 9, 1, 9, 3, 9, 3, 1, 3, 5, 3, 3, 7, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# wrap simple model in modularized model\n",
    "mlp_pl = MLP_PL(28*28, 64, n_out=10, dropout=0.2, lr=1e-3)\n",
    "\n",
    "# fake input\n",
    "b = torch.rand((5,1, 28*28))\n",
    "\n",
    "# move model and data to hardware\n",
    "model = mlp_pl.to(device)\n",
    "\n",
    "b = b.to(device)\n",
    "y_hat = mlp_pl(b)\n",
    "print(y_hat.shape)\n",
    "\n",
    "# real data\n",
    "batch = next(iter(test_loader))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "print(model.predict_step(batch, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(model.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = Trainer(accelerator='mps', devices = 1, max_epochs=1)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scripts with config file \n",
    "\n",
    "To check an example script leveraging model training with configurable yaml files check recipes folder\n",
    "\n",
    "```bash\n",
    "cd recipes/image/mnist\n",
    "python train.py trainer.max_epochs 20 trainer.accelerator='mps' datamodule.num_workers=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimrod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
