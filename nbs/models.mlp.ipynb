{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "> Simple feedforward Multilayer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syl20/mambaforge/envs/nimrod/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torchmetrics import Accuracy\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nimrod.data.datasets import MNISTDataModule\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.image.datasets import ImageDataset\n",
    "\n",
    "# from IPython.core.debugger import set_trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                n_in:int, # input dimension e.g. (H,W) for image\n",
    "                n_h:int, # hidden dimension\n",
    "                n_out:int, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2\n",
    "                ) -> None:\n",
    "        super().__init__()\n",
    "        l1 = nn.Linear(n_in, n_h)\n",
    "        l2 = nn.Linear(n_h, n_out)\n",
    "        relu = nn.ReLU()\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.Sequential(l1, l2, relu, dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor # dim (B, H*W)\n",
    "                ) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand((5, 28*28))\n",
    "mlp = MLP(n_in=28*28, n_h=64, n_out=10)\n",
    "out = mlp(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic training\n",
    "#### Data Module\n",
    "Data module\n",
    "c.f. recipes/image/mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cat ../config/data/image/mnist.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "original shape (C,H,W):  torch.Size([1, 28, 28])\n",
      "reshape (C,HxW):  torch.Size([1, 784])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# load from config file\n",
    "cfg = OmegaConf.load('../config/data/image/mnist.yaml')\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "x = datamodule.data_test[0][0] # (C, H, W)\n",
    "print(len(datamodule.data_test))\n",
    "label = datamodule.data_test[0][1] #(int)\n",
    "print(\"original shape (C,H,W): \", x.shape)\n",
    "print(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default Pytorch datasets\n",
    "train_dataset = MNIST(\"../data/image\", train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(\"../data/image\", train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# using nimrod datamodule\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataset.Subset"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datamodule.data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hardware acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cpu\" # for CI on cpu instance\n",
    "device = torch.device(device)\n",
    "model = mlp.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss & optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "n_epochs = 1\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28*28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            # model expects input (B,H*W)\n",
    "            images = images.view(-1, 28*28).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Pass the input through the model\n",
    "            outputs = model(images)\n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated model + training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP_PL(LightningModule):\n",
    "    def __init__(self,\n",
    "                n_in:int, # input dimension e.g. (H,W) for image\n",
    "                n_h:int, # hidden dimension\n",
    "                n_out:int, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2, # dropout factor\n",
    "                lr:float=1e-3 # learning rate\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.mlp = MLP(n_in, n_h, n_out, dropout)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.lr = lr\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor, # X input images dim(B, H*W)\n",
    "                ) -> torch.Tensor: # y class probabilities (B, n_classes)\n",
    "        return(self.mlp(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.mlp(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.mlp(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        acc = self.accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, acc = self._step(batch, batch_idx)\n",
    "        metrics = {\"val/loss\":loss, \"val/acc\": acc}\n",
    "        self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, acc = self._step(batch, batch_idx)\n",
    "        metrics = {\"test/loss\":loss, \"test/acc\": acc}\n",
    "        self.log_dict(metrics, on_step=on_step, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.mlp(x)\n",
    "        return y_hat.argmax(dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 10])\n",
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
      "tensor([6, 8, 8, 8, 8, 8, 6, 4, 8, 8, 5, 5, 9, 5, 9, 8, 8, 2, 4, 4, 5, 2, 4, 5,\n",
      "        9, 8, 6, 0, 6, 9, 9, 4, 9, 6, 8, 6, 5, 9, 4, 9, 6, 8, 2, 5, 8, 5, 9, 6,\n",
      "        2, 9, 4, 0, 2, 0, 9, 6, 2, 2, 8, 9, 8, 9, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# wrap simple model in modularized model\n",
    "mlp_pl = MLP_PL(28*28, 64, n_out=10, dropout=0.2, lr=1e-3)\n",
    "\n",
    "# fake input\n",
    "b = torch.rand((5,1, 28*28))\n",
    "\n",
    "# move model and data to hardware\n",
    "model = mlp_pl.to(device)\n",
    "\n",
    "b = b.to(device)\n",
    "y_hat = mlp_pl(b)\n",
    "print(y_hat.shape)\n",
    "\n",
    "# real data\n",
    "batch = next(iter(test_loader))\n",
    "print(batch[0].shape, batch[1].shape)\n",
    "print(model.predict_step(batch, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/syl20/Projects/nimrod/nbs/models.mlp.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/syl20/Projects/nimrod/nbs/models.mlp.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mlr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "trainer = Trainer(accelerator='mps', devices = 1, max_epochs=1)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "trainer.fit(mlp_pl, datamodule.data_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training scripts with config file \n",
    "\n",
    "To check an example script leveraging model training with configurable yaml files check recipes folder\n",
    "\n",
    "```bash\n",
    "cd recipes/image/mnist\n",
    "python train.py trainer.max_epochs 20 trainer.accelerator='mps' datamodule.num_workers=0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
