{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP)\n",
    "\n",
    "> Simple feedforward Multilayer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from nimrod.utils import get_device\n",
    "from nimrod.image.datasets import MNISTDataModule\n",
    "from nimrod.models.core import Classifier\n",
    "# torch.set_num_interop_threads(1)\n",
    "# from IPython.core.debugger import set_trace\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "                self,\n",
    "                n_in:int=784, # input dimension e.g. (H,W) for image\n",
    "                n_h:int=64, # hidden dimension\n",
    "                n_out:int=10, # output dimension (= number of classes for classification)\n",
    "                dropout:float=0.2\n",
    "                ) -> None:\n",
    "        logger.info(\"MLP: init\")\n",
    "        super().__init__()\n",
    "        l1 = nn.Linear(n_in, n_h)\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        relu = nn.ReLU()\n",
    "        l2 = nn.Linear(n_h, n_out)\n",
    "        self.layers = nn.Sequential(l1, dropout, relu, l2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor # dim (B, H*W)\n",
    "                ) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.rand((5, 28*28))\n",
    "mlp = MLP(n_in=28*28, n_h=64, n_out=10, dropout=0.1)\n",
    "out = mlp(image)\n",
    "print(out.shape)\n",
    "cfg = OmegaConf.load('../config/image/model/mlp.yaml')\n",
    "model = instantiate(cfg.nnet)\n",
    "out = model(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from config file\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "x = datamodule.data_test[0][0] # (C, H, W)\n",
    "print(len(datamodule.data_test))\n",
    "label = datamodule.data_test[0][1] #(int)\n",
    "print(\"original shape (C,H,W): \", x.shape)\n",
    "print(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\n",
    "print(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nimrod datamodule\n",
    "train_loader = datamodule.train_dataloader()\n",
    "val_loader = datamodule.val_dataloader()\n",
    "test_loader = datamodule.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\" # for CI on cpu instance\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "%time\n",
    "# data\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "cfg.batch_size = 2048\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "# model\n",
    "model = mlp.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "n_epochs = 2\n",
    "losses = []\n",
    "lrs = []\n",
    "current_step = 0\n",
    "steps_per_epoch = len(datamodule.data_train) // cfg.datamodule.batch_size\n",
    "total_steps = steps_per_epoch * n_epochs\n",
    "print(f\"steps_per_epoch: {steps_per_epoch}, total_steps: {total_steps}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for images, labels in datamodule.train_dataloader():\n",
    "        optimizer.zero_grad()\n",
    "        images = images.view(-1, 28*28)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        lrs.append(current_lr)\n",
    "        if not (current_step % 100):\n",
    "            print(f\"Loss {loss.item():.4f}, Current LR: {current_lr:.10f}, Step: {current_step}/{total_steps}\")\n",
    "        current_step += 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in datamodule.test_dataloader():\n",
    "            # model expects input (B,H*W)\n",
    "            images = images.view(-1, 28*28).to(device)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Pass the input through the model\n",
    "            outputs = model(images)\n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Update the total and correct counts\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "        # Print the accuracy\n",
    "        print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "# plt.figure(1)\n",
    "# plt.subplot(211)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.plot(losses)\n",
    "# plt.subplot(212)\n",
    "# plt.ylabel('lr')\n",
    "# plt.xlabel('step')\n",
    "# plt.plot(lrs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MLP_X(Classifier, LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet:MLP,\n",
    "            num_classes:int,\n",
    "            optimizer:torch.optim.Optimizer,\n",
    "            scheduler:torch.optim.lr_scheduler\n",
    "        ):\n",
    "        \n",
    "        logger.info(\"MLP_X init\")\n",
    "        super().__init__(num_classes, optimizer, scheduler)\n",
    "        self.nnet = nnet\n",
    "        self.save_hyperparameters(logger=False,ignore=['nnet'])\n",
    "        self.lr = optimizer.keywords['lr'] # for lr finder\n",
    "    \n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "    \n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        return loss, preds, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../config/image/model/mlp.yaml')\n",
    "model = instantiate(cfg)\n",
    "b = torch.rand((16,1, 28*28))\n",
    "y = model(b)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nimrod training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cfg = OmegaConf.load('../config/image/model/conv.yaml')\n",
    "model = instantiate(cfg)\n",
    "\n",
    "# data module config\n",
    "cfg = OmegaConf.load('../config/image/data/mnist.yaml')\n",
    "cfg.datamodule.batch_size = 2048\n",
    "cfg.datamodule.num_workers = 0\n",
    "datamodule = instantiate(cfg.datamodule)\n",
    "# datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=3,\n",
    "    logger=CSVLogger(\"logs\", name=\"mnist_mlp\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "csv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\n",
    "metrics = pd.read_csv(csv_path)\n",
    "metrics.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "plt.figure()\n",
    "plt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\n",
    "plt.plot(metrics['step'], metrics['val/loss'],'r.-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
