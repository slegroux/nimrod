{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch_lr_finder import LRFinder\n",
    "from torchinfo import summary\n",
    "from lightning import LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger\n",
    "from lightning.pytorch.tuner.tuning import Tuner\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from nimrod.utils import get_device, set_seed\n",
    "from nimrod.models.core import Classifier\n",
    "\n",
    "from pprint import pprint\n",
    "import logging\n",
    "from typing import List, Optional, Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 64, 3, 16, 16\n",
    "x = torch.randn(B, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x.view(B, C, -1).transpose(1, 2) # B, T, C\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_projection = nn.Linear(C, C)\n",
    "q_projection = nn.Linear(C, C)\n",
    "v_projection = nn.Linear(C, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "k = k_projection(t)\n",
    "q = q_projection(t)\n",
    "v = v_projection(t)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 256, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 64, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(k.shape)\n",
    "k.transpose(1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention layer for image data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of input features (channels).\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Forward pass through the self-attention layer.    \n",
    "    Notes\n",
    "    -----\n",
    "    This implementation uses a single linear layer to project the input tensor\n",
    "    into query, key, and value tensors. The attention mechanism is applied to\n",
    "    these tensors, and the result is projected back to the original feature space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int # number of features\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(n_features)\n",
    "\n",
    "        # TODO: check why GroupNorm is used\n",
    "        # self.norm = nn.GroupNorm(1, n_features)\n",
    "        self.norm = nn.BatchNorm2d(n_features)\n",
    "\n",
    "        # project to focus on specific channel areas\n",
    "        # self.k_projection = nn.Linear(n_features, n_features)\n",
    "        # self.q_projection = nn.Linear(n_features, n_features)\n",
    "        # self.v_projection = nn.Linear(n_features, n_features)\n",
    "\n",
    "        # the 3 previous projections can be replaced by 1 larger projection\n",
    "        self.kqv = nn.Linear(n_features, n_features*3)\n",
    "        self.proj = nn.Linear(n_features, n_features)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, C, H, W = x.shape\n",
    "        inp = x\n",
    "        t = self.norm(x).view(B, C, -1).transpose(1, 2) # B, T=H*W, C\n",
    "        # k = self.k_projection(t)\n",
    "        # q = self.q_projection(t)\n",
    "        # v = self.v_projection(t)\n",
    "        kqv = self.kqv(t)\n",
    "        q, k, v = kqv.chunk(3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        return x + inp #specific to stable diffusion ~resnet style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### SelfAttention\n",
       "\n",
       ">      SelfAttention (n_features:int)\n",
       "\n",
       "*Self-Attention layer for image data.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| n_features | int | number of features |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### SelfAttention\n",
       "\n",
       ">      SelfAttention (n_features:int)\n",
       "\n",
       "*Self-Attention layer for image data.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| n_features | int | number of features |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(SelfAttention, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, C, H, W = 64, 3, 16, 16\n",
    "x = torch.randn(B, C, H, W)\n",
    "sa = SelfAttention(3)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead Attention module.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Number of features.\n",
    "    n_heads : int\n",
    "        Number of heads or groups of features.\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Forward pass of the multihead attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: int, #number of features\n",
    "            n_heads: int #number of heads or groups of features\n",
    "            ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads # number of heads\n",
    "        self.scale = math.sqrt(n_features)\n",
    "        self.norm = nn.BatchNorm2d(n_features)\n",
    "        self.kqv = nn.Linear(n_features, n_features*3)\n",
    "        self.proj = nn.Linear(n_features, n_features)\n",
    "\n",
    "    def forward(self, x: torch.Tensor)->torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        inp = x\n",
    "        t = self.norm(x).view(B, C, -1).transpose(1, 2) # B, T=H*W, C\n",
    "        kqv = self.kqv(t)\n",
    "        # split channels into n_heads and reshape\n",
    "        kqv = rearrange(kqv, 'b t (h d) -> (b h) t d', h=self.n_heads)\n",
    "        q, k, v = kqv.chunk(3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(b h) t d -> b t (h d)', h=self.n_heads)\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2).view(B, C, H, W)\n",
    "        return x + inp #specific to stable diffusion ~resnet style\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### MultiheadAttention\n",
       "\n",
       ">      MultiheadAttention (n_features:int, n_heads:int)\n",
       "\n",
       "*Multihead Attention module.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| n_features | int | number of features |\n",
       "| n_heads | int | number of heads or groups of features |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### MultiheadAttention\n",
       "\n",
       ">      MultiheadAttention (n_features:int, n_heads:int)\n",
       "\n",
       "*Multihead Attention module.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| n_features | int | number of features |\n",
       "| n_heads | int | number of heads or groups of features |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MultiheadAttention, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B, C, H, W = 64, 32, 16, 16\n",
    "x = torch.randn(B, C, H, W)\n",
    "mha = MultiheadAttention(32, 8)\n",
    "mha(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
