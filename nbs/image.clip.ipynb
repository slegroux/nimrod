{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP\n",
    "\n",
    "> Contrastive Language–Image Pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp image.clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from PIL import Image\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9949, 0.0051]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can \n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_image(images):\n",
    "    if not isinstance(images, list): images = [images]\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad(): return model.get_image_features(**inputs)\n",
    "\n",
    "def embed_text(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad(): return model.get_text_features(**inputs)\n",
    "\n",
    "def normalize(a): return a / a.norm(dim=-1, keepdim=True)\n",
    "\n",
    "def cosine_sim(a, b): return normalize(a) @ normalize(b).T\n",
    "\n",
    "def logits(a, b): return model.logit_scale.exp() * cosine_sim(a, b)\n",
    "\n",
    "def probs(a, b): return logits(a, b).softmax(dim=0)\n",
    "\n",
    "def classify(image, classes, template=\"a photo of a {}\"):\n",
    "    image_embs = embed_image(image)\n",
    "    text_embs = embed_text([template.format(o) for o in classes])\n",
    "    return probs(text_embs, image_embs)\n",
    "\n",
    "def search(image_embs, query_embs):\n",
    "    sims = cosine_sim(image_embs, query_embs).flatten()\n",
    "    indices = sims.argsort(descending=True)\n",
    "    return indices, sims[indices]\n",
    "\n",
    "def thumbnail(image, scale=3):\n",
    "    return image.resize(np.array(image.size)//scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "paintings = load_dataset(\"huggan/few-shot-art-painting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:23<00:00,  6.96it/s]\n"
     ]
    }
   ],
   "source": [
    "all_image_embs_path = Path(\"paintings_embeddings.npy\")\n",
    "if all_image_embs_path.exists():\n",
    "    all_image_embs = torch.tensor(np.load(all_image_embs_path))\n",
    "else:\n",
    "    all_image_embs = torch.cat([embed_image(row['image']) for row in tqdm(paintings['train'])])\n",
    "    np.save(all_image_embs_path, np.array(all_image_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
