{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Core Utils\n",
    "\n",
    "> core classes & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchmetrics import Accuracy, MaxMetric, MeanMetric\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "from nimrod.image.datasets import ImageDataModule\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Callable, Optional\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Abstract Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Classifier(ABC, L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet: nn.Module,\n",
    "            num_classes:int,\n",
    "            optimizer: Callable[...,torch.optim.Optimizer], # partial of optimizer\n",
    "            scheduler: Optional[Callable[...,Any]]=None, # partial of scheduler\n",
    "            ):\n",
    "\n",
    "        logger.info(\"Classifier: init\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.nnet = nnet\n",
    "        self.register_module('nnet', self.nnet)\n",
    "        self.lr = optimizer.keywords.get('lr') if optimizer else None # for lr finder\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        self.val_acc_best = MaxMetric()\n",
    "        self.step = 0\n",
    "\n",
    "        self.optimizer_config = None\n",
    "        self.scheduler_config = None\n",
    "        self.nnet_config = None\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        self.optimizer = optimizer\n",
    "        logger.info(f\"Optimizer: {optimizer.__class__}\")\n",
    "\n",
    "        if self.hparams.scheduler is None:\n",
    "            logger.warning(\"no scheduler has been setup\")\n",
    "            return {\"optimizer\": optimizer}\n",
    "        \n",
    "        scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "        self.scheduler = scheduler\n",
    "        logger.info(f\"Scheduler: {scheduler.__class__}\")\n",
    "\n",
    "        scheduler_config = {\"scheduler\": scheduler}\n",
    "        \n",
    "        # Special handling for different scheduler types\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler_config.update({\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            })\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"step\",\n",
    "            })\n",
    "        else:\n",
    "            # Default configuration for other scheduler types\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"epoch\",\n",
    "            })\n",
    "        # setup config to be able to save in wandb\n",
    "        self.optimizer_config = {\n",
    "            'type': optimizer.__class__.__name__,\n",
    "            'params': optimizer.defaults\n",
    "        }\n",
    "        self.scheduler_config = {\n",
    "            'type': scheduler.__class__.__name__,\n",
    "            'params': scheduler.__dict__\n",
    "        }\n",
    "        self.nnet_config = {\n",
    "            'type': self.nnet.__class__.__name__,\n",
    "            'architecture': str(self.nnet),\n",
    "\n",
    "        }\n",
    "        if self.logger and hasattr(self.logger, 'experiment'):\n",
    "            self.logger.experiment.config.update({\n",
    "                'optimizer_config': self.optimizer_config,\n",
    "                'scheduler_config': self.scheduler_config,\n",
    "                'nnet_config': self.nnet_config\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler_config,\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        # by default lightning executes validation step sanity checks before training starts,\n",
    "        # so it's worth to make sure validation metrics don't store results from these checks\n",
    "        self.val_loss.reset()\n",
    "        self.val_acc.reset()\n",
    "        self.val_acc_best.reset()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "        #     logger.info(\"scheduler is instance of OneCycleLR\")\n",
    "        #     if self.step >= self.scheduler.total_steps:\n",
    "        #         logger.warning(\"Max steps reached for 1-cycle LR scheduler\")\n",
    "        #         return\n",
    "        \n",
    "        self.step += 1\n",
    "\n",
    "        opt = self.optimizers() # optimizer defined in configure_optimizers\n",
    "        sched = self.lr_schedulers() # access scheduler defined in configure_optimizers\n",
    "         \n",
    "        opt.zero_grad()\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        if not isinstance(sched, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            sched.step() #reduce plateau sched is updated at end of epoch only instead TODO: should it be applied to val loop by default?\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_acc(preds, y)\n",
    "        metrics = {\"train/loss\": self.train_loss, \"train/acc\": self.train_acc}\n",
    "        self.log_dict(metrics, on_epoch=True, on_step=True, prog_bar=True)# Pass the validation loss to the scheduler\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.val_loss(loss)\n",
    "        self.val_acc(preds, y)\n",
    "        metrics = {\"val/loss\":self.val_loss, \"val/acc\": self.val_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"Lightning hook that is called when a validation epoch ends.\"\n",
    "        acc = self.val_acc.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)\n",
    "        \n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            logger.info(\"scheduler is an instance of Reduce plateau\")\n",
    "            sch.step(self.trainer.callback_metrics[\"val/loss\"])\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.test_loss(loss)\n",
    "        self.test_acc(preds, y)\n",
    "        metrics = {\"test/loss\":self.test_loss, \"test/acc\": self.test_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Lightning hook that is called when a test epoch ends.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def plot_classifier_metrics_from_csv(metrics_csv_path:str | os.PathLike):\n",
    "    metrics = pd.read_csv(metrics_csv_path)\n",
    "    # Create figure with secondary y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot loss\n",
    "    ax1.plot(metrics['step'], metrics['train/loss_step'], 'b-', label='Train Loss')\n",
    "    ax1.plot(metrics['step'], metrics['val/loss'], 'b*', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(metrics['step'], metrics['train/acc_step'], 'r-', label='Train Acc')\n",
    "    ax2.plot(metrics['step'], metrics['val/acc'], 'r*', label='Val Acc')\n",
    "    ax2.set_ylabel('Accuracy', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Add legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "    plt.title('Training Metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialModelX(Classifier):\n",
    "    def __init__(self, modules: List[nn.Module], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LRFinder pythonm module (other version with lightning)\n",
    "\n",
    "def find_optimal_lr(model, train_loader, criterion=None, optimizer=None, device='cuda'):\n",
    "    # If no criterion provided, use default CrossEntropyLoss\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # If no optimizer provided, use Adam\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    \n",
    "    # Initialize LR Finder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "    \n",
    "    # Run LR range test\n",
    "    lr_finder.range_test(\n",
    "        train_loader, \n",
    "        start_lr=1e-7,  # Very small starting learning rate\n",
    "        end_lr=10,      # Large ending learning rate\n",
    "        num_iter=100,   # Number of iterations to test\n",
    "        smooth_f=0.05   # Smoothing factor for the loss\n",
    "    )\n",
    "    \n",
    "    # Plot the learning rate vs loss\n",
    "    lr_finder.plot(log_lr=True)\n",
    "    \n",
    "    # Suggest optimal learning rate\n",
    "    suggested_lr = lr_finder.reset()\n",
    "    \n",
    "    print(f\"Suggested Learning Rate: {suggested_lr}\")\n",
    "    \n",
    "    return suggested_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def lr_finder(\n",
    "    model: Callable[...,torch.nn.Module], # partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule, # data module\n",
    "    num_training:int=100, # number of iterations\n",
    "    plot:bool=True # plot the learning rate vs loss\n",
    "    ):\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\")\n",
    "    tuner = Tuner(trainer)\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5)\n",
    "    model = model(optimizer=optimizer, scheduler=None)\n",
    "    lr_finder = tuner.lr_find(\n",
    "        model,\n",
    "        datamodule=datamodule,\n",
    "        min_lr=1e-5,\n",
    "        max_lr=1.0,\n",
    "        num_training=num_training,  # number of iterations\n",
    "        # attr_name=\"optimizer.lr\",\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        _ = lr_finder.plot(suggest=True)\n",
    "        plt.show()\n",
    "    return lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-cycle train helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_one_cycle(\n",
    "    model: Callable[...,torch.nn.Module], #partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule,\n",
    "    max_lr:float=0.1,\n",
    "    weight_decay=1e-5,\n",
    "    n_epochs: int=5,\n",
    "    project_name:str='MNIST-Classifier',\n",
    "    tags = ['arch', 'dev'],\n",
    "    test:bool=True,\n",
    "    run_name:str=None\n",
    "    ):\n",
    "\n",
    "    \"\"\"train one cycle, adamW optim with wandb logging & learning rate monitor by default\"\"\"\n",
    "\n",
    "    model_name = model.func.__name__ \n",
    "    if run_name is None:\n",
    "        run_name = f\"{model_name}-bs:{datamodule.batch_size}-epochs:{n_epochs}\"\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=project_name,\n",
    "        name=run_name,\n",
    "        save_dir='wandb',\n",
    "        entity='slegroux',\n",
    "        tags=tags,\n",
    "        group=model_name,\n",
    "        log_model=True, # log artefacts at the end of run\n",
    "        # monitor_gym=False,\n",
    "        mode='online'\n",
    "        )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=n_epochs,\n",
    "        logger=wandb_logger,\n",
    "        callbacks = [lr_monitor],\n",
    "        check_val_every_n_epoch=1,\n",
    "        log_every_n_steps=1\n",
    "        )\n",
    "\n",
    "    total_steps = len(datamodule.train_dataloader()) * n_epochs\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=weight_decay)\n",
    "    scheduler = partial(torch.optim.lr_scheduler.OneCycleLR, total_steps=total_steps, max_lr=max_lr) \n",
    "    model = model(optimizer=optimizer, scheduler=scheduler)\n",
    "    \n",
    "    trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "    if test:\n",
    "        trainer.test(model, datamodule.test_dataloader())\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:31:27] INFO - Init ImageDataModule for mnist\n",
      "[14:31:29] INFO - loading dataset mnist with args () from split train\n",
      "[14:31:36] INFO - loading dataset mnist with args () from split test\n",
      "[14:31:38] INFO - split train into train/val [0.8, 0.2]\n",
      "[14:31:38] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "# data\n",
    "cfg = OmegaConf.load('../config/data/image/mnist.yaml')\n",
    "cfg.data_dir = \"../data/image\"\n",
    "cfg.batch_size = 512\n",
    "cfg.num_workers = 0\n",
    "dm = instantiate(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[14:54:16] INFO - ConvNetX: init\n",
      "[14:54:16] INFO - Classifier: init\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/wandb/run-20250124_145416-4zsrrs5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slegroux/MNIST-Classifier/runs/4zsrrs5m' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/slegroux/MNIST-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slegroux/MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slegroux/MNIST-Classifier/runs/4zsrrs5m' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Classifier/runs/4zsrrs5m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:54:17] INFO - Optimizer: <class 'torch.optim.adamw.AdamW'>\n",
      "[14:54:17] INFO - Scheduler: <class 'torch.optim.lr_scheduler.OneCycleLR'>\n",
      "\n",
      "  | Name         | Type               | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | nnet         | ConvNet            | 12.0 K | train\n",
      "1 | loss         | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc    | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc      | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc     | MulticlassAccuracy | 0      | train\n",
      "5 | train_loss   | MeanMetric         | 0      | train\n",
      "6 | val_loss     | MeanMetric         | 0      | train\n",
      "7 | test_loss    | MeanMetric         | 0      | train\n",
      "8 | val_acc_best | MaxMetric          | 0      | train\n",
      "------------------------------------------------------------\n",
      "12.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 K    Total params\n",
      "0.048     Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9c70e43d724984b9083002767604fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346b0b4b98e945eaa8f679a614516519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb73837b3be4041bfb848df19fcb0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38b0b278de847c89e96bed3a6541df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8596000075340271     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8904655575752258     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8596000075340271    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8904655575752258    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>▁▁▁▂▃▄▅▆▇████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>test/acc</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/acc_epoch</td><td>▁</td></tr><tr><td>train/acc_step</td><td>▁▁▁▁▁▂▂▃▂▃▄▄▅▅▅▆▆▆▆▇▇▇▇█▇▇██▇▇██████████</td></tr><tr><td>train/loss_epoch</td><td>▁</td></tr><tr><td>train/loss_step</td><td>█████▇▇▇▇▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>val/acc</td><td>▁</td></tr><tr><td>val/acc_best</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>test/acc</td><td>0.8596</td></tr><tr><td>test/loss</td><td>0.89047</td></tr><tr><td>train/acc_epoch</td><td>0.6549</td></tr><tr><td>train/acc_step</td><td>0.84896</td></tr><tr><td>train/loss_epoch</td><td>1.43099</td></tr><tr><td>train/loss_step</td><td>0.93093</td></tr><tr><td>trainer/global_step</td><td>94</td></tr><tr><td>val/acc</td><td>0.84958</td></tr><tr><td>val/acc_best</td><td>0.84958</td></tr><tr><td>val/loss</td><td>0.90855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/slegroux/MNIST-Classifier/runs/4zsrrs5m' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Classifier/runs/4zsrrs5m</a><br> View project at: <a href='https://wandb.ai/slegroux/MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Classifier</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>wandb/wandb/run-20250124_145416-4zsrrs5m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| notest\n",
    "# model\n",
    "cfg_model = OmegaConf.load('../config/model/image/convnetx.yaml')\n",
    "feats_dim = [1, 8, 16, 32, 16]\n",
    "# feats_dim = [1, 4, 8, 16, 8]\n",
    "# feats_dim = [1, 16, 32, 64, 32]\n",
    "cfg_model.nnet.n_features = feats_dim\n",
    "model = instantiate(cfg_model) #partial\n",
    "\n",
    "# train\n",
    "N_EPOCHS = 1\n",
    "suggested_lr = 1e-3\n",
    "run_name = \"test\"\n",
    "\n",
    "train_one_cycle(\n",
    "    model,\n",
    "    dm,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    max_lr=suggested_lr,\n",
    "    project_name='MNIST-Classifier',\n",
    "    tags=[f\"feats:{feats_dim}\", 'dev'],\n",
    "    run_name=run_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
