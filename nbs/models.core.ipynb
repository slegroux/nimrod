{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Core Utils\n",
    "\n",
    "> core classes & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, MaxMetric, MeanMetric, MinMetric\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "from torch_lr_finder import LRFinder\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "from nimrod.image.datasets import ImageDataModule\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple\n",
    "from functools import partial\n",
    "from rich import print\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weight_init(\n",
    "        m:nn.Module, # the module to initialize\n",
    "        leaky:int=0 # if leaky relu used\n",
    "        ):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight,a=leaky)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet = nn.Sequential(nn.Conv2d(3, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2, 2))\n",
    "nnet.apply(weight_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Abstract Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Classifier(ABC, L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet: nn.Module,\n",
    "            num_classes:int,\n",
    "            optimizer: Callable[...,torch.optim.Optimizer], # partial of optimizer\n",
    "            scheduler: Optional[Callable[...,Any]]=None, # partial of scheduler\n",
    "            ):\n",
    "\n",
    "        logger.info(\"Classifier: init\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.nnet = nnet\n",
    "        self.register_module('nnet', self.nnet)\n",
    "        self.lr = optimizer.keywords.get('lr') if optimizer else None # for lr finder\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        self.val_acc_best = MaxMetric()\n",
    "        self.step = 0\n",
    "\n",
    "        # self.optimizer_config = None\n",
    "        # self.scheduler_config = None\n",
    "        # self.nnet_config = None\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        self.optimizer = optimizer\n",
    "        logger.info(f\"Optimizer: {optimizer.__class__}\")\n",
    "\n",
    "        if self.hparams.scheduler is None:\n",
    "            logger.warning(\"no scheduler has been setup\")\n",
    "            return {\"optimizer\": optimizer}\n",
    "        \n",
    "        scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "        self.scheduler = scheduler\n",
    "        logger.info(f\"Scheduler: {scheduler.__class__}\")\n",
    "\n",
    "        scheduler_config = {\"scheduler\": scheduler}\n",
    "        \n",
    "        # Special handling for different scheduler types\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler_config.update({\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            })\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"step\",\n",
    "            })\n",
    "        else:\n",
    "            # Default configuration for other scheduler types\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"epoch\",\n",
    "            })\n",
    "        # setup config to be able to save in wandb\n",
    "        # self.optimizer_config = {\n",
    "        #     'type': optimizer.__class__.__name__,\n",
    "        #     'params': optimizer.defaults\n",
    "        # }\n",
    "        # self.scheduler_config = {\n",
    "        #     'type': scheduler.__class__.__name__,\n",
    "        #     'params': scheduler.__dict__\n",
    "        # }\n",
    "        # self.nnet_config = {\n",
    "        #     'type': self.nnet.__class__.__name__,\n",
    "        #     'architecture': str(self.nnet),\n",
    "\n",
    "        # }\n",
    "        # if self.logger and hasattr(self.logger, 'experiment'):\n",
    "        #     self.logger.experiment.config.update({\n",
    "        #         'optimizer_config': self.optimizer_config,\n",
    "        #         'scheduler_config': self.scheduler_config,\n",
    "        #         'nnet_config': self.nnet_config\n",
    "        #     }, allow_val_change=True)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler_config,\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        # by default lightning executes validation step sanity checks before training starts,\n",
    "        # so it's worth to make sure validation metrics don't store results from these checks\n",
    "        self.val_loss.reset()\n",
    "        self.val_acc.reset()\n",
    "        self.val_acc_best.reset()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "        #     logger.info(\"scheduler is instance of OneCycleLR\")\n",
    "        #     if self.step >= self.scheduler.total_steps:\n",
    "        #         logger.warning(\"Max steps reached for 1-cycle LR scheduler\")\n",
    "        #         return\n",
    "        \n",
    "        self.step += 1\n",
    "\n",
    "        opt = self.optimizers() # optimizer defined in configure_optimizers\n",
    "        sched = self.lr_schedulers() # access scheduler defined in configure_optimizers\n",
    "         \n",
    "        opt.zero_grad()\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        if not isinstance(sched, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            sched.step() #reduce plateau sched is updated at end of epoch only instead TODO: should it be applied to val loop by default?\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_acc(preds, y)\n",
    "        metrics = {\"train/loss\": self.train_loss, \"train/acc\": self.train_acc}\n",
    "        self.log_dict(metrics, on_epoch=True, on_step=True, prog_bar=True)# Pass the validation loss to the scheduler\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.val_loss(loss)\n",
    "        self.val_acc(preds, y)\n",
    "        metrics = {\"val/loss\":self.val_loss, \"val/acc\": self.val_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"Lightning hook that is called when a validation epoch ends.\"\n",
    "        acc = self.val_acc.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)\n",
    "        \n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            logger.info(\"scheduler is an instance of Reduce plateau\")\n",
    "            sch.step(self.trainer.callback_metrics[\"val/loss\"])\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.test_loss(loss)\n",
    "        self.test_acc(preds, y)\n",
    "        metrics = {\"test/loss\":self.test_loss, \"test/acc\": self.test_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Lightning hook that is called when a test epoch ends.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def plot_classifier_metrics_from_csv(metrics_csv_path:str | os.PathLike):\n",
    "    metrics = pd.read_csv(metrics_csv_path)\n",
    "    # Create figure with secondary y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot loss\n",
    "    ax1.plot(metrics['step'], metrics['train/loss_step'], 'b-', label='Train Loss')\n",
    "    ax1.plot(metrics['step'], metrics['val/loss'], 'b*', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(metrics['step'], metrics['train/acc_step'], 'r-', label='Train Acc')\n",
    "    ax2.plot(metrics['step'], metrics['val/acc'], 'r*', label='Val Acc')\n",
    "    ax2.set_ylabel('Accuracy', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Add legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "    plt.title('Training Metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Regressor(ABC, L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nnet: L.LightningModule,\n",
    "        optimizer: Callable[...,torch.optim.Optimizer], # partial of optimizer\n",
    "        scheduler: Optional[Callable[...,Any]]=None, # partial of scheduler\n",
    "\n",
    "    ):\n",
    "        logger.info(\"Regressor: init\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = optimizer.keywords.get('lr') if optimizer else None # for lr finder\n",
    "        self.nnet = nnet\n",
    "        # explicitely register nnet as a  module to track its parameters\n",
    "        self.register_module('nnet', self.nnet)\n",
    "\n",
    "        # loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # loss accross batches\n",
    "        self.train_mse = MeanSquaredError()\n",
    "        self.val_mse = MeanSquaredError()\n",
    "        self.test_mse = MeanSquaredError()\n",
    "        self.val_mse_best = MinMetric()\n",
    "\n",
    "        # average accross batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        logger.info(\"Regressor: configure_optimizers\")\n",
    "        self.optimizer = self.hparams.optimizer(params=self.parameters())\n",
    "        logger.info(f\"Optimizer: {self.optimizer.__class__}\")\n",
    "        if self.hparams.scheduler is None:\n",
    "            logger.warning(\"no scheduler has been setup\")\n",
    "            return {\"optimizer\": self.optimizer}\n",
    "        self.scheduler = self.hparams.scheduler(optimizer=self.optimizer)\n",
    "        if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            lr_scheduler = {\n",
    "                \"scheduler\": self.scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        else:\n",
    "            lr_scheduler = {\n",
    "                \"scheduler\": self.scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        logger.info(f\"Scheduler: {self.scheduler.__class__}\")\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.val_loss.reset()\n",
    "        self.val_mse.reset()\n",
    "        self.val_mse_best.reset()\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n",
    "\n",
    "    def _step(self, batch:Tuple[torch.Tensor, torch.Tensor], batch_idx:int):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        return loss, y_hat, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.train_loss(loss)\n",
    "        self.train_mse(y_hat, y)\n",
    "        # self.log(\"train/mse\", self.train_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"train/loss\", self.train_loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.val_loss(loss)\n",
    "        self.val_mse(y_hat, y)\n",
    "        # self.log(\"val/mse\", self.val_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"val/loss\", self.val_loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mse = self.val_mse.compute()  # get current val acc\n",
    "        self.val_mse_best(mse)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/mse_best\", self.val_mse_best.compute(), sync_dist=True, prog_bar=True)\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.test_loss(loss)\n",
    "        self.test_mse(y_hat, y)\n",
    "    \n",
    "        # self.log(\"test/mse\", self.test_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"test/loss\", self.test_loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:33] INFO - Init ImageDataModule for mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:36] INFO - loading dataset mnist with args () from split train\n",
      "[21:54:36] INFO - loading dataset mnist from split train\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:38] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:38] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[21:54:38] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:38] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:42] INFO - loading dataset mnist with args () from split test\n",
      "[21:54:42] INFO - loading dataset mnist from split test\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:43] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:43] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[21:54:43] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:43] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:44] INFO - split train into train/val [0.8, 0.2]\n",
      "[21:54:44] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "cfg = OmegaConf.load(\"../config/data/image/mnist.yaml\")\n",
    "dm = instantiate(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dm.show(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialModelX(Classifier):\n",
    "    def __init__(self, modules: List[nn.Module], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LRFinder pythonm module (other version with lightning)\n",
    "\n",
    "def find_optimal_lr(model, train_loader, criterion=None, optimizer=None, device='cuda'):\n",
    "    # If no criterion provided, use default CrossEntropyLoss\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # If no optimizer provided, use Adam\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    \n",
    "    # Initialize LR Finder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "    \n",
    "    # Run LR range test\n",
    "    lr_finder.range_test(\n",
    "        train_loader, \n",
    "        start_lr=1e-7,  # Very small starting learning rate\n",
    "        end_lr=10,      # Large ending learning rate\n",
    "        num_iter=100,   # Number of iterations to test\n",
    "        smooth_f=0.05   # Smoothing factor for the loss\n",
    "    )\n",
    "    \n",
    "    # Plot the learning rate vs loss\n",
    "    lr_finder.plot(log_lr=True)\n",
    "    \n",
    "    # Suggest optimal learning rate\n",
    "    suggested_lr = lr_finder.reset()\n",
    "    \n",
    "    print(f\"Suggested Learning Rate: {suggested_lr}\")\n",
    "    \n",
    "    return suggested_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def lr_finder(\n",
    "    model: Callable[...,torch.nn.Module], # partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule, # data module\n",
    "    num_training:int=100, # number of iterations\n",
    "    plot:bool=True # plot the learning rate vs loss\n",
    "    ):\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\")\n",
    "    tuner = Tuner(trainer)\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5)\n",
    "    model = model(optimizer=optimizer, scheduler=None)\n",
    "    lr_finder = tuner.lr_find(\n",
    "        model,\n",
    "        datamodule=datamodule,\n",
    "        min_lr=1e-5,\n",
    "        max_lr=1.0,\n",
    "        num_training=num_training,  # number of iterations\n",
    "        # attr_name=\"optimizer.lr\",\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        _ = lr_finder.plot(suggest=True)\n",
    "        plt.show()\n",
    "    return lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-cycle train helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_one_cycle(\n",
    "    model: Callable[...,torch.nn.Module], #partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule,\n",
    "    max_lr:float=0.1,\n",
    "    weight_decay=1e-5,\n",
    "    n_epochs:int=5,\n",
    "    project_name:str='MNIST-Classifier',\n",
    "    tags = ['arch', 'dev'],\n",
    "    test:bool=True,\n",
    "    run_name:str=None,\n",
    "    model_summary:bool=True,\n",
    "    logger_cb:str='wandb',\n",
    "    precision=\"32-true\", # 16-mixed, 32-true\n",
    "    ):\n",
    "\n",
    "    \"\"\"train one cycle, adamW optim with wandb logging & learning rate monitor by default\"\"\"\n",
    "\n",
    "    model_name = model.func.__name__ \n",
    "    if run_name is None:\n",
    "        run_name = f\"{model_name}-bs:{datamodule.batch_size}-epochs:{n_epochs}\"\n",
    "    \n",
    "    # logger\n",
    "    if logger_cb=='wandb':\n",
    "\n",
    "        exp_logger = WandbLogger(\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            save_dir='wandb',\n",
    "            entity='slegroux',\n",
    "            tags=tags,\n",
    "            group=model_name,\n",
    "            log_model=True, # log artefacts at the end of run\n",
    "            # monitor_gym=False,\n",
    "            mode='online',\n",
    "            )\n",
    "    else:\n",
    "        exp_logger = TensorBoardLogger(\n",
    "            save_dir='./tensorboard',\n",
    "            log_graph=True,\n",
    "            default_hp_metric=True\n",
    "            )\n",
    "    # lr monitor\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    # checkpoint callback\n",
    "    ckpt_cb = ModelCheckpoint(\n",
    "        dirpath=f\"checkpoints/{project_name}/{run_name}\",\n",
    "        filename='{epoch}-{val/loss:.2f}',\n",
    "        auto_insert_metric_name=False,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=n_epochs,\n",
    "        logger=exp_logger,\n",
    "        callbacks = [lr_monitor, ckpt_cb],\n",
    "        check_val_every_n_epoch=1,\n",
    "        log_every_n_steps=1,\n",
    "        precision=precision\n",
    "        )\n",
    "\n",
    "    total_steps = len(datamodule.train_dataloader()) * n_epochs\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=weight_decay)\n",
    "    scheduler = partial(torch.optim.lr_scheduler.OneCycleLR, total_steps=total_steps, max_lr=max_lr) \n",
    "    model = model(optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "    if model_summary:\n",
    "        xb, yb = next(iter(datamodule.train_dataloader()))\n",
    "        print(summary(model.nnet, input_size=xb.shape, depth=3, device='cpu'))\n",
    "    \n",
    "    trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "    if test:\n",
    "        trainer.test(model, datamodule.test_dataloader())\n",
    "    logger.info(f\"Best ckpt path: {ckpt_cb.best_model_path}\")\n",
    "    wandb.finish()\n",
    "    return model, ckpt_cb.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:44] INFO - Init ImageDataModule for fashion_mnist\n",
      "[21:54:46] INFO - loading dataset fashion_mnist with args () from split train\n",
      "[21:54:46] INFO - loading dataset fashion_mnist from split train\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:48] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:48] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "[21:54:48] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:48] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:52] INFO - loading dataset fashion_mnist with args () from split test\n",
      "[21:54:52] INFO - loading dataset fashion_mnist from split test\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:53] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "[21:54:53] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - split train into train/val [0.8, 0.2]\n",
      "[21:54:53] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "# data\n",
    "cfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\n",
    "cfg.data_dir = \"../data/image\"\n",
    "cfg.batch_size = 128\n",
    "cfg.num_workers = 0\n",
    "dm = instantiate(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[21:54:54] INFO - ConvNetX: init\n",
      "[21:54:54] INFO - Classifier: init\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==========================================================================================\n",
       "Layer <span style=\"font-weight: bold\">(</span>typ<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:de</span>pth-idx<span style=\"font-weight: bold\">)</span>                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "├─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>          --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">]</span>         --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">]</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">672</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">560</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>          --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">984</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">540</span>\n",
       "│    └─Flatten: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>                      <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "==========================================================================================\n",
       "Total params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span>\n",
       "Trainable params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span>\n",
       "Non-trainable params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "Total mult-adds <span style=\"font-weight: bold\">(</span>Units.MEGABYTES<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161.97</span>\n",
       "==========================================================================================\n",
       "Input size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.52</span>\n",
       "Forward/backward pass size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.53</span>\n",
       "Params size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.44</span>\n",
       "Estimated Total Size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33.49</span>\n",
       "==========================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==========================================================================================\n",
       "Layer \u001b[1m(\u001b[0mtyp\u001b[1;92me:de\u001b[0mpth-idx\u001b[1m)\u001b[0m                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "├─Sequential: \u001b[1;36m1\u001b[0m-\u001b[1;36m1\u001b[0m                        \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m1\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m          --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m1\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m          \u001b[1;36m88\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m2\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m         --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m2\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m         \u001b[1;36m1\u001b[0m,\u001b[1;36m184\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m3\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m3\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m4\u001b[0m,\u001b[1;36m672\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m4\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m4\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m18\u001b[0m,\u001b[1;36m560\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m5\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m          --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m          \u001b[1;36m73\u001b[0m,\u001b[1;36m984\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m6\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m6\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m11\u001b[0m,\u001b[1;36m540\u001b[0m\n",
       "│    └─Flatten: \u001b[1;36m2\u001b[0m-\u001b[1;36m7\u001b[0m                      \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "==========================================================================================\n",
       "Total params: \u001b[1;36m110\u001b[0m,\u001b[1;36m028\u001b[0m\n",
       "Trainable params: \u001b[1;36m110\u001b[0m,\u001b[1;36m028\u001b[0m\n",
       "Non-trainable params: \u001b[1;36m0\u001b[0m\n",
       "Total mult-adds \u001b[1m(\u001b[0mUnits.MEGABYTES\u001b[1m)\u001b[0m: \u001b[1;36m161.97\u001b[0m\n",
       "==========================================================================================\n",
       "Input size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m0.52\u001b[0m\n",
       "Forward/backward pass size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m32.53\u001b[0m\n",
       "Params size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m0.44\u001b[0m\n",
       "Estimated Total Size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m33.49\u001b[0m\n",
       "==========================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20250206_215454-1fc1d7re</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">ConvNetX-bs:128-epochs:1</a></strong> to <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[21:54:54] INFO - Optimizer: <class 'torch.optim.adamw.AdamW'>\n",
      "[21:54:54] INFO - Scheduler: <class 'torch.optim.lr_scheduler.OneCycleLR'>\n",
      "\n",
      "  | Name         | Type               | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | nnet         | ConvNet            | 110 K  | train\n",
      "1 | loss         | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc    | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc      | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc     | MulticlassAccuracy | 0      | train\n",
      "5 | train_loss   | MeanMetric         | 0      | train\n",
      "6 | val_loss     | MeanMetric         | 0      | train\n",
      "7 | test_loss    | MeanMetric         | 0      | train\n",
      "8 | val_acc_best | MaxMetric          | 0      | train\n",
      "------------------------------------------------------------\n",
      "110 K     Trainable params\n",
      "0         Non-trainable params\n",
      "110 K     Total params\n",
      "0.440     Total estimated model params size (MB)\n",
      "41        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c4555ef1f04dc68fa34514f80d5060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4253e185ec40ec973f214548e9e68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44a8e846b1c45a7bf07a9486c0e97bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d45a8a15591480994778c5d2f1929c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8658000230789185     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5964178442955017     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8658000230789185    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5964178442955017    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:55:01] INFO - Best ckpt path: /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1/0-0.57.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>▁▁▂▂▃▃▄▅▅▅▆▆▇██████████▇▇▆▅▅▅▅▄▄▄▃▃▂▁▁▁▁</td></tr><tr><td>test/acc</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/acc_epoch</td><td>▁</td></tr><tr><td>train/acc_step</td><td>▁▂▄▅▅▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇██▇▇███▇▇██▇██▇█</td></tr><tr><td>train/loss_epoch</td><td>▁</td></tr><tr><td>train/loss_step</td><td>█▇▆▆▆▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>val/acc</td><td>▁</td></tr><tr><td>val/acc_best</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>test/acc</td><td>0.8658</td></tr><tr><td>test/loss</td><td>0.59642</td></tr><tr><td>train/acc_epoch</td><td>0.79585</td></tr><tr><td>train/acc_step</td><td>0.89844</td></tr><tr><td>train/loss_epoch</td><td>0.84797</td></tr><tr><td>train/loss_step</td><td>0.56174</td></tr><tr><td>trainer/global_step</td><td>375</td></tr><tr><td>val/acc</td><td>0.87608</td></tr><tr><td>val/acc_best</td><td>0.87608</td></tr><tr><td>val/loss</td><td>0.5739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ConvNetX-bs:128-epochs:1</strong> at: <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re</a><br> View project at: <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier</a><br>Synced 6 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20250206_215454-1fc1d7re/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| notest\n",
    "# model\n",
    "cfg_model = OmegaConf.load('../config/model/image/convnetx.yaml')\n",
    "feats_dim = [1, 8, 16, 32, 64, 128]\n",
    "# feats_dim = [1, 4, 8, 16, 8]\n",
    "# feats_dim = [1, 16, 32, 64, 32]\n",
    "cfg_model.nnet.n_features = feats_dim\n",
    "model = instantiate(cfg_model) #partial\n",
    "do_lr_finder = False\n",
    "\n",
    "if do_lr_finder:\n",
    "    suggested_lr = lr_finder(model=model, datamodule=dm, plot=True)\n",
    "else:\n",
    "    suggested_lr = 1e-3\n",
    "\n",
    "# train\n",
    "N_EPOCHS = 1\n",
    "\n",
    "project_name = \"FASHION-MNIST-Classifier\"\n",
    "run_name = f\"{model.func.__name__}-bs:{dm.batch_size}-epochs:{N_EPOCHS}\"\n",
    "tags = [f\"feats:{feats_dim}\", f\"bs:{dm.batch_size}\", f\"epochs:{N_EPOCHS}\"]\n",
    "\n",
    "trained_model, best_ckpt = train_one_cycle(\n",
    "    model,\n",
    "    dm,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    max_lr=suggested_lr,\n",
    "    project_name=project_name,\n",
    "    tags=tags,\n",
    "    run_name=run_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">/user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ConvNetX-bs</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>-epochs:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.58</span>.ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m/user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/\u001b[0m\u001b[95mConvNetX-bs\u001b[0m:\u001b[1;36m128\u001b[0m-epochs:\u001b[1;36m1\u001b[0m/\u001b[1;36m0\u001b[0m-\u001b[1;36m0.58\u001b[0m.ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.5579, 0.1776, 0.0000, 0.3419, 0.0000, 0.0000, 2.2810,\n",
       "         0.0000]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "print(best_ckpt)\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "trained_model.eval()\n",
    "trained_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
