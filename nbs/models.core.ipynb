{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Core Utils\n",
    "\n",
    "> core classes & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy, MaxMetric, MeanMetric, MinMetric\n",
    "from torchmetrics.regression import MeanSquaredError\n",
    "from torch_lr_finder import LRFinder\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger, TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import wandb\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "\n",
    "from nimrod.image.datasets import ImageDataModule\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, List, Callable, Optional, Tuple\n",
    "from functools import partial\n",
    "from rich import print\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init\n",
    "Apply init to layers with relu activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def weight_init(\n",
    "        m:nn.Module, # the module to initialize\n",
    "        leaky:int=0 # if leaky relu used\n",
    "        ):\n",
    "    if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):\n",
    "        nn.init.kaiming_normal_(m.weight, a=leaky)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, a=leaky)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">after conv: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "after conv: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">x flat dim:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "x flat dim:\n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m1024\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">after linear:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "after linear:\n",
       "\u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m3\u001b[0m, \u001b[1;36m64\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'kaiming init')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 32, 32)\n",
    "in_channels = x.shape[1]\n",
    "out_channels = 3\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "c1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size//2)\n",
    "x1 = c1(x)\n",
    "print(f\"after conv: {x1.shape}\")\n",
    "print(\"x flat dim:\", x1.flatten(2).shape)\n",
    "l1 = nn.Linear(32*32, 64)\n",
    "x2 = l1(x1.flatten(2))\n",
    "print(\"after linear:\", x2.shape)\n",
    "\n",
    "nnet = nn.Sequential(c1, nn.ReLU(), nn.Flatten(2), l1)\n",
    "nnet.eval().cpu()\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(x.permute(0,2,3,1).squeeze())\n",
    "ax[0].set_title(\"x\")\n",
    "y = nnet(x)\n",
    "ax[1].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\n",
    "ax[1].set_title(\"no init\")\n",
    "nnet.apply(weight_init)\n",
    "y = nnet(x)\n",
    "ax[2].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\n",
    "ax[2].set_title(\"kaiming init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Abstract Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Classifier(ABC, L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            nnet: nn.Module,\n",
    "            num_classes:int,\n",
    "            optimizer: Callable[...,torch.optim.Optimizer], # partial of optimizer\n",
    "            scheduler: Optional[Callable[...,Any]]=None, # partial of scheduler\n",
    "            ):\n",
    "\n",
    "        logger.info(\"Classifier: init\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.nnet = nnet\n",
    "        self.register_module('nnet', self.nnet)\n",
    "        self.lr = optimizer.keywords.get('lr') if optimizer else None # for lr finder\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "        self.val_acc_best = MaxMetric()\n",
    "        self.step = 0\n",
    "\n",
    "        # self.optimizer_config = None\n",
    "        # self.scheduler_config = None\n",
    "        # self.nnet_config = None\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "\n",
    "    def configure_optimizers(self) -> Dict[str, Any]:\n",
    "        optimizer = self.hparams.optimizer(params=self.trainer.model.parameters())\n",
    "        self.optimizer = optimizer\n",
    "        logger.info(f\"Optimizer: {optimizer.__class__}\")\n",
    "\n",
    "        if self.hparams.scheduler is None:\n",
    "            logger.warning(\"no scheduler has been setup\")\n",
    "            return {\"optimizer\": optimizer}\n",
    "        \n",
    "        scheduler = self.hparams.scheduler(optimizer=optimizer)\n",
    "        self.scheduler = scheduler\n",
    "        logger.info(f\"Scheduler: {scheduler.__class__}\")\n",
    "\n",
    "        scheduler_config = {\"scheduler\": scheduler}\n",
    "        \n",
    "        # Special handling for different scheduler types\n",
    "        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler_config.update({\n",
    "                \"monitor\": \"val/loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            })\n",
    "        elif isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"step\",\n",
    "            })\n",
    "        else:\n",
    "            # Default configuration for other scheduler types\n",
    "            scheduler_config.update({\n",
    "                \"interval\": \"epoch\",\n",
    "            })\n",
    "        # setup config to be able to save in wandb\n",
    "        # self.optimizer_config = {\n",
    "        #     'type': optimizer.__class__.__name__,\n",
    "        #     'params': optimizer.defaults\n",
    "        # }\n",
    "        # self.scheduler_config = {\n",
    "        #     'type': scheduler.__class__.__name__,\n",
    "        #     'params': scheduler.__dict__\n",
    "        # }\n",
    "        # self.nnet_config = {\n",
    "        #     'type': self.nnet.__class__.__name__,\n",
    "        #     'architecture': str(self.nnet),\n",
    "\n",
    "        # }\n",
    "        # if self.logger and hasattr(self.logger, 'experiment'):\n",
    "        #     self.logger.experiment.config.update({\n",
    "        #         'optimizer_config': self.optimizer_config,\n",
    "        #         'scheduler_config': self.scheduler_config,\n",
    "        #         'nnet_config': self.nnet_config\n",
    "        #     }, allow_val_change=True)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler_config,\n",
    "        }\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        pass\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        # by default lightning executes validation step sanity checks before training starts,\n",
    "        # so it's worth to make sure validation metrics don't store results from these checks\n",
    "        self.val_loss.reset()\n",
    "        self.val_acc.reset()\n",
    "        self.val_acc_best.reset()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "        #     logger.info(\"scheduler is instance of OneCycleLR\")\n",
    "        #     if self.step >= self.scheduler.total_steps:\n",
    "        #         logger.warning(\"Max steps reached for 1-cycle LR scheduler\")\n",
    "        #         return\n",
    "        \n",
    "        self.step += 1\n",
    "\n",
    "        opt = self.optimizers() # optimizer defined in configure_optimizers\n",
    "        sched = self.lr_schedulers() # access scheduler defined in configure_optimizers\n",
    "         \n",
    "        opt.zero_grad()\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.manual_backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        if not isinstance(sched, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            sched.step() #reduce plateau sched is updated at end of epoch only instead TODO: should it be applied to val loop by default?\n",
    "\n",
    "        self.train_loss(loss)\n",
    "        self.train_acc(preds, y)\n",
    "        metrics = {\"train/loss\": self.train_loss, \"train/acc\": self.train_acc}\n",
    "        self.log_dict(metrics, on_epoch=True, on_step=True, prog_bar=True)# Pass the validation loss to the scheduler\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.val_loss(loss)\n",
    "        self.val_acc(preds, y)\n",
    "        metrics = {\"val/loss\":self.val_loss, \"val/acc\": self.val_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"Lightning hook that is called when a validation epoch ends.\"\n",
    "        acc = self.val_acc.compute()  # get current val acc\n",
    "        self.val_acc_best(acc)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/acc_best\", self.val_acc_best.compute(), sync_dist=True, prog_bar=True)\n",
    "        \n",
    "        sch = self.lr_schedulers()\n",
    "        if isinstance(sch, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            logger.info(\"scheduler is an instance of Reduce plateau\")\n",
    "            sch.step(self.trainer.callback_metrics[\"val/loss\"])\n",
    "\n",
    "    def test_step(self, batch, batch_idx, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True):\n",
    "        loss, preds, y = self._step(batch, batch_idx)\n",
    "        self.test_loss(loss)\n",
    "        self.test_acc(preds, y)\n",
    "        metrics = {\"test/loss\":self.test_loss, \"test/acc\": self.test_acc}\n",
    "        self.log_dict(metrics, on_step=on_step, prog_bar=prog_bar, on_epoch=on_epoch, sync_dist=sync_dist)\n",
    "    \n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"Lightning hook that is called when a test epoch ends.\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "def plot_classifier_metrics_from_csv(metrics_csv_path:str | os.PathLike):\n",
    "    metrics = pd.read_csv(metrics_csv_path)\n",
    "    # Create figure with secondary y-axis\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    # Plot loss\n",
    "    ax1.plot(metrics['step'], metrics['train/loss_step'], 'b-', label='Train Loss')\n",
    "    ax1.plot(metrics['step'], metrics['val/loss'], 'b*', label='Val Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(metrics['step'], metrics['train/acc_step'], 'r-', label='Train Acc')\n",
    "    ax2.plot(metrics['step'], metrics['val/acc'], 'r*', label='Val Acc')\n",
    "    ax2.set_ylabel('Accuracy', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Add legend\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "    plt.title('Training Metrics')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor Abstract Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Regressor(ABC, L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nnet: L.LightningModule,\n",
    "        optimizer: Callable[...,torch.optim.Optimizer], # partial of optimizer\n",
    "        scheduler: Optional[Callable[...,Any]]=None, # partial of scheduler\n",
    "\n",
    "    ):\n",
    "        logger.info(\"Regressor: init\")\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = optimizer.keywords.get('lr') if optimizer else None # for lr finder\n",
    "        self.nnet = nnet\n",
    "        # explicitely register nnet as a  module to track its parameters\n",
    "        self.register_module('nnet', self.nnet)\n",
    "\n",
    "        # loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # loss accross batches\n",
    "        self.train_mse = MeanSquaredError()\n",
    "        self.val_mse = MeanSquaredError()\n",
    "        self.test_mse = MeanSquaredError()\n",
    "        self.val_mse_best = MinMetric()\n",
    "\n",
    "        # average accross batches\n",
    "        self.train_loss = MeanMetric()\n",
    "        self.val_loss = MeanMetric()\n",
    "        self.test_loss = MeanMetric()\n",
    "\n",
    "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
    "        return self.nnet(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        logger.info(\"Regressor: configure_optimizers\")\n",
    "        self.optimizer = self.hparams.optimizer(params=self.parameters())\n",
    "        logger.info(f\"Optimizer: {self.optimizer.__class__}\")\n",
    "        if self.hparams.scheduler is None:\n",
    "            logger.warning(\"no scheduler has been setup\")\n",
    "            return {\"optimizer\": self.optimizer}\n",
    "        self.scheduler = self.hparams.scheduler(optimizer=self.optimizer)\n",
    "        if isinstance(self.scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "            lr_scheduler = {\n",
    "                \"scheduler\": self.scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        else:\n",
    "            lr_scheduler = {\n",
    "                \"scheduler\": self.scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        logger.info(f\"Scheduler: {self.scheduler.__class__}\")\n",
    "        return {\"optimizer\": self.optimizer, \"lr_scheduler\": lr_scheduler}\n",
    "\n",
    "    def on_train_start(self) -> None:\n",
    "        self.val_loss.reset()\n",
    "        self.val_mse.reset()\n",
    "        self.val_mse_best.reset()\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n",
    "\n",
    "    def _step(self, batch:Tuple[torch.Tensor, torch.Tensor], batch_idx:int):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        return loss, y_hat, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.train_loss(loss)\n",
    "        self.train_mse(y_hat, y)\n",
    "        # self.log(\"train/mse\", self.train_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"train/loss\", self.train_loss, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.val_loss(loss)\n",
    "        self.val_mse(y_hat, y)\n",
    "        # self.log(\"val/mse\", self.val_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"val/loss\", self.val_loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mse = self.val_mse.compute()  # get current val acc\n",
    "        self.val_mse_best(mse)  # update best so far val acc\n",
    "        # log `val_acc_best` as a value through `.compute()` method, instead of as a metric object\n",
    "        # otherwise metric would be reset by lightning after each epoch\n",
    "        self.log(\"val/mse_best\", self.val_mse_best.compute(), sync_dist=True, prog_bar=True)\n",
    "        \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y_hat, y = self._step(batch, batch_idx)\n",
    "        self.test_loss(loss)\n",
    "        self.test_mse(y_hat, y)\n",
    "    \n",
    "        # self.log(\"test/mse\", self.test_mse, on_epoch=True, on_step=True, prog_bar=True)\n",
    "        self.log(\"test/loss\", self.test_loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:33] INFO - Init ImageDataModule for mnist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:36] INFO - loading dataset mnist with args () from split train\n",
      "[21:54:36] INFO - loading dataset mnist from split train\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:38] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:38] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[21:54:38] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:38] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:42] INFO - loading dataset mnist with args () from split test\n",
      "[21:54:42] INFO - loading dataset mnist from split test\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:43] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:43] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[21:54:43] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:43] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[21:54:44] INFO - split train into train/val [0.8, 0.2]\n",
      "[21:54:44] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "cfg = OmegaConf.load(\"../config/data/image/mnist.yaml\")\n",
    "dm = instantiate(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "dm.show(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SequentialModelX(Classifier):\n",
    "    def __init__(self, modules: List[nn.Module], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._model = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LRFinder pythonm module (other version with lightning)\n",
    "\n",
    "def find_optimal_lr(model, train_loader, criterion=None, optimizer=None, device='cuda'):\n",
    "    # If no criterion provided, use default CrossEntropyLoss\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # If no optimizer provided, use Adam\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n",
    "    \n",
    "    # Initialize LR Finder\n",
    "    lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "    \n",
    "    # Run LR range test\n",
    "    lr_finder.range_test(\n",
    "        train_loader, \n",
    "        start_lr=1e-7,  # Very small starting learning rate\n",
    "        end_lr=10,      # Large ending learning rate\n",
    "        num_iter=100,   # Number of iterations to test\n",
    "        smooth_f=0.05   # Smoothing factor for the loss\n",
    "    )\n",
    "    \n",
    "    # Plot the learning rate vs loss\n",
    "    lr_finder.plot(log_lr=True)\n",
    "    \n",
    "    # Suggest optimal learning rate\n",
    "    suggested_lr = lr_finder.reset()\n",
    "    \n",
    "    print(f\"Suggested Learning Rate: {suggested_lr}\")\n",
    "    \n",
    "    return suggested_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def lr_finder(\n",
    "    model: Callable[...,torch.nn.Module], # partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule, # data module\n",
    "    num_training:int=100, # number of iterations\n",
    "    plot:bool=True # plot the learning rate vs loss\n",
    "    ):\n",
    "\n",
    "    trainer = Trainer(accelerator=\"auto\")\n",
    "    tuner = Tuner(trainer)\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5)\n",
    "    model = model(optimizer=optimizer, scheduler=None)\n",
    "    lr_finder = tuner.lr_find(\n",
    "        model,\n",
    "        datamodule=datamodule,\n",
    "        min_lr=1e-5,\n",
    "        max_lr=1.0,\n",
    "        num_training=num_training,  # number of iterations\n",
    "        # attr_name=\"optimizer.lr\",\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        _ = lr_finder.plot(suggest=True)\n",
    "        plt.show()\n",
    "    return lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-cycle train helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def train_one_cycle(\n",
    "    model: Callable[...,torch.nn.Module], #partial model (missing optim & sched)\n",
    "    datamodule: ImageDataModule,\n",
    "    max_lr:float=0.1,\n",
    "    weight_decay=1e-5,\n",
    "    n_epochs:int=5,\n",
    "    project_name:str='MNIST-Classifier',\n",
    "    tags = ['arch', 'dev'],\n",
    "    test:bool=True,\n",
    "    run_name:str=None,\n",
    "    model_summary:bool=True,\n",
    "    logger_cb:str='wandb',\n",
    "    precision=\"32-true\", # 16-mixed, 32-true\n",
    "    ):\n",
    "\n",
    "    \"\"\"train one cycle, adamW optim with wandb logging & learning rate monitor by default\"\"\"\n",
    "\n",
    "    model_name = model.func.__name__ \n",
    "    if run_name is None:\n",
    "        run_name = f\"{model_name}-bs:{datamodule.batch_size}-epochs:{n_epochs}\"\n",
    "    \n",
    "    # logger\n",
    "    if logger_cb=='wandb':\n",
    "\n",
    "        exp_logger = WandbLogger(\n",
    "            project=project_name,\n",
    "            name=run_name,\n",
    "            save_dir='wandb',\n",
    "            entity='slegroux',\n",
    "            tags=tags,\n",
    "            group=model_name,\n",
    "            log_model=True, # log artefacts at the end of run\n",
    "            # monitor_gym=False,\n",
    "            mode='online',\n",
    "            )\n",
    "    else:\n",
    "        exp_logger = TensorBoardLogger(\n",
    "            save_dir='./tensorboard',\n",
    "            log_graph=True,\n",
    "            default_hp_metric=True\n",
    "            )\n",
    "    # lr monitor\n",
    "    lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "    # checkpoint callback\n",
    "    ckpt_cb = ModelCheckpoint(\n",
    "        dirpath=f\"checkpoints/{project_name}/{run_name}\",\n",
    "        filename='{epoch}-{val/loss:.2f}',\n",
    "        auto_insert_metric_name=False,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=n_epochs,\n",
    "        logger=exp_logger,\n",
    "        callbacks = [lr_monitor, ckpt_cb],\n",
    "        check_val_every_n_epoch=1,\n",
    "        log_every_n_steps=1,\n",
    "        precision=precision\n",
    "        )\n",
    "\n",
    "    total_steps = len(datamodule.train_dataloader()) * n_epochs\n",
    "    optimizer = partial(torch.optim.AdamW, lr=1e-4, weight_decay=weight_decay)\n",
    "    scheduler = partial(torch.optim.lr_scheduler.OneCycleLR, total_steps=total_steps, max_lr=max_lr) \n",
    "    model = model(optimizer=optimizer, scheduler=scheduler)\n",
    "\n",
    "    if model_summary:\n",
    "        xb, yb = next(iter(datamodule.train_dataloader()))\n",
    "        print(summary(model.nnet, input_size=xb.shape, depth=3, device='cpu'))\n",
    "    \n",
    "    trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "    if test:\n",
    "        trainer.test(model, datamodule.test_dataloader())\n",
    "    logger.info(f\"Best ckpt path: {ckpt_cb.best_model_path}\")\n",
    "    wandb.finish()\n",
    "    return model, ckpt_cb.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:54:44] INFO - Init ImageDataModule for fashion_mnist\n",
      "[21:54:46] INFO - loading dataset fashion_mnist with args () from split train\n",
      "[21:54:46] INFO - loading dataset fashion_mnist from split train\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:48] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:48] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "[21:54:48] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:48] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:52] INFO - loading dataset fashion_mnist with args () from split test\n",
      "[21:54:52] INFO - loading dataset fashion_mnist from split test\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[21:54:53] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "[21:54:53] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n",
      "Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n",
      "[21:54:53] INFO - split train into train/val [0.8, 0.2]\n",
      "[21:54:53] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "\n",
    "# data\n",
    "cfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\n",
    "cfg.data_dir = \"../data/image\"\n",
    "cfg.batch_size = 128\n",
    "cfg.num_workers = 0\n",
    "dm = instantiate(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[21:54:54] INFO - ConvNetX: init\n",
      "[21:54:54] INFO - Classifier: init\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==========================================================================================\n",
       "Layer <span style=\"font-weight: bold\">(</span>typ<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e:de</span>pth-idx<span style=\"font-weight: bold\">)</span>                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "├─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                        <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>          --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"font-weight: bold\">]</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">88</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">]</span>         --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"font-weight: bold\">]</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">184</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">672</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">560</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>          --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">]</span>          <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">73</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">984</span>\n",
       "│    └─ConvLayer: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>                    <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>           --\n",
       "│    │    └─Sequential: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>              <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">540</span>\n",
       "│    └─Flatten: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>                      <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">]</span>                 --\n",
       "==========================================================================================\n",
       "Total params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span>\n",
       "Trainable params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">110</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">028</span>\n",
       "Non-trainable params: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "Total mult-adds <span style=\"font-weight: bold\">(</span>Units.MEGABYTES<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161.97</span>\n",
       "==========================================================================================\n",
       "Input size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.52</span>\n",
       "Forward/backward pass size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.53</span>\n",
       "Params size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.44</span>\n",
       "Estimated Total Size <span style=\"font-weight: bold\">(</span>MB<span style=\"font-weight: bold\">)</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33.49</span>\n",
       "==========================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==========================================================================================\n",
       "Layer \u001b[1m(\u001b[0mtyp\u001b[1;92me:de\u001b[0mpth-idx\u001b[1m)\u001b[0m                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ConvNet                                  \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "├─Sequential: \u001b[1;36m1\u001b[0m-\u001b[1;36m1\u001b[0m                        \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m1\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m          --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m1\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m32\u001b[0m\u001b[1m]\u001b[0m          \u001b[1;36m88\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m2\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m         --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m2\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m, \u001b[1;36m16\u001b[0m\u001b[1m]\u001b[0m         \u001b[1;36m1\u001b[0m,\u001b[1;36m184\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m3\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m3\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m4\u001b[0m,\u001b[1;36m672\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m4\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m4\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m18\u001b[0m,\u001b[1;36m560\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m5\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m          --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m5\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m128\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m          \u001b[1;36m73\u001b[0m,\u001b[1;36m984\u001b[0m\n",
       "│    └─ConvLayer: \u001b[1;36m2\u001b[0m-\u001b[1;36m6\u001b[0m                    \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m           --\n",
       "│    │    └─Sequential: \u001b[1;36m3\u001b[0m-\u001b[1;36m6\u001b[0m              \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m           \u001b[1;36m11\u001b[0m,\u001b[1;36m540\u001b[0m\n",
       "│    └─Flatten: \u001b[1;36m2\u001b[0m-\u001b[1;36m7\u001b[0m                      \u001b[1m[\u001b[0m\u001b[1;36m128\u001b[0m, \u001b[1;36m10\u001b[0m\u001b[1m]\u001b[0m                 --\n",
       "==========================================================================================\n",
       "Total params: \u001b[1;36m110\u001b[0m,\u001b[1;36m028\u001b[0m\n",
       "Trainable params: \u001b[1;36m110\u001b[0m,\u001b[1;36m028\u001b[0m\n",
       "Non-trainable params: \u001b[1;36m0\u001b[0m\n",
       "Total mult-adds \u001b[1m(\u001b[0mUnits.MEGABYTES\u001b[1m)\u001b[0m: \u001b[1;36m161.97\u001b[0m\n",
       "==========================================================================================\n",
       "Input size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m0.52\u001b[0m\n",
       "Forward/backward pass size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m32.53\u001b[0m\n",
       "Params size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m0.44\u001b[0m\n",
       "Estimated Total Size \u001b[1m(\u001b[0mMB\u001b[1m)\u001b[0m: \u001b[1;36m33.49\u001b[0m\n",
       "==========================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20250206_215454-1fc1d7re</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">ConvNetX-bs:128-epochs:1</a></strong> to <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[21:54:54] INFO - Optimizer: <class 'torch.optim.adamw.AdamW'>\n",
      "[21:54:54] INFO - Scheduler: <class 'torch.optim.lr_scheduler.OneCycleLR'>\n",
      "\n",
      "  | Name         | Type               | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | nnet         | ConvNet            | 110 K  | train\n",
      "1 | loss         | CrossEntropyLoss   | 0      | train\n",
      "2 | train_acc    | MulticlassAccuracy | 0      | train\n",
      "3 | val_acc      | MulticlassAccuracy | 0      | train\n",
      "4 | test_acc     | MulticlassAccuracy | 0      | train\n",
      "5 | train_loss   | MeanMetric         | 0      | train\n",
      "6 | val_loss     | MeanMetric         | 0      | train\n",
      "7 | test_loss    | MeanMetric         | 0      | train\n",
      "8 | val_acc_best | MaxMetric          | 0      | train\n",
      "------------------------------------------------------------\n",
      "110 K     Trainable params\n",
      "0         Non-trainable params\n",
      "110 K     Total params\n",
      "0.440     Total estimated model params size (MB)\n",
      "41        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c4555ef1f04dc68fa34514f80d5060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4253e185ec40ec973f214548e9e68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44a8e846b1c45a7bf07a9486c0e97bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d45a8a15591480994778c5d2f1929c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8658000230789185     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5964178442955017     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8658000230789185    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5964178442955017    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21:55:01] INFO - Best ckpt path: /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1/0-0.57.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-AdamW</td><td>▁▁▂▂▃▃▄▅▅▅▆▆▇██████████▇▇▆▅▅▅▅▄▄▄▃▃▂▁▁▁▁</td></tr><tr><td>test/acc</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/acc_epoch</td><td>▁</td></tr><tr><td>train/acc_step</td><td>▁▂▄▅▅▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇██▇▇███▇▇██▇██▇█</td></tr><tr><td>train/loss_epoch</td><td>▁</td></tr><tr><td>train/loss_step</td><td>█▇▆▆▆▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>val/acc</td><td>▁</td></tr><tr><td>val/acc_best</td><td>▁</td></tr><tr><td>val/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>lr-AdamW</td><td>0.0</td></tr><tr><td>test/acc</td><td>0.8658</td></tr><tr><td>test/loss</td><td>0.59642</td></tr><tr><td>train/acc_epoch</td><td>0.79585</td></tr><tr><td>train/acc_step</td><td>0.89844</td></tr><tr><td>train/loss_epoch</td><td>0.84797</td></tr><tr><td>train/loss_step</td><td>0.56174</td></tr><tr><td>trainer/global_step</td><td>375</td></tr><tr><td>val/acc</td><td>0.87608</td></tr><tr><td>val/acc_best</td><td>0.87608</td></tr><tr><td>val/loss</td><td>0.5739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ConvNetX-bs:128-epochs:1</strong> at: <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re</a><br> View project at: <a href='https://wandb.ai/slegroux/FASHION-MNIST-Classifier' target=\"_blank\">https://wandb.ai/slegroux/FASHION-MNIST-Classifier</a><br>Synced 6 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/tmp/wandb/run-20250206_215454-1fc1d7re/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| notest\n",
    "# model\n",
    "cfg_model = OmegaConf.load('../config/model/image/convnetx.yaml')\n",
    "feats_dim = [1, 8, 16, 32, 64, 128]\n",
    "# feats_dim = [1, 4, 8, 16, 8]\n",
    "# feats_dim = [1, 16, 32, 64, 32]\n",
    "cfg_model.nnet.n_features = feats_dim\n",
    "model = instantiate(cfg_model) #partial\n",
    "do_lr_finder = False\n",
    "\n",
    "if do_lr_finder:\n",
    "    suggested_lr = lr_finder(model=model, datamodule=dm, plot=True)\n",
    "else:\n",
    "    suggested_lr = 1e-3\n",
    "\n",
    "# train\n",
    "N_EPOCHS = 1\n",
    "\n",
    "project_name = \"FASHION-MNIST-Classifier\"\n",
    "run_name = f\"{model.func.__name__}-bs:{dm.batch_size}-epochs:{N_EPOCHS}\"\n",
    "tags = [f\"feats:{feats_dim}\", f\"bs:{dm.batch_size}\", f\"epochs:{N_EPOCHS}\"]\n",
    "\n",
    "trained_model, best_ckpt = train_one_cycle(\n",
    "    model,\n",
    "    dm,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    max_lr=suggested_lr,\n",
    "    project_name=project_name,\n",
    "    tags=tags,\n",
    "    run_name=run_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">/user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ConvNetX-bs</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>-epochs:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.58</span>.ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m/user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/\u001b[0m\u001b[95mConvNetX-bs\u001b[0m:\u001b[1;36m128\u001b[0m-epochs:\u001b[1;36m1\u001b[0m/\u001b[1;36m0\u001b[0m-\u001b[1;36m0.58\u001b[0m.ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.5579, 0.1776, 0.0000, 0.3419, 0.0000, 0.0000, 2.2810,\n",
       "         0.0000]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| notest\n",
    "print(best_ckpt)\n",
    "x = torch.randn(1, 1, 32, 32)\n",
    "trained_model.eval()\n",
    "trained_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
