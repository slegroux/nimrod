{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "> Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "from nimrod.models.conv import ConvLayer\n",
    "from nimrod.models.core import Classifier\n",
    "from nimrod.utils import get_device, set_seed\n",
    "from nimrod.image.datasets import ImageDataModule\n",
    "\n",
    "from typing import List, Optional, Callable, Any\n",
    "import logging\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels: int=3 # Number of input & output channels\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        conv = partial(ConvLayer, n_channels, n_channels, stride=1, normalization=nn.BatchNorm2d)\n",
    "        layers += [conv(activation=nn.ReLU), conv(activation=None)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:34:36] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:34:36] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResBlock                                 [1, 3, 32, 32]            --\n",
       "├─Sequential: 1-1                        [1, 3, 32, 32]            --\n",
       "│    └─ConvLayer: 2-1                    [1, 3, 32, 32]            --\n",
       "│    │    └─Sequential: 3-1              [1, 3, 32, 32]            87\n",
       "│    └─ConvLayer: 2-2                    [1, 3, 32, 32]            --\n",
       "│    │    └─Sequential: 3-2              [1, 3, 32, 32]            87\n",
       "==========================================================================================\n",
       "Total params: 174\n",
       "Trainable params: 174\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.17\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.11\n",
       "=========================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResBlock(3)\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "summary(model=model, input_size=(1, 3, 32, 32), depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: List[int]=[1, 8, 16, 32, 16], # Number of input & output channels\n",
    "            num_classes: int=10, # Number of classes\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        logger.info(\"ResNet: init\")\n",
    "        layers = []\n",
    "        conv = partial(ConvLayer, stride=2, normalization=nn.BatchNorm2d, activation=nn.ReLU)\n",
    "        # convnet with resblock between\n",
    "        for i in range(len(n_features)-1):\n",
    "            layers += [conv(n_features[i], n_features[i+1])]\n",
    "            layers += [ResBlock(n_features[i+1])]\n",
    "\n",
    "        # last layer back to n_classes and flatten\n",
    "        layers.append(conv(n_features[-1], num_classes))\n",
    "        layers.append(nn.Flatten())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:35:47] INFO - ResNet: init\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:35:47] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 1, 28, 28)\n",
    "model = ResNet(n_features=[1, 8, 16, 32, 16], num_classes=10)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNetX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ResNetX(Classifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nnet:ResNet,\n",
    "        num_classes:int,\n",
    "        optimizer:Callable[...,torch.optim.Optimizer], # optimizer,\n",
    "        scheduler: Optional[Callable[...,Any]]=None, # scheduler\n",
    "        ):\n",
    "        \n",
    "        logger.info(\"ResNetX: init\")\n",
    "        super().__init__(\n",
    "            nnet=nnet,\n",
    "            num_classes=num_classes,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler\n",
    "            )\n",
    "\n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        return loss, preds, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:40:37] INFO - ResNet: init\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n",
      "[15:40:37] WARNING - setting conv bias to False as Batchnorm is used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../config/model/image/resnetx.yaml')\n",
    "B, C, H, W = 64, 1, 28, 28\n",
    "x = torch.randn(B, C, H, W)\n",
    "nnet = instantiate(cfg.nnet, num_classes=10)\n",
    "y = nnet(x)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             [64, 10]                  --\n",
       "├─Sequential: 1-1                                  [64, 10]                  --\n",
       "│    └─ConvLayer: 2-1                              [64, 8, 14, 14]           --\n",
       "│    │    └─Sequential: 3-1                        [64, 8, 14, 14]           --\n",
       "│    │    │    └─Conv2d: 4-1                       [64, 8, 14, 14]           72\n",
       "│    │    │    └─BatchNorm2d: 4-2                  [64, 8, 14, 14]           16\n",
       "│    │    │    └─ReLU: 4-3                         [64, 8, 14, 14]           --\n",
       "│    └─ResBlock: 2-2                               [64, 8, 14, 14]           --\n",
       "│    │    └─Sequential: 3-2                        [64, 8, 14, 14]           --\n",
       "│    │    │    └─ConvLayer: 4-4                    [64, 8, 14, 14]           --\n",
       "│    │    │    │    └─Sequential: 5-1              [64, 8, 14, 14]           592\n",
       "│    │    │    └─ConvLayer: 4-5                    [64, 8, 14, 14]           --\n",
       "│    │    │    │    └─Sequential: 5-2              [64, 8, 14, 14]           592\n",
       "│    └─ConvLayer: 2-3                              [64, 16, 7, 7]            --\n",
       "│    │    └─Sequential: 3-3                        [64, 16, 7, 7]            --\n",
       "│    │    │    └─Conv2d: 4-6                       [64, 16, 7, 7]            1,152\n",
       "│    │    │    └─BatchNorm2d: 4-7                  [64, 16, 7, 7]            32\n",
       "│    │    │    └─ReLU: 4-8                         [64, 16, 7, 7]            --\n",
       "│    └─ResBlock: 2-4                               [64, 16, 7, 7]            --\n",
       "│    │    └─Sequential: 3-4                        [64, 16, 7, 7]            --\n",
       "│    │    │    └─ConvLayer: 4-9                    [64, 16, 7, 7]            --\n",
       "│    │    │    │    └─Sequential: 5-3              [64, 16, 7, 7]            2,336\n",
       "│    │    │    └─ConvLayer: 4-10                   [64, 16, 7, 7]            --\n",
       "│    │    │    │    └─Sequential: 5-4              [64, 16, 7, 7]            2,336\n",
       "│    └─ConvLayer: 2-5                              [64, 32, 4, 4]            --\n",
       "│    │    └─Sequential: 3-5                        [64, 32, 4, 4]            --\n",
       "│    │    │    └─Conv2d: 4-11                      [64, 32, 4, 4]            4,608\n",
       "│    │    │    └─BatchNorm2d: 4-12                 [64, 32, 4, 4]            64\n",
       "│    │    │    └─ReLU: 4-13                        [64, 32, 4, 4]            --\n",
       "│    └─ResBlock: 2-6                               [64, 32, 4, 4]            --\n",
       "│    │    └─Sequential: 3-6                        [64, 32, 4, 4]            --\n",
       "│    │    │    └─ConvLayer: 4-14                   [64, 32, 4, 4]            --\n",
       "│    │    │    │    └─Sequential: 5-5              [64, 32, 4, 4]            9,280\n",
       "│    │    │    └─ConvLayer: 4-15                   [64, 32, 4, 4]            --\n",
       "│    │    │    │    └─Sequential: 5-6              [64, 32, 4, 4]            9,280\n",
       "│    └─ConvLayer: 2-7                              [64, 16, 2, 2]            --\n",
       "│    │    └─Sequential: 3-7                        [64, 16, 2, 2]            --\n",
       "│    │    │    └─Conv2d: 4-16                      [64, 16, 2, 2]            4,608\n",
       "│    │    │    └─BatchNorm2d: 4-17                 [64, 16, 2, 2]            32\n",
       "│    │    │    └─ReLU: 4-18                        [64, 16, 2, 2]            --\n",
       "│    └─ResBlock: 2-8                               [64, 16, 2, 2]            --\n",
       "│    │    └─Sequential: 3-8                        [64, 16, 2, 2]            --\n",
       "│    │    │    └─ConvLayer: 4-19                   [64, 16, 2, 2]            --\n",
       "│    │    │    │    └─Sequential: 5-7              [64, 16, 2, 2]            2,336\n",
       "│    │    │    └─ConvLayer: 4-20                   [64, 16, 2, 2]            --\n",
       "│    │    │    │    └─Sequential: 5-8              [64, 16, 2, 2]            2,336\n",
       "│    └─ConvLayer: 2-9                              [64, 10, 1, 1]            --\n",
       "│    │    └─Sequential: 3-9                        [64, 10, 1, 1]            --\n",
       "│    │    │    └─Conv2d: 4-21                      [64, 10, 1, 1]            1,440\n",
       "│    │    │    └─BatchNorm2d: 4-22                 [64, 10, 1, 1]            20\n",
       "│    │    │    └─ReLU: 4-23                        [64, 10, 1, 1]            --\n",
       "│    └─Flatten: 2-10                               [64, 10]                  --\n",
       "====================================================================================================\n",
       "Total params: 41,132\n",
       "Trainable params: 41,132\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 59.49\n",
       "====================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 9.01\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 9.37\n",
       "===================================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(nnet, input_size=(B, C, H, W), depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
