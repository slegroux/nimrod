{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "> Neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "from nimrod.models.conv import ConvBlock, PreActivationConvBlock\n",
    "from nimrod.models.core import Classifier, weight_init\n",
    "from nimrod.utils import get_device, set_seed\n",
    "from nimrod.image.datasets import ImageDataModule\n",
    "\n",
    "from typing import List, Optional, Callable, Any, Type\n",
    "import logging\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int, # Number of input channels\n",
    "            out_channels:int, # Number of output channels\n",
    "            stride:int=2, # Stride\n",
    "            kernel_size:int=3, # Kernel size\n",
    "            activation:Optional[Type[nn.Module]]=nn.ReLU, # Activation class if no activatoin set to nn.Identity\n",
    "            normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d, # Normalization class\n",
    "            pre_activation:bool=False # replace conv block by pre-act block. used in unets e.g.\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.activation = activation()\n",
    "        conv_block = []\n",
    "        if pre_activation:\n",
    "            conv_ = partial(PreActivationConvBlock, stride=1, activation=activation, normalization=normalization)\n",
    "        else:\n",
    "            conv_ = partial(ConvBlock, stride=1, activation=activation, normalization=normalization)\n",
    "        # conv stride 1 to be able to go deeper while keeping the same spatial resolution\n",
    "        c1 = conv_(in_channels, out_channels, stride=1, kernel_size=kernel_size)\n",
    "        # conv stride to be able to go wider in number of channels\n",
    "        # activation will be added at very end\n",
    "        c2 = conv_(out_channels, out_channels, stride=stride, activation=None, kernel_size=kernel_size) #adding activation to the whole layer at the end c.f. forward\n",
    "        conv_block += [c1,c2]\n",
    "        self.conv_layer = nn.Sequential(*conv_block)\n",
    "\n",
    "        if in_channels == out_channels:\n",
    "            self.id = nn.Identity()\n",
    "        else:\n",
    "            # resize x to match channels\n",
    "            self.id = conv_(in_channels, out_channels, kernel_size=1, stride=1, activation=None)\n",
    "        \n",
    "        if stride == 1:\n",
    "            self.pooling = nn.Identity()\n",
    "        else:\n",
    "            # resize x to match the stride\n",
    "            self.pooling = nn.AvgPool2d(stride, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.activation(self.conv_layer(x) + self.id(self.pooling(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 16, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResBlock                                 [1, 8, 16, 16]            --\n",
       "├─Sequential: 1-1                        [1, 8, 16, 16]            --\n",
       "│    └─ConvBlock: 2-1                    [1, 8, 32, 32]            232\n",
       "│    └─ConvBlock: 2-2                    [1, 8, 16, 16]            592\n",
       "├─AvgPool2d: 1-2                         [1, 3, 16, 16]            --\n",
       "├─ConvBlock: 1-3                         [1, 8, 16, 16]            --\n",
       "│    └─Sequential: 2-3                   [1, 8, 16, 16]            40\n",
       "├─LeakyReLU: 1-4                         [1, 8, 16, 16]            --\n",
       "==========================================================================================\n",
       "Total params: 864\n",
       "Trainable params: 864\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.37\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.20\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.21\n",
       "=========================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResBlock(3, 8, stride=2, activation=partial(nn.LeakyReLU, negative_slope=0.1), normalization=nn.BatchNorm2d)\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "summary(model=model, input_size=(1, 3, 32, 32), depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_features: List[int]=[1, 8, 16, 32, 64, 32], # Number of input & output channels\n",
    "            num_classes: int=10, # Number of classes\n",
    "            activation:Optional[Type[nn.Module]]=nn.ReLU, # Activation function if None set to nn.Identity\n",
    "            normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d, # Normalization function if None set to nn.Identity\n",
    "            weight_initialization: bool = False, # weight init with kaiming\n",
    "            pre_activation: bool = False # pre-activation block for deep nets\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        logger.info(\"ResNet: init\")\n",
    "        layers = []\n",
    "        res_ = partial(ResBlock, stride=2, activation=activation, normalization=normalization, pre_activation=pre_activation)\n",
    "\n",
    "        layers.append(res_(in_channels=n_features[0], out_channels=n_features[1], stride=1))\n",
    "\n",
    "        for i in range(1, len(n_features)-1):\n",
    "            layers += [res_(in_channels=n_features[i], out_channels=n_features[i+1])]\n",
    "\n",
    "        # last layer back to n_classes and flatten\n",
    "        layers.append(res_(in_channels=n_features[-1], out_channels=num_classes))\n",
    "        layers.append(nn.Flatten())\n",
    "\n",
    "        # layers += [nn.Flatten(), nn.Linear(n_features[-1], num_classes, bias=False), nn.BatchNorm1d(num_classes)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        if weight_initialization:\n",
    "            logger.info(\"Init conv & linear with kaiming\")\n",
    "            if isinstance(activation, partial):\n",
    "                if activation.func == nn.LeakyReLU:\n",
    "                    logger.info(\"LeakyRelu layers weight init\")\n",
    "                    wi = partial(weight_init, leaky=activation.keywords.get('negative_slope'))\n",
    "                self.apply(wi)\n",
    "            else:\n",
    "                logger.info(\"ReLU layers weight init\")\n",
    "                self.apply(weight_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:27] INFO - ResNet: init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[13:12:27] INFO - Init conv & linear with kaiming\n",
      "[13:12:27] INFO - LeakyRelu layers weight init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 3, 28, 28)\n",
    "model = ResNet(\n",
    "    n_features=[3, 8, 16, 32, 64, 32],\n",
    "    num_classes=10,\n",
    "    activation=partial(nn.LeakyReLU, negative_slope=0.1),\n",
    "    # activation=nn.ReLU,\n",
    "    normalization=torch.nn.BatchNorm2d,\n",
    "    weight_initialization=True,\n",
    "    pre_activation=True\n",
    "    )\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "# summary(model=model, input_size=(64, 3, 28, 28), depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:43:51] INFO - ResNet: init\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet                                             [64, 40]                  --\n",
       "├─Sequential: 1-1                                  [64, 40]                  --\n",
       "│    └─ResBlock: 2-1                               [64, 8, 28, 28]           --\n",
       "│    │    └─Sequential: 3-1                        [64, 8, 28, 28]           --\n",
       "│    │    │    └─ConvBlock: 4-1                    [64, 8, 28, 28]           --\n",
       "│    │    │    │    └─Sequential: 5-1              [64, 8, 28, 28]           --\n",
       "│    │    │    │    │    └─Conv2d: 6-1             [64, 8, 28, 28]           72\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-2        [64, 8, 28, 28]           16\n",
       "│    │    │    │    │    └─ReLU: 6-3               [64, 8, 28, 28]           --\n",
       "│    │    │    └─ConvBlock: 4-2                    [64, 8, 28, 28]           --\n",
       "│    │    │    │    └─Sequential: 5-2              [64, 8, 28, 28]           --\n",
       "│    │    │    │    │    └─Conv2d: 6-4             [64, 8, 28, 28]           576\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-5        [64, 8, 28, 28]           16\n",
       "│    │    └─Identity: 3-2                          [64, 1, 28, 28]           --\n",
       "│    │    └─ConvBlock: 3-3                         [64, 8, 28, 28]           --\n",
       "│    │    │    └─Sequential: 4-3                   [64, 8, 28, 28]           --\n",
       "│    │    │    │    └─Conv2d: 5-3                  [64, 8, 28, 28]           8\n",
       "│    │    │    │    └─BatchNorm2d: 5-4             [64, 8, 28, 28]           16\n",
       "│    │    └─ReLU: 3-4                              [64, 8, 28, 28]           --\n",
       "│    └─ResBlock: 2-2                               [64, 16, 14, 14]          --\n",
       "│    │    └─Sequential: 3-5                        [64, 16, 14, 14]          --\n",
       "│    │    │    └─ConvBlock: 4-4                    [64, 16, 28, 28]          --\n",
       "│    │    │    │    └─Sequential: 5-5              [64, 16, 28, 28]          --\n",
       "│    │    │    │    │    └─Conv2d: 6-6             [64, 16, 28, 28]          1,152\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-7        [64, 16, 28, 28]          32\n",
       "│    │    │    │    │    └─ReLU: 6-8               [64, 16, 28, 28]          --\n",
       "│    │    │    └─ConvBlock: 4-5                    [64, 16, 14, 14]          --\n",
       "│    │    │    │    └─Sequential: 5-6              [64, 16, 14, 14]          --\n",
       "│    │    │    │    │    └─Conv2d: 6-9             [64, 16, 14, 14]          2,304\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-10       [64, 16, 14, 14]          32\n",
       "│    │    └─AvgPool2d: 3-6                         [64, 8, 14, 14]           --\n",
       "│    │    └─ConvBlock: 3-7                         [64, 16, 14, 14]          --\n",
       "│    │    │    └─Sequential: 4-6                   [64, 16, 14, 14]          --\n",
       "│    │    │    │    └─Conv2d: 5-7                  [64, 16, 14, 14]          128\n",
       "│    │    │    │    └─BatchNorm2d: 5-8             [64, 16, 14, 14]          32\n",
       "│    │    └─ReLU: 3-8                              [64, 16, 14, 14]          --\n",
       "│    └─ResBlock: 2-3                               [64, 32, 7, 7]            --\n",
       "│    │    └─Sequential: 3-9                        [64, 32, 7, 7]            --\n",
       "│    │    │    └─ConvBlock: 4-7                    [64, 32, 14, 14]          --\n",
       "│    │    │    │    └─Sequential: 5-9              [64, 32, 14, 14]          --\n",
       "│    │    │    │    │    └─Conv2d: 6-11            [64, 32, 14, 14]          4,608\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-12       [64, 32, 14, 14]          64\n",
       "│    │    │    │    │    └─ReLU: 6-13              [64, 32, 14, 14]          --\n",
       "│    │    │    └─ConvBlock: 4-8                    [64, 32, 7, 7]            --\n",
       "│    │    │    │    └─Sequential: 5-10             [64, 32, 7, 7]            --\n",
       "│    │    │    │    │    └─Conv2d: 6-14            [64, 32, 7, 7]            9,216\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-15       [64, 32, 7, 7]            64\n",
       "│    │    └─AvgPool2d: 3-10                        [64, 16, 7, 7]            --\n",
       "│    │    └─ConvBlock: 3-11                        [64, 32, 7, 7]            --\n",
       "│    │    │    └─Sequential: 4-9                   [64, 32, 7, 7]            --\n",
       "│    │    │    │    └─Conv2d: 5-11                 [64, 32, 7, 7]            512\n",
       "│    │    │    │    └─BatchNorm2d: 5-12            [64, 32, 7, 7]            64\n",
       "│    │    └─ReLU: 3-12                             [64, 32, 7, 7]            --\n",
       "│    └─ResBlock: 2-4                               [64, 16, 4, 4]            --\n",
       "│    │    └─Sequential: 3-13                       [64, 16, 4, 4]            --\n",
       "│    │    │    └─ConvBlock: 4-10                   [64, 16, 7, 7]            --\n",
       "│    │    │    │    └─Sequential: 5-13             [64, 16, 7, 7]            --\n",
       "│    │    │    │    │    └─Conv2d: 6-16            [64, 16, 7, 7]            4,608\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-17       [64, 16, 7, 7]            32\n",
       "│    │    │    │    │    └─ReLU: 6-18              [64, 16, 7, 7]            --\n",
       "│    │    │    └─ConvBlock: 4-11                   [64, 16, 4, 4]            --\n",
       "│    │    │    │    └─Sequential: 5-14             [64, 16, 4, 4]            --\n",
       "│    │    │    │    │    └─Conv2d: 6-19            [64, 16, 4, 4]            2,304\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-20       [64, 16, 4, 4]            32\n",
       "│    │    └─AvgPool2d: 3-14                        [64, 32, 4, 4]            --\n",
       "│    │    └─ConvBlock: 3-15                        [64, 16, 4, 4]            --\n",
       "│    │    │    └─Sequential: 4-12                  [64, 16, 4, 4]            --\n",
       "│    │    │    │    └─Conv2d: 5-15                 [64, 16, 4, 4]            512\n",
       "│    │    │    │    └─BatchNorm2d: 5-16            [64, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-16                             [64, 16, 4, 4]            --\n",
       "│    └─ResBlock: 2-5                               [64, 10, 2, 2]            --\n",
       "│    │    └─Sequential: 3-17                       [64, 10, 2, 2]            --\n",
       "│    │    │    └─ConvBlock: 4-13                   [64, 10, 4, 4]            --\n",
       "│    │    │    │    └─Sequential: 5-17             [64, 10, 4, 4]            --\n",
       "│    │    │    │    │    └─Conv2d: 6-21            [64, 10, 4, 4]            1,440\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-22       [64, 10, 4, 4]            20\n",
       "│    │    │    │    │    └─ReLU: 6-23              [64, 10, 4, 4]            --\n",
       "│    │    │    └─ConvBlock: 4-14                   [64, 10, 2, 2]            --\n",
       "│    │    │    │    └─Sequential: 5-18             [64, 10, 2, 2]            --\n",
       "│    │    │    │    │    └─Conv2d: 6-24            [64, 10, 2, 2]            900\n",
       "│    │    │    │    │    └─BatchNorm2d: 6-25       [64, 10, 2, 2]            20\n",
       "│    │    └─AvgPool2d: 3-18                        [64, 16, 2, 2]            --\n",
       "│    │    └─ConvBlock: 3-19                        [64, 10, 2, 2]            --\n",
       "│    │    │    └─Sequential: 4-15                  [64, 10, 2, 2]            --\n",
       "│    │    │    │    └─Conv2d: 5-19                 [64, 10, 2, 2]            160\n",
       "│    │    │    │    └─BatchNorm2d: 5-20            [64, 10, 2, 2]            20\n",
       "│    │    └─ReLU: 3-20                             [64, 10, 2, 2]            --\n",
       "│    └─Flatten: 2-6                                [64, 40]                  --\n",
       "====================================================================================================\n",
       "Total params: 28,992\n",
       "Trainable params: 28,992\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 228.65\n",
       "====================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 49.74\n",
       "Params size (MB): 0.12\n",
       "Estimated Total Size (MB): 50.06\n",
       "===================================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../config/model/image/resnetx.yaml')\n",
    "B, C, H, W = 64, 1, 28, 28\n",
    "x = torch.randn(B, C, H, W)\n",
    "nnet = instantiate(cfg.nnet)\n",
    "y = nnet(x)\n",
    "print(y.shape)\n",
    "summary(nnet, input_size=(B, C, H, W), depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNetX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class ResNetX(Classifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nnet:ResNet,\n",
    "        num_classes:int,\n",
    "        optimizer:Callable[...,torch.optim.Optimizer], # optimizer,\n",
    "        scheduler: Optional[Callable[...,Any]]=None, # scheduler\n",
    "        ):\n",
    "        \n",
    "        logger.info(\"ResNetX: init\")\n",
    "        super().__init__(\n",
    "            nnet=nnet,\n",
    "            num_classes=num_classes,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            )\n",
    "\n",
    "    def _step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        preds = y_hat.argmax(dim=1)\n",
    "        return loss, preds, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "- need to instantiate optimizer to get X models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:57:43] INFO - ResNet: init\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n",
      "[14:57:43] INFO - ResNetX: init\n",
      "[14:57:43] INFO - Classifier: init\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../config/optimizer/adam_w.yaml')\n",
    "optimizer = instantiate(cfg)\n",
    "\n",
    "cfg = OmegaConf.load('../config/scheduler/step_lr.yaml')\n",
    "scheduler = instantiate(cfg)\n",
    "\n",
    "cfg = OmegaConf.load('../config/model/image/resnetx.yaml')\n",
    "\n",
    "B, C, H, W = 64, 1, 28, 28\n",
    "x = torch.randn(B, C, H, W)\n",
    "\n",
    "nnet = instantiate(cfg)(optimizer=optimizer, scheduler=scheduler)\n",
    "y = nnet(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "ResNetX                                                 [64, 40]                  --\n",
       "├─ResNet: 1-1                                           [64, 40]                  --\n",
       "│    └─Sequential: 2-1                                  [64, 40]                  --\n",
       "│    │    └─ResBlock: 3-1                               [64, 8, 28, 28]           --\n",
       "│    │    │    └─Sequential: 4-1                        [64, 8, 28, 28]           --\n",
       "│    │    │    │    └─ConvBlock: 5-1                    [64, 8, 28, 28]           88\n",
       "│    │    │    │    └─ConvBlock: 5-2                    [64, 8, 28, 28]           592\n",
       "│    │    │    └─Identity: 4-2                          [64, 1, 28, 28]           --\n",
       "│    │    │    └─ConvBlock: 4-3                         [64, 8, 28, 28]           --\n",
       "│    │    │    │    └─Sequential: 5-3                   [64, 8, 28, 28]           24\n",
       "│    │    │    └─LeakyReLU: 4-4                         [64, 8, 28, 28]           --\n",
       "│    │    └─ResBlock: 3-2                               [64, 16, 14, 14]          --\n",
       "│    │    │    └─Sequential: 4-5                        [64, 16, 14, 14]          --\n",
       "│    │    │    │    └─ConvBlock: 5-4                    [64, 16, 28, 28]          1,184\n",
       "│    │    │    │    └─ConvBlock: 5-5                    [64, 16, 14, 14]          2,336\n",
       "│    │    │    └─AvgPool2d: 4-6                         [64, 8, 14, 14]           --\n",
       "│    │    │    └─ConvBlock: 4-7                         [64, 16, 14, 14]          --\n",
       "│    │    │    │    └─Sequential: 5-6                   [64, 16, 14, 14]          160\n",
       "│    │    │    └─LeakyReLU: 4-8                         [64, 16, 14, 14]          --\n",
       "│    │    └─ResBlock: 3-3                               [64, 32, 7, 7]            --\n",
       "│    │    │    └─Sequential: 4-9                        [64, 32, 7, 7]            --\n",
       "│    │    │    │    └─ConvBlock: 5-7                    [64, 32, 14, 14]          4,672\n",
       "│    │    │    │    └─ConvBlock: 5-8                    [64, 32, 7, 7]            9,280\n",
       "│    │    │    └─AvgPool2d: 4-10                        [64, 16, 7, 7]            --\n",
       "│    │    │    └─ConvBlock: 4-11                        [64, 32, 7, 7]            --\n",
       "│    │    │    │    └─Sequential: 5-9                   [64, 32, 7, 7]            576\n",
       "│    │    │    └─LeakyReLU: 4-12                        [64, 32, 7, 7]            --\n",
       "│    │    └─ResBlock: 3-4                               [64, 16, 4, 4]            --\n",
       "│    │    │    └─Sequential: 4-13                       [64, 16, 4, 4]            --\n",
       "│    │    │    │    └─ConvBlock: 5-10                   [64, 16, 7, 7]            4,640\n",
       "│    │    │    │    └─ConvBlock: 5-11                   [64, 16, 4, 4]            2,336\n",
       "│    │    │    └─AvgPool2d: 4-14                        [64, 32, 4, 4]            --\n",
       "│    │    │    └─ConvBlock: 4-15                        [64, 16, 4, 4]            --\n",
       "│    │    │    │    └─Sequential: 5-12                  [64, 16, 4, 4]            544\n",
       "│    │    │    └─LeakyReLU: 4-16                        [64, 16, 4, 4]            --\n",
       "│    │    └─ResBlock: 3-5                               [64, 10, 2, 2]            --\n",
       "│    │    │    └─Sequential: 4-17                       [64, 10, 2, 2]            --\n",
       "│    │    │    │    └─ConvBlock: 5-13                   [64, 10, 4, 4]            1,460\n",
       "│    │    │    │    └─ConvBlock: 5-14                   [64, 10, 2, 2]            920\n",
       "│    │    │    └─AvgPool2d: 4-18                        [64, 16, 2, 2]            --\n",
       "│    │    │    └─ConvBlock: 4-19                        [64, 10, 2, 2]            --\n",
       "│    │    │    │    └─Sequential: 5-15                  [64, 10, 2, 2]            180\n",
       "│    │    │    └─LeakyReLU: 4-20                        [64, 10, 2, 2]            --\n",
       "│    │    └─Flatten: 3-6                                [64, 40]                  --\n",
       "=========================================================================================================\n",
       "Total params: 28,992\n",
       "Trainable params: 28,992\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 228.65\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.20\n",
       "Forward/backward pass size (MB): 49.74\n",
       "Params size (MB): 0.12\n",
       "Estimated Total Size (MB): 50.06\n",
       "========================================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(nnet, input_size=(B, C, H, W), depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
