{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/syl20/mambaforge/envs/nimrod/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import SGD\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "from collections import Counter, OrderedDict\n",
    "from dataclasses import dataclass, asdict\n",
    "from plum import dispatch\n",
    "\n",
    "# nimrod\n",
    "from nimrod.models.lm import Vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' This ammunition , and that which I brought with me , was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose . As illustrating as the pitiful scarcity of material in the country , the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper . Gunsmiths were employed or conscripted , tools purchased or impressed , and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once . But , after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen \\'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances , with which most of them were armed . Two days elapsed before the change could be effected . \" \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][88])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based tokenization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', 'pitiful', 'scarcity', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', 'gunsmiths', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'gen', \"'\", 'l', 'rust', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', 'pikes', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', 'elapsed', 'before', 'the', 'change', 'could', 'be', 'effected', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n",
    "fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][88]['tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'ammunition', ',', 'and', 'that', 'which', 'I', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'Laboratory', 'established', 'at', 'the', 'Little', 'Rock', 'Arsenal', 'for', 'that', 'purpose', '.', 'As', 'ill', '##ust', '##rating', 'as', 'the', 'pit', '##iful', 'scar', '##city', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'State', 'Library', 'for', 'cartridge', 'paper', '.', 'Guns', '##mith', '##s', 'were', 'employed', 'or', 'con', '##s', '##cript', '##ed', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'I', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'Little', 'Rock', 'commenced', 'at', 'once', '.', 'But', ',', 'after', 'inspect', '##ing', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'I', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'Fi', '##tch', 'and', 'that', 'I', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'Gen', \"'\", 'l', 'R', '##ust', 'as', 'soon', 'as', 'shotgun', '##s', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'Little', 'Rock', 'instead', 'of', 'p', '##ike', '##s', 'and', 'lance', '##s', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'Two', 'days', 'el', '##ap', '##sed', 'before', 'the', 'change', 'could', 'be', 'effect', '##ed', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokens = tokenizer.tokenize(dataset['train'][88]['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29473\n",
      "['<unk>', '<eos>', 'the', ',', '.', 'of', 'and', 'in', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
    "min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1188, 9448, 117, 1105, 1115, 1134, 146, 1814, 1114, 1143, 117, 1108, 5223, 4029, 1111, 1329, 1120, 1103, 8891, 1628, 1120, 1103, 2743, 2977, 10503, 1111, 1115, 3007, 119, 1249, 5178, 8954, 7969, 1112, 1103, 7172, 17126, 14161, 9041, 1104, 2578, 1107, 1103, 1583, 117, 1103, 1864, 1336, 1129, 2202, 1115, 1122, 1108, 1276, 3238, 1106, 1329, 1470, 4961, 1104, 1103, 1426, 3371, 1111, 16542, 2526, 119, 18270, 17740, 1116, 1127, 4071, 1137, 14255, 1116, 13590, 1174, 117, 5537, 3310, 1137, 7351, 117, 1105, 1103, 6949, 1104, 1103, 4938, 3832, 146, 1814, 1114, 1143, 1105, 1164, 1126, 4463, 1295, 1276, 1120, 2743, 2977, 8042, 1120, 1517, 119, 1252, 117, 1170, 25151, 1158, 1103, 1250, 1105, 15639, 1103, 4840, 1104, 1103, 1441, 146, 1879, 1115, 170, 10609, 2260, 2012, 1180, 2080, 1149, 1222, 17355, 6943, 1105, 1115, 146, 1156, 1730, 1103, 6311, 118, 1164, 10204, 118, 1106, 9198, 112, 181, 155, 8954, 1112, 1770, 1112, 15210, 1116, 1105, 12385, 1180, 1129, 3836, 1121, 2743, 2977, 1939, 1104, 185, 13012, 1116, 1105, 27017, 1116, 117, 1114, 1134, 1211, 1104, 1172, 1127, 4223, 119, 1960, 1552, 8468, 11478, 5591, 1196, 1103, 1849, 1180, 1129, 2629, 1174, 119, 107]\n",
      "This ammunition, and that which I brought with me, was rapidly prepared for use at the Laboratory established at the Little Rock Arsenal for that purpose. As illustrating as the pitiful scarcity of material in the country, the fact may be stated that it was found necessary to use public documents of the State Library for cartridge paper. Gunsmiths were employed or conscripted, tools purchased or impressed, and the repair of the damaged guns I brought with me and about an equal number found at Little Rock commenced at once. But, after inspecting the work and observing the spirit of the men I decided that a garrison 500 strong could hold out against Fitch and that I would lead the remainder - about 1500 - to Gen'l Rust as soon as shotguns and rifles could be obtained from Little Rock instead of pikes and lances, with which most of them were armed. Two days elapsed before the change could be effected. \"\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face for LM without intermediary steps\n",
    "https://huggingface.co/course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1142, 1110, 102], [101, 170, 3087, 102], [101, 119, 102], [101, 1137, 1177, 102], [101, 1122, 3093, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], 'length': [4, 4, 3, 4, 4], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])\n",
      "['[CLS] this is [SEP]', '[CLS] a text [SEP]', '[CLS]. [SEP]', '[CLS] or so [SEP]', '[CLS] it seems [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# directly without intermediary steps\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "text = [\"this is a text.\", \"or so it seems\"]\n",
    "padded = tokenizer(text, max_length=4, truncation=True, return_length=True,return_overflowing_tokens=True)\n",
    "\n",
    "print(padded)\n",
    "print(padded.keys())\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 4])\n",
      "labels shape: torch.Size([5, 4])\n",
      "tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102,    0],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]]) tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102, -100],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator(padded['input_ids'])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "print(out['input_ids'], out['labels'])\n",
    "# Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Concatenate all data into one large string of text and then chunk it into context length chunks\n",
    "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
    "- https://www.youtube.com/watch?v=ma1TrR7gE7I&t=273s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                      \n",
    "            tokens = example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': (4358, 1), 'train': (36718, 1), 'validation': (3760, 1)}\n",
      "['this', 'ammunition', ',', 'and', 'that', 'which', 'i', 'brought', 'with', 'me', ',', 'was', 'rapidly', 'prepared', 'for', 'use', 'at', 'the', 'laboratory', 'established', 'at', 'the', 'little', 'rock', 'arsenal', 'for', 'that', 'purpose', '.', 'as', 'illustrating', 'as', 'the', 'pitiful', 'scarcity', 'of', 'material', 'in', 'the', 'country', ',', 'the', 'fact', 'may', 'be', 'stated', 'that', 'it', 'was', 'found', 'necessary', 'to', 'use', 'public', 'documents', 'of', 'the', 'state', 'library', 'for', 'cartridge', 'paper', '.', 'gunsmiths', 'were', 'employed', 'or', 'conscripted', ',', 'tools', 'purchased', 'or', 'impressed', ',', 'and', 'the', 'repair', 'of', 'the', 'damaged', 'guns', 'i', 'brought', 'with', 'me', 'and', 'about', 'an', 'equal', 'number', 'found', 'at', 'little', 'rock', 'commenced', 'at', 'once', '.', 'but', ',', 'after', 'inspecting', 'the', 'work', 'and', 'observing', 'the', 'spirit', 'of', 'the', 'men', 'i', 'decided', 'that', 'a', 'garrison', '500', 'strong', 'could', 'hold', 'out', 'against', 'fitch', 'and', 'that', 'i', 'would', 'lead', 'the', 'remainder', '-', 'about', '1500', '-', 'to', 'gen', \"'\", 'l', 'rust', 'as', 'soon', 'as', 'shotguns', 'and', 'rifles', 'could', 'be', 'obtained', 'from', 'little', 'rock', 'instead', 'of', 'pikes', 'and', 'lances', ',', 'with', 'which', 'most', 'of', 'them', 'were', 'armed', '.', 'two', 'days', 'elapsed', 'before', 'the', 'change', 'could', 'be', 'effected', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.shape)\n",
    "print(tokenized_dataset['train']['tokens'][88])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
