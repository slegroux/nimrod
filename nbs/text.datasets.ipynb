{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as torchtext_vocab\n",
    "from torch.utils.data import DataLoader, dataset, Dataset, random_split\n",
    "\n",
    "# L\n",
    "from lightning import LightningDataModule\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# param\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any\n",
    "from collections import Counter, OrderedDict\n",
    "from plum import dispatch\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# nimrod\n",
    "# from nimrod.models.lm import Vocab\n",
    "from nimrod.utils import set_seed\n",
    "from nimrod.data.core import DataModule\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "Each row is a list of words (sentence). For each row, extract unique character and add to vocabulary. deals with special characters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Vocab:\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to text data file\n",
    "                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters\n",
    "                ):\n",
    "\n",
    "        self.data_path = Path(data_path)\n",
    "        if not self.data_path.exists():\n",
    "            self._download_data()\n",
    "\n",
    "        logger.info(f\"Vocab: read text file\")\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        chars = set(text)\n",
    "        if specials is not None:\n",
    "            for special in specials:\n",
    "                chars.add(special)\n",
    "\n",
    "        self._stoi = {c: i for i, c in enumerate(chars)}\n",
    "        self._itos = {i: c for i, c in enumerate(chars)}\n",
    "        self.voc = chars\n",
    "\n",
    "\n",
    "    \n",
    "    def _download_data(self):\n",
    "        logger.info(f\"Vocab: download data from url\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(self.data_path, 'w') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    @dispatch\n",
    "    def stoi(self, token:str)->int:\n",
    "        if token not in self._stoi:\n",
    "            return self._stoi['<unk>']\n",
    "        return self._stoi[token]\n",
    "\n",
    "    @dispatch\n",
    "    # for list of characters\n",
    "    def stoi(self, tokens:List[str])->List[int]:\n",
    "        # TODO: deal with unknown tokens\n",
    "        return [self._stoi[tok] if tok in self._stoi else self._stoi['<unk>'] for tok in tokens]\n",
    "    \n",
    "    # @dispatch #TODO\n",
    "    # def stoi(self, tokens:List[List[str]])->List[List[int]]:\n",
    "    #     return [self._stoi[u] for tok in tokens for ]\n",
    "    # TODO:\n",
    "    # support torch tensors\n",
    "\n",
    "    @dispatch    \n",
    "    def itos(self, index:int)->str:\n",
    "        return self._itos[index]\n",
    "    \n",
    "    @dispatch    \n",
    "    def itos(self, indices:List[int])->List[str]:\n",
    "        return [self._itos[index] for index in indices]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.voc)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary(self)->Set:\n",
    "        return sorted(set([k for k,v in self._stoi.items()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "read text file into a pandas data framew with each row as a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/19/24 10:00:51] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:00:51</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">988</span> - INFO - Loading data from                    <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3273953393.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/19/24 10:00:51]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:00:51\u001b[0m,\u001b[1;36m988\u001b[0m - INFO - Loading data from                    \u001b]8;id=234053;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\u001b\\\u001b[2m3273953393.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=146316;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\u001b\\\u001b[2m12\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m                                     \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '<bos>', '<eos>', '<pad>', '<unk>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "v = Vocab('../data/text/tiny_shakespeare.txt', specials=['<pad>', '<unk>', '<bos>', '<eos>'])\n",
    "print(v.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "# egs where token * is not in vocab\n",
    "print(v.stoi('*'))\n",
    "print(v.itos(61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '<bos>', '<eos>', '<pad>', '<unk>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "[28, 54, 14, 11, 11, 18, 61, 62]\n",
      "['<bos>', 'h', 'e', 'l', 'l', 'o', '<unk>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(v.vocabulary)\n",
    "s = v.stoi([\"<bos>\",\"h\", \"e\", \"l\", \"l\", \"o\", \"*\", \"<eos>\"])\n",
    "print(s)\n",
    "print(v.itos(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Dataset\n",
    "C.f. https://karpathy.github.io/char-rnn/ text is a long continuous string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to the data file\n",
    "                context_length: int=3, # context length\n",
    "                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters\n",
    "                add_sentence_tokens: bool = True, # add special tokens to the data\n",
    "                ):\n",
    "        logger.info(f\"CharDataset: init\")\n",
    "        \n",
    "        # vocab will download data if not found\n",
    "        self.v = Vocab(data_path=data_path, specials=specials)\n",
    "        self._vocab_size = len(self.v)\n",
    "        self.context_length = context_length\n",
    "        self.special_token_pattern = re.compile(f'({re.escape(\"<bos>\")}|{re.escape(\"<eos>\")})')\n",
    "\n",
    "        with open(data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        if add_sentence_tokens:\n",
    "            # Split into sentences (roughly, using periods) and add special tokens\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            sentences = ['<bos>' + s + '<eos>' for s in sentences]\n",
    "            # Join list of words into single continuous text\n",
    "            text = \" \".join(sentences)\n",
    "\n",
    "        # text = text.replace(\"\\n\", \" \")\n",
    "        tokens = self._tokenizer(text)\n",
    "        self.data = torch.tensor(self.v.stoi(tokens), dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _tokenizer(self, text: str) -> List[str]:\n",
    "        parts = self.special_token_pattern.split(text)\n",
    "        tokens = []\n",
    "        for part in parts:\n",
    "            if part in [\"<bos>\", \"<eos>\"]:\n",
    "                tokens.append(part)\n",
    "            else:\n",
    "                tokens.extend(part)\n",
    "        return tokens\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self)->Set:\n",
    "        return sorted(set([k for k,v in self.v._stoi.items()]))\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self)->int:\n",
    "        return self._vocab_size\n",
    "\n",
    "    @property\n",
    "    def vocab_class(self)->Vocab:\n",
    "        return self.v\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # i = random.randint(0, len(self.data) - (self.context_length + 1))\n",
    "        max_index = len(self.data) - (self.context_length + 1)\n",
    "        if i > max_index:\n",
    "            # wrap around to the beginning if we hit the end\n",
    "            i = i % (max_index + 1)\n",
    "        chunk = self.data[i : i + self.context_length + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "    def to_tokens(self, message: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)\n",
    "\n",
    "    def from_tokens(self, tokens: torch.Tensor) -> str:\n",
    "        return \"\".join([self.v.itos(int(i)) for i in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/19/24 10:01:17] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:01:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">528</span> - INFO - CharDataset:                         <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2462447885.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>, context_length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, specials:       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;pad&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;unk&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;bos&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;eos&gt;'</span><span style=\"font-weight: bold\">]</span>, add_sentence_tokens: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/19/24 10:01:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:01:17\u001b[0m,\u001b[1;36m528\u001b[0m - INFO - CharDataset:                         \u001b]8;id=101414;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\u001b\\\u001b[2m2462447885.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=376417;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\u001b\\\u001b[2m10\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m, context_length: \u001b[1;36m3\u001b[0m, specials:       \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32mpad\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<unk>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<bos>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<eos\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m, add_sentence_tokens: \u001b[3;92mTrue\u001b[0m       \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:01:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">529</span> - INFO - Loading data from                    <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3273953393.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:01:17\u001b[0m,\u001b[1;36m529\u001b[0m - INFO - Loading data from                    \u001b]8;id=846335;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\u001b\\\u001b[2m3273953393.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=45561;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\u001b\\\u001b[2m12\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m                                     \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:01:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">616</span> - INFO - CharDataset:                         <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2462447885.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>, context_length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, specials:       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;unk&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;pad&gt;'</span><span style=\"font-weight: bold\">]</span>, add_sentence_tokens: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:01:17\u001b[0m,\u001b[1;36m616\u001b[0m - INFO - CharDataset:                         \u001b]8;id=396922;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\u001b\\\u001b[2m2462447885.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=82627;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\u001b\\\u001b[2m10\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m, context_length: \u001b[1;36m3\u001b[0m, specials:       \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32munk\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<pad\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m, add_sentence_tokens: \u001b[3;91mFalse\u001b[0m                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:01:17</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">617</span> - INFO - Loading data from                    <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3273953393.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:01:17\u001b[0m,\u001b[1;36m617\u001b[0m - INFO - Loading data from                    \u001b]8;id=648564;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\u001b\\\u001b[2m3273953393.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=928463;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\u001b\\\u001b[2m12\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m                                     \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  67\n",
      "1115394\n",
      "x: tensor([ 6, 37, 17]) itos:  Fir \n",
      "y: tensor([37, 17, 29]) itos:  s\n",
      "x: tensor([37, 17, 29]) itos:  irs \n",
      "y: tensor([17, 29, 35]) itos:  t\n",
      "CPU times: user 159 ms, sys: 9.46 ms, total: 168 ms\n",
      "Wall time: 168 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "block_size = 3 #context_length\n",
    "ds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['<pad>', '<unk>', '<bos>', '<eos>'], add_sentence_tokens=True)\n",
    "# just encode <unk> in case unknown characters are encountered in test set\n",
    "ds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['<unk>', '<pad>'], add_sentence_tokens=False)\n",
    "print(\"vocab size: \", ds.vocab_size)\n",
    "print(len(ds))\n",
    "for i in range(2):\n",
    "    x, y = ds[i]\n",
    "    print(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 6, 37, 17]) itos:  Fir \n",
      "y: tensor([37, 17, 29]) itos:  irs\n",
      "vocab size:  67\n",
      "vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '<pad>', '<unk>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "x,y = ds[0]\n",
    "print(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y))\n",
    "print(\"vocab size: \", ds.vocab_size)\n",
    "print(\"vocabulary: \", ds.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118332\n",
      "[894665, 111833, 111834]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset>,\n",
       " <torch.utils.data.dataset.Subset>,\n",
       " <torch.utils.data.dataset.Subset>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ds))\n",
    "t = len(ds)*torch.tensor((0.8, 0.1, 0.1))\n",
    "lengths = [int(p * len(ds)) for p in (0.8, 0.1, 0.1)]\n",
    "lengths[-1] = len(ds) - sum(lengths[:-1])\n",
    "print(lengths)\n",
    "\n",
    "random_split(ds, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CharDataModule(DataModule, LightningDataModule):\n",
    "    def __init__(self,\n",
    "            # dataset\n",
    "            data_path: str | os.PathLike = '../data/text/tiny_shakespeare.txt',\n",
    "            specials=['<pad>', '<unk>', '<bos>', '<eos>'],\n",
    "            add_sentence_tokens: bool = False,\n",
    "            # data module\n",
    "            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),\n",
    "            context_size: int = 3,\n",
    "            batch_size: int = 32,\n",
    "            num_workers: int = 1,\n",
    "            pin_memory: bool = False,\n",
    "            persistent_workers: bool = False,\n",
    "            random_split: bool = True\n",
    "            ):\n",
    "\n",
    "        logger.info(f\"CharDataModule: init\")\n",
    "\n",
    "        super().__init__(\n",
    "            train_val_test_split=train_val_test_split,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            )\n",
    "        self.save_hyperparameters()\n",
    "        self.ds: CharDataset = None\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        logger.info(\"CharDataModule: setup, split datasets\")\n",
    "        # run in each GPU process. define, split DS, etc.\n",
    "        self.ds = CharDataset(\n",
    "            self.hparams.data_path,\n",
    "            self.hparams.context_size,\n",
    "            self.hparams.specials,\n",
    "            self.hparams.add_sentence_tokens,\n",
    "            )\n",
    "        if self.hparams.random_split:\n",
    "            lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]\n",
    "            lengths[-1] = len(self.ds) - sum(lengths[:-1])\n",
    "            self.data_train, self.data_val, self.data_test = random_split(self.ds, lengths)\n",
    "        else:\n",
    "            self.data_train, self.data_val, self.data_test = self._sequential_split(self.ds, self.hparams.train_val_test_split)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self)->int:\n",
    "        if self.ds is None:\n",
    "            raise ValueError(\"Dataset not initialized\")\n",
    "        return self.ds.vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/19/24 10:04:36] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:04:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">080</span> - INFO - Initalizing CharDataModule           <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2539470188.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py#19\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/19/24 10:04:36]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:04:36\u001b[0m,\u001b[1;36m080\u001b[0m - INFO - Initalizing CharDataModule           \u001b]8;id=240174;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py\u001b\\\u001b[2m2539470188.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=861722;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py#19\u001b\\\u001b[2m19\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:04:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">081</span> - INFO - split data                           <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2539470188.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:04:36\u001b[0m,\u001b[1;36m081\u001b[0m - INFO - split data                           \u001b]8;id=69403;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py\u001b\\\u001b[2m2539470188.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=221231;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2539470188.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:04:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">082</span> - INFO - CharDataset:                         <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2462447885.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>, context_length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, specials:       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;unk&gt;'</span><span style=\"color: #000000; text-decoration-color: #000000\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'&lt;pad&gt;'</span><span style=\"font-weight: bold\">]</span>, add_sentence_tokens: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:04:36\u001b[0m,\u001b[1;36m082\u001b[0m - INFO - CharDataset:                         \u001b]8;id=752787;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py\u001b\\\u001b[2m2462447885.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=329963;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/2462447885.py#10\u001b\\\u001b[2m10\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m, context_length: \u001b[1;36m3\u001b[0m, specials:       \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m[\u001b[0m\u001b[32m'\u001b[0m\u001b[32m<\u001b[0m\u001b[32munk\u001b[0m\u001b[32m>'\u001b[0m\u001b[39m, \u001b[0m\u001b[32m'<pad\u001b[0m\u001b[32m>\u001b[0m\u001b[32m'\u001b[0m\u001b[1m]\u001b[0m, add_sentence_tokens: \u001b[3;91mFalse\u001b[0m                        \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">10:04:36</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">084</span> - INFO - Loading data from                    <a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3273953393.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         ..<span style=\"color: #800080; text-decoration-color: #800080\">/data/text/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tiny_shakespeare.txt</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;36m2024\u001b[0m-\u001b[1;36m12\u001b[0m-\u001b[1;36m19\u001b[0m \u001b[1;92m10:04:36\u001b[0m,\u001b[1;36m084\u001b[0m - INFO - Loading data from                    \u001b]8;id=927657;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py\u001b\\\u001b[2m3273953393.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=958972;file:///var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_60811/3273953393.py#12\u001b\\\u001b[2m12\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         ..\u001b[35m/data/text/\u001b[0m\u001b[95mtiny_shakespeare.txt\u001b[0m                                     \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm = CharDataModule(\n",
    "    data_path=\"../data/text/tiny_shakespeare.txt\",\n",
    "    add_sentence_tokens=False,\n",
    "    specials=['<unk>', '<pad>'],\n",
    "    context_size=3,\n",
    "    train_val_test_split = (0.8, 0.1, 0.1),\n",
    "    random_split=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fir irs 67\n"
     ]
    }
   ],
   "source": [
    "X, Y = dm.data_train[0]\n",
    "print(dm.ds.from_tokens(X), dm.ds.from_tokens(Y), dm.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([46, 20, 36]), tensor([20, 36, 36]))\n",
      "1748\n"
     ]
    }
   ],
   "source": [
    "len(dm.data_train), len(dm.data_val), len(dm.data_test)\n",
    "print(dm.data_test[0])\n",
    "print(len(dm.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (B,T):  torch.Size([64, 3]) X:  tensor([46, 20, 36]) chars:  tuf\n",
      "Y (B):  torch.Size([64, 3]) Y:  tensor([20, 36, 36]) chars:  uff\n"
     ]
    }
   ],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'nimrod.text.datasets.CharDataModule', 'data_path': '../data/text/tiny_shakespeare.txt', 'train_val_test_split': [0.8, 0.1, 0.1], 'batch_size': 64, 'context_size': 3, 'num_workers': 0, 'pin_memory': False, 'persistent_workers': False}\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load('../config/text/data/tinyshakespeare.yaml')\n",
    "print(cfg)\n",
    "dm = instantiate(cfg)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (B,T):  torch.Size([64, 20]) X:  tensor([12,  6,  4, 28,  5,  5, 13,  4, 16,  6,  6,  8, 17,  6, 22,  4,  8, 12,\n",
      "        11,  8]) chars:  he poor beetle, that\n",
      "Y (B):  torch.Size([64, 20]) Y:  tensor([ 6,  4, 28,  5,  5, 13,  4, 16,  6,  6,  8, 17,  6, 22,  4,  8, 12, 11,\n",
      "         8,  4]) chars:  e poor beetle, that \n"
     ]
    }
   ],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files=\"../data/text/tiny_shakespeare.txt\") #, split=['train','dev','test'])\n",
    "print(dataset)\n",
    "full = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 32000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_test = full.train_test_split(train_size=0.8)\n",
    "test_valid = train_test['test'].train_test_split(train_size=0.5)\n",
    "shake = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "print(shake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"There's no more to be said, but he is banish'd,\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization / Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  50257\n",
      "text row 0:  There's no more to be said, but he is banish'd,\n",
      "tokens of row 0:  ['There', \"'s\", 'no', 'more', 'to', 'be', 'said', ',', 'but', 'he', 'is', 'ban', 'ish', \"'d\", ',']\n",
      "context block & padding for lm:  {'input_ids': [[1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], [318, 3958, 680, 1549, 11]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'length': [10, 5], 'overflow_to_sample_mapping': [0, 0]}\n",
      "decode single input_id:  oth\n",
      "[\"There's no more to be said, but he\", \" is banish'd,\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"vocab size: \", len(tokenizer))\n",
    "print(\"text row 0: \", shake['test'][0]['text'])\n",
    "tokens = tokenizer.tokenize(shake['test'][0]['text'])\n",
    "print(\"tokens of row 0: \", tokens)\n",
    "\n",
    "context_length = 10\n",
    "padded = tokenizer(shake['test'][0]['text'], max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "print(\"context block & padding for lm: \", padded)\n",
    "# print(padded.keys())\n",
    "print('decode single input_id: ', tokenizer.decode(849))\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize whole dataset using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"context_length\": 10,\n",
    "    \"truncation\": True,\n",
    "    \"return_length\": True,\n",
    "    \"return_overflowing_tokens\": True,\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "# tokenizer function called via dataset map\n",
    "def tokenize_function(examples:List[dict[str,str]], cfg:OmegaConf=cfg) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=cfg.context_length,\n",
    "        truncation=cfg.truncation,\n",
    "        return_length=cfg.return_length,\n",
    "        return_overflowing_tokens=cfg.return_overflowing_tokens\n",
    "        )\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], [318, 3958, 680, 1549, 11]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'length': [10, 5], 'overflow_to_sample_mapping': [0, 0], 'word_ids': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 11, 12, 13]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(shake['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_toked = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'length': 10, 'overflow_to_sample_mapping': 0, 'word_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "['There', \"'s\", ' no', ' more', ' to', ' be', ' said', ',', ' but', ' he']\n",
      "There's no more to be said, but he\n"
     ]
    }
   ],
   "source": [
    "print(shake_toked['test'][0])\n",
    "print([tokenizer.decode(x) for x in shake_toked['test'][0]['input_ids']])\n",
    "print(tokenizer.decode(shake_toked['test'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 42263\n",
      "})\n",
      "test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 5276\n",
      "})\n",
      "valid Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 5295\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for split, dset in shake_toked.items():\n",
    "    print(split, dset)\n",
    "    arr_len = np.sum(dset['length'], dtype=np.uint64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769],\n",
      "        [   13,   314, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'labels': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769],\n",
      "        [   13,   314,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator([shake_toked[\"test\"]['input_ids'][i] for i in range(5)])\n",
    "# out = default_data_collator(shake_toked['test']['input_ids'][0])\n",
    "print(out)\n",
    "# for key in out:\n",
    "#     print(f\"{key} shape: {out[key].shape}\")\n",
    "# print('inputs: ', out['input_ids'])\n",
    "# print('labels: ', out['labels'])\n",
    "\n",
    "# data_collator = DefaultDataCollator(tokenizer)\n",
    "# out = data_collator([shake_toked[\"test\"][i] for i in range(5)])\n",
    "# print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(examples, block_size: int, **kwargs):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There's no more to be said, but he is banish'd,\"]\n"
     ]
    }
   ],
   "source": [
    "example = shake['test'][0]\n",
    "# concatenated_examples = {k: sum(example[k], []) for k in example.keys()}\n",
    "print([example[k] for k in example.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769]]), 'labels': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769]])}\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([shake_toked['test']['input_ids'][i] for i in range(4)])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', \"'s\", ' no', ' more', ' to', ' be', ' said', ',', ' but', ' he']\n",
      "[' is', ' ban', 'ish', \"'d\", ',', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "['I', ' bid', ' them', ' that', ' did', ' love', ' their', ' country', \"'s\", ' good']\n",
      "['N', 'urse', ',', ' commend', ' me', ' to', ' thy', ' lady', ' and', ' mistress']\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print([tokenizer.decode(x) for x in out['input_ids'][i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    shake_toked['test']['input_ids'],\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head ../data/text/tiny_shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "tensor([  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256])\n",
      "tensor([ 318, 3958,  680, 1549,   11, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(test_dl))\n",
    "print(b['input_ids'].shape)\n",
    "print(b['input_ids'][1])\n",
    "print(b['labels'][1])\n",
    "# for i in range(128):\n",
    "#     print([tokenizer.decode(x) for x in b['input_ids'][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4358 <class 'datasets.arrow_dataset.Dataset'> Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4358\n",
      "})\n",
      "{'text': ' Du Fu \\'s popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him . While there was never another Du Fu , individual poets followed in the traditions of specific aspects of his work : Bai Juyi \\'s concern for the poor , Lu You \\'s patriotism , and Mei Yaochen \\'s reflections on the quotidian are a few examples . More broadly , Du Fu \\'s work in transforming the lshi from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre . \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), type(dataset), dataset)\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Du Fu \\'s popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him . While there was never another Du Fu , individual poets followed in the traditions of specific aspects of his work : Bai Juyi \\'s concern for the poor , Lu You \\'s patriotism , and Mei Yaochen \\'s reflections on the quotidian are a few examples . More broadly , Du Fu \\'s work in transforming the lshi from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre . \\n'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from torchtext\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(root='../data/text', split='test')\n",
    "tokenizer = get_tokenizer('basic_english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab(tokenizer('this is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate all sentences together\n",
    "def data_process(raw_text_iter) -> torch.Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(train_iter):\n",
    "#     print(idx, i)\n",
    "#     print(vocab(tokenizer(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = WikiText2()\n",
    "# train_data = data_process(train_iter)\n",
    "# val_data = data_process(val_iter)\n",
    "# test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: torch.Tensor, bsz: int) -> torch.Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = batchify(test_data, 10)\n",
    "# print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: torch.Tensor, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch(test_data, 0)\n",
    "# print(\"x: \", x[:2])\n",
    "# print(\"y: \", y[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based tokenization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# # vocab.set_default_index(vocab['<unk>'])\n",
    "# tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n",
    "# fn_kwargs={'tokenizer': tokenizer})\n",
    "# print(tokenized_dataset['train'][88]['tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Du', 'Fu', \"'\", 's', 'popularity', 'grew', 'to', 'such', 'an', 'extent', 'that', 'it', 'is', 'as', 'hard', 'to', 'measure', 'his', 'influence', 'as', 'that', 'of', 'Shakespeare', 'in', 'England', ':', 'it', 'was', 'hard', 'for', 'any', 'Chinese', 'poet', 'not', 'to', 'be', 'influenced', 'by', 'him', '.', 'While', 'there', 'was', 'never', 'another', 'Du', 'Fu', ',', 'individual', 'poets', 'followed', 'in', 'the', 'traditions', 'of', 'specific', 'aspects', 'of', 'his', 'work', ':', 'Bai', 'Ju', '##yi', \"'\", 's', 'concern', 'for', 'the', 'poor', ',', 'Lu', 'You', \"'\", 's', 'pat', '##riot', '##ism', ',', 'and', 'Mei', 'Yao', '##chen', \"'\", 's', 'reflections', 'on', 'the', 'q', '##uo', '##ti', '##dian', 'are', 'a', 'few', 'examples', '.', 'More', 'broadly', ',', 'Du', 'Fu', \"'\", 's', 'work', 'in', 'transforming', 'the', '[UNK]', 'from', 'mere', 'word', 'play', 'into', '\"', 'a', 'vehicle', 'for', 'serious', 'poetic', 'utter', '##ance', '\"', 'set', 'the', 'stage', 'for', 'every', 'subsequent', 'writer', 'in', 'the', 'genre', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokens = tokenizer.tokenize(dataset[100]['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
    "# min_freq=3) \n",
    "# vocab.insert_token('<unk>', 0)           \n",
    "# vocab.insert_token('<eos>', 1)            \n",
    "# vocab.set_default_index(vocab['<unk>'])   \n",
    "# print(len(vocab))                         \n",
    "# print(vocab.get_itos()[:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12786, 14763, 112, 188, 5587, 2580, 1106, 1216, 1126, 6102, 1115, 1122, 1110, 1112, 1662, 1106, 4929, 1117, 2933, 1112, 1115, 1104, 7647, 1107, 1652, 131, 1122, 1108, 1662, 1111, 1251, 1922, 4225, 1136, 1106, 1129, 4401, 1118, 1140, 119, 1799, 1175, 1108, 1309, 1330, 12786, 14763, 117, 2510, 11587, 1723, 1107, 1103, 7181, 1104, 2747, 5402, 1104, 1117, 1250, 131, 27900, 23915, 10279, 112, 188, 4517, 1111, 1103, 2869, 117, 14557, 1192, 112, 188, 26227, 23326, 1863, 117, 1105, 24563, 27762, 10415, 112, 188, 26906, 1113, 1103, 186, 11848, 3121, 10359, 1132, 170, 1374, 5136, 119, 3046, 14548, 117, 12786, 14763, 112, 188, 1250, 1107, 20892, 1103, 100, 1121, 8574, 1937, 1505, 1154, 107, 170, 3686, 1111, 3021, 15751, 15462, 3923, 107, 1383, 1103, 2016, 1111, 1451, 4194, 2432, 1107, 1103, 6453, 119]\n",
      "Du Fu's popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him. While there was never another Du Fu, individual poets followed in the traditions of specific aspects of his work : Bai Juyi's concern for the poor, Lu You's patriotism, and Mei Yaochen's reflections on the quotidian are a few examples. More broadly, Du Fu's work in transforming the [UNK] from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre.\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face for LM without intermediary steps\n",
    "https://huggingface.co/course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1142, 1110, 102], [101, 170, 3087, 102], [101, 119, 102], [101, 1137, 1177, 102], [101, 1122, 3093, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], 'length': [4, 4, 3, 4, 4], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])\n",
      "['[CLS] this is [SEP]', '[CLS] a text [SEP]', '[CLS]. [SEP]', '[CLS] or so [SEP]', '[CLS] it seems [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# directly without intermediary steps\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "text = [\"this is a text.\", \"or so it seems\"]\n",
    "padded = tokenizer(text, max_length=4, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "\n",
    "print(padded)\n",
    "print(padded.keys())\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 4])\n",
      "labels shape: torch.Size([5, 4])\n",
      "tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102,    0],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]]) tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102, -100],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator(padded['input_ids'])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "print(out['input_ids'], out['labels'])\n",
    "# Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Concatenate all data into one large string of text and then chunk it into context length chunks\n",
    "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
    "- https://www.youtube.com/watch?v=ma1TrR7gE7I&t=273s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                      \n",
    "            tokens = example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024\n",
    "# train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "# test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_dataset.shape)\n",
    "# print(tokenized_dataset['train']['tokens'][88])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling dataset\n",
    "\n",
    "Basically concatenate all data into one big array of ids and then create block_lengths inputs. shift for corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 32000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(shake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples:List[dict[str,str]]) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(examples[\"text\"]) #, max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 32000\n",
      "}) <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenized['train'], type(tokenized['train']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 32000\n",
      "})\n",
      "238064\n",
      "test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "267859\n",
      "valid Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "298027\n",
      "[8496, 674, 826, 46258, 2988, 318, 1716, 13, 1870, 373, 379, 938, 503, 12, 24903]\n"
     ]
    }
   ],
   "source": [
    "all = []\n",
    "for k,v in tokenized.items():\n",
    "    print(k, v)\n",
    "    for x in v['input_ids']:\n",
    "        all += x\n",
    "    print(len(all))\n",
    "print(all[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10]) torch.Size([16, 10])\n",
      "tensor([  790, 32460, 33769,   314,   787,   817,   272,   428,  4939,  3211]) tensor([32460, 33769,   314,   787,   817,   272,   428,  4939,  3211,    25])\n",
      " every tedious stride I makeThan this weak arm  tedious stride I makeThan this weak arm:\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(np.array(all), 16, 10)\n",
    "print(x.shape, y.shape)\n",
    "print(x[0], y[0])\n",
    "print(tokenizer.decode(x[0]), tokenizer.decode(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nimrod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
