{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab as torchtext_vocab\n",
    "from torch.utils.data import DataLoader, dataset, Dataset, random_split\n",
    "\n",
    "# L\n",
    "from lightning import LightningDataModule\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# param\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any\n",
    "from collections import Counter, OrderedDict\n",
    "from plum import dispatch\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# nimrod\n",
    "# from nimrod.models.lm import Vocab\n",
    "from nimrod.utils import set_seed\n",
    "from nimrod.data.core import DataModule\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "Each row is a list of words (sentence). For each row, extract unique character and add to vocabulary. deals with special characters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Vocab:\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to text data file\n",
    "                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters\n",
    "                ):\n",
    "\n",
    "        self.data_path = Path(data_path)\n",
    "        if not self.data_path.exists():\n",
    "            self._download_data()\n",
    "\n",
    "        logger.info(f\"Vocab: read text file\")\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        chars = set(text)\n",
    "        if specials is not None:\n",
    "            for special in specials:\n",
    "                chars.add(special)\n",
    "\n",
    "        self._stoi = {c: i for i, c in enumerate(chars)}\n",
    "        self._itos = {i: c for i, c in enumerate(chars)}\n",
    "        self.voc = chars\n",
    "\n",
    "\n",
    "    \n",
    "    def _download_data(self):\n",
    "        logger.info(f\"Vocab: download data from url\")\n",
    "        url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        with open(self.data_path, 'w') as f:\n",
    "            f.write(response.text)\n",
    "\n",
    "    @dispatch\n",
    "    def stoi(self, token:str)->int:\n",
    "        if token not in self._stoi:\n",
    "            return self._stoi['<unk>']\n",
    "        return self._stoi[token]\n",
    "\n",
    "    @dispatch\n",
    "    # for list of characters\n",
    "    def stoi(self, tokens:List[str])->List[int]:\n",
    "        # TODO: deal with unknown tokens\n",
    "        return [self._stoi[tok] if tok in self._stoi else self._stoi['<unk>'] for tok in tokens]\n",
    "    \n",
    "    # @dispatch #TODO\n",
    "    # def stoi(self, tokens:List[List[str]])->List[List[int]]:\n",
    "    #     return [self._stoi[u] for tok in tokens for ]\n",
    "    # TODO:\n",
    "    # support torch tensors\n",
    "\n",
    "    @dispatch    \n",
    "    def itos(self, index:int)->str:\n",
    "        return self._itos[index]\n",
    "    \n",
    "    @dispatch    \n",
    "    def itos(self, indices:List[int])->List[str]:\n",
    "        return [self._itos[index] for index in indices]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.voc)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary(self)->Set:\n",
    "        return sorted(set([k for k,v in self._stoi.items()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "read text file into a pandas data framew with each row as a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Vocab('../data/text/tiny_shakespeare.txt', specials=['<pad>', '<unk>', '<bos>', '<eos>'])\n",
    "print(v.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# egs where token * is not in vocab\n",
    "print(v.stoi('*'))\n",
    "print(v.itos(61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.vocabulary)\n",
    "s = v.stoi([\"<bos>\",\"h\", \"e\", \"l\", \"l\", \"o\", \"*\", \"<eos>\"])\n",
    "print(s)\n",
    "print(v.itos(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Dataset\n",
    "C.f. https://karpathy.github.io/char-rnn/ text is a long continuous string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to the data file\n",
    "                context_length: int=3, # context length\n",
    "                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters\n",
    "                add_sentence_tokens: bool = True, # add special tokens to the data\n",
    "                ):\n",
    "        logger.info(f\"CharDataset: init\")\n",
    "        \n",
    "        # vocab will download data if not found\n",
    "        self.v = Vocab(data_path=data_path, specials=specials)\n",
    "        self._vocab_size = len(self.v)\n",
    "        self.context_length = context_length\n",
    "        self.special_token_pattern = re.compile(f'({re.escape(\"<bos>\")}|{re.escape(\"<eos>\")})')\n",
    "\n",
    "        with open(data_path, 'r') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        if add_sentence_tokens:\n",
    "            # Split into sentences (roughly, using periods) and add special tokens\n",
    "            sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "            sentences = ['<bos>' + s + '<eos>' for s in sentences]\n",
    "            # Join list of words into single continuous text\n",
    "            text = \" \".join(sentences)\n",
    "\n",
    "        # text = text.replace(\"\\n\", \" \")\n",
    "        tokens = self._tokenizer(text)\n",
    "        self.data = torch.tensor(self.v.stoi(tokens), dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def _tokenizer(self, text: str) -> List[str]:\n",
    "        parts = self.special_token_pattern.split(text)\n",
    "        tokens = []\n",
    "        for part in parts:\n",
    "            if part in [\"<bos>\", \"<eos>\"]:\n",
    "                tokens.append(part)\n",
    "            else:\n",
    "                tokens.extend(part)\n",
    "        return tokens\n",
    "\n",
    "    @property\n",
    "    def vocabulary(self)->Set:\n",
    "        return sorted(set([k for k,v in self.v._stoi.items()]))\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self)->int:\n",
    "        return self._vocab_size\n",
    "\n",
    "    @property\n",
    "    def vocab_class(self)->Vocab:\n",
    "        return self.v\n",
    "\n",
    "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # i = random.randint(0, len(self.data) - (self.context_length + 1))\n",
    "        max_index = len(self.data) - (self.context_length + 1)\n",
    "        if i > max_index:\n",
    "            # wrap around to the beginning if we hit the end\n",
    "            i = i % (max_index + 1)\n",
    "        chunk = self.data[i : i + self.context_length + 1]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "    def to_tokens(self, message: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)\n",
    "\n",
    "    def from_tokens(self, tokens: torch.Tensor) -> str:\n",
    "        return \"\".join([self.v.itos(int(i)) for i in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "block_size = 3 #context_length\n",
    "ds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['<pad>', '<unk>', '<bos>', '<eos>'], add_sentence_tokens=True)\n",
    "# just encode <unk> in case unknown characters are encountered in test set\n",
    "ds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['<unk>', '<pad>'], add_sentence_tokens=False)\n",
    "print(\"vocab size: \", ds.vocab_size)\n",
    "print(len(ds))\n",
    "for i in range(2):\n",
    "    x, y = ds[i]\n",
    "    print(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y)[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = ds[0]\n",
    "print(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y))\n",
    "print(\"vocab size: \", ds.vocab_size)\n",
    "print(\"vocabulary: \", ds.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ds))\n",
    "t = len(ds)*torch.tensor((0.8, 0.1, 0.1))\n",
    "lengths = [int(p * len(ds)) for p in (0.8, 0.1, 0.1)]\n",
    "lengths[-1] = len(ds) - sum(lengths[:-1])\n",
    "print(lengths)\n",
    "\n",
    "random_split(ds, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CharDataModule(DataModule, LightningDataModule):\n",
    "    def __init__(self,\n",
    "            # dataset\n",
    "            data_path: str | os.PathLike = '../data/text/tiny_shakespeare.txt',\n",
    "            specials=['<pad>', '<unk>', '<bos>', '<eos>'],\n",
    "            add_sentence_tokens: bool = False,\n",
    "            # data module\n",
    "            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),\n",
    "            context_size: int = 3,\n",
    "            batch_size: int = 32,\n",
    "            num_workers: int = 1,\n",
    "            pin_memory: bool = False,\n",
    "            persistent_workers: bool = False,\n",
    "            random_split: bool = True\n",
    "            ):\n",
    "\n",
    "        logger.info(f\"CharDataModule: init\")\n",
    "\n",
    "        super().__init__(\n",
    "            train_val_test_split=train_val_test_split,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            )\n",
    "        self.save_hyperparameters()\n",
    "        self.ds: CharDataset = None\n",
    "    \n",
    "    def prepare_data(self) -> None:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        logger.info(\"CharDataModule: setup, split datasets\")\n",
    "        # run in each GPU process. define, split DS, etc.\n",
    "        self.ds = CharDataset(\n",
    "            self.hparams.data_path,\n",
    "            self.hparams.context_size,\n",
    "            self.hparams.specials,\n",
    "            self.hparams.add_sentence_tokens,\n",
    "            )\n",
    "        if self.hparams.random_split:\n",
    "            lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]\n",
    "            lengths[-1] = len(self.ds) - sum(lengths[:-1])\n",
    "            self.data_train, self.data_val, self.data_test = random_split(self.ds, lengths)\n",
    "        else:\n",
    "            self.data_train, self.data_val, self.data_test = self._sequential_split(self.ds, self.hparams.train_val_test_split)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self)->int:\n",
    "        if self.ds is None:\n",
    "            raise ValueError(\"Dataset not initialized\")\n",
    "        return self.ds.vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CharDataModule(\n",
    "    data_path=\"../data/text/tiny_shakespeare.txt\",\n",
    "    add_sentence_tokens=False,\n",
    "    specials=['<unk>', '<pad>'],\n",
    "    context_size=3,\n",
    "    train_val_test_split = (0.8, 0.1, 0.1),\n",
    "    random_split=False,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    persistent_workers=False,\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = dm.data_train[0]\n",
    "print(dm.ds.from_tokens(X), dm.ds.from_tokens(Y), dm.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dm.data_train), len(dm.data_val), len(dm.data_test)\n",
    "print(dm.data_test[0])\n",
    "print(len(dm.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('../config/text/data/tinyshakespeare.yaml')\n",
    "print(cfg)\n",
    "dm = instantiate(cfg)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"text\", data_files=\"../data/text/tiny_shakespeare.txt\") #, split=['train','dev','test'])\n",
    "print(dataset)\n",
    "full = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = full.train_test_split(train_size=0.8)\n",
    "test_valid = train_test['test'].train_test_split(train_size=0.5)\n",
    "shake = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "print(shake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization / Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"vocab size: \", len(tokenizer))\n",
    "print(\"text row 0: \", shake['test'][0]['text'])\n",
    "tokens = tokenizer.tokenize(shake['test'][0]['text'])\n",
    "print(\"tokens of row 0: \", tokens)\n",
    "\n",
    "context_length = 10\n",
    "padded = tokenizer(shake['test'][0]['text'], max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "print(\"context block & padding for lm: \", padded)\n",
    "# print(padded.keys())\n",
    "print('decode single input_id: ', tokenizer.decode(849))\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize whole dataset using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"context_length\": 10,\n",
    "    \"truncation\": True,\n",
    "    \"return_length\": True,\n",
    "    \"return_overflowing_tokens\": True,\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "# tokenizer function called via dataset map\n",
    "def tokenize_function(examples:List[dict[str,str]], cfg:OmegaConf=cfg) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=cfg.context_length,\n",
    "        truncation=cfg.truncation,\n",
    "        return_length=cfg.return_length,\n",
    "        return_overflowing_tokens=cfg.return_overflowing_tokens\n",
    "        )\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_function(shake['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_toked = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shake_toked['test'][0])\n",
    "print([tokenizer.decode(x) for x in shake_toked['test'][0]['input_ids']])\n",
    "print(tokenizer.decode(shake_toked['test'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split, dset in shake_toked.items():\n",
    "    print(split, dset)\n",
    "    arr_len = np.sum(dset['length'], dtype=np.uint64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator([shake_toked[\"test\"]['input_ids'][i] for i in range(5)])\n",
    "# out = default_data_collator(shake_toked['test']['input_ids'][0])\n",
    "print(out)\n",
    "# for key in out:\n",
    "#     print(f\"{key} shape: {out[key].shape}\")\n",
    "# print('inputs: ', out['input_ids'])\n",
    "# print('labels: ', out['labels'])\n",
    "\n",
    "# data_collator = DefaultDataCollator(tokenizer)\n",
    "# out = data_collator([shake_toked[\"test\"][i] for i in range(5)])\n",
    "# print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(examples, block_size: int, **kwargs):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = shake['test'][0]\n",
    "# concatenated_examples = {k: sum(example[k], []) for k in example.keys()}\n",
    "print([example[k] for k in example.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([shake_toked['test']['input_ids'][i] for i in range(4)])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print([tokenizer.decode(x) for x in out['input_ids'][i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    shake_toked['test']['input_ids'],\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head ../data/text/tiny_shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(test_dl))\n",
    "print(b['input_ids'].shape)\n",
    "print(b['input_ids'][1])\n",
    "print(b['labels'][1])\n",
    "# for i in range(128):\n",
    "#     print([tokenizer.decode(x) for x in b['input_ids'][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset), type(dataset), dataset)\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from torchtext\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(root='../data/text', split='test')\n",
    "tokenizer = get_tokenizer('basic_english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab(tokenizer('this is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate all sentences together\n",
    "def data_process(raw_text_iter) -> torch.Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(train_iter):\n",
    "#     print(idx, i)\n",
    "#     print(vocab(tokenizer(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = WikiText2()\n",
    "# train_data = data_process(train_iter)\n",
    "# val_data = data_process(val_iter)\n",
    "# test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: torch.Tensor, bsz: int) -> torch.Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = batchify(test_data, 10)\n",
    "# print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: torch.Tensor, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch(test_data, 0)\n",
    "# print(\"x: \", x[:2])\n",
    "# print(\"y: \", y[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based tokenization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# # vocab.set_default_index(vocab['<unk>'])\n",
    "# tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n",
    "# fn_kwargs={'tokenizer': tokenizer})\n",
    "# print(tokenized_dataset['train'][88]['tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokens = tokenizer.tokenize(dataset[100]['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
    "# min_freq=3) \n",
    "# vocab.insert_token('<unk>', 0)           \n",
    "# vocab.insert_token('<eos>', 1)            \n",
    "# vocab.set_default_index(vocab['<unk>'])   \n",
    "# print(len(vocab))                         \n",
    "# print(vocab.get_itos()[:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face for LM without intermediary steps\n",
    "https://huggingface.co/course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly without intermediary steps\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "text = [\"this is a text.\", \"or so it seems\"]\n",
    "padded = tokenizer(text, max_length=4, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "\n",
    "print(padded)\n",
    "print(padded.keys())\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator(padded['input_ids'])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "print(out['input_ids'], out['labels'])\n",
    "# Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Concatenate all data into one large string of text and then chunk it into context length chunks\n",
    "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
    "- https://www.youtube.com/watch?v=ma1TrR7gE7I&t=273s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                      \n",
    "            tokens = example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024\n",
    "# train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "# test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_dataset.shape)\n",
    "# print(tokenized_dataset['train']['tokens'][88])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling dataset\n",
    "\n",
    "Basically concatenate all data into one big array of ids and then create block_lengths inputs. shift for corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples:List[dict[str,str]]) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(examples[\"text\"]) #, max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized['train'], type(tokenized['train']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = []\n",
    "for k,v in tokenized.items():\n",
    "    print(k, v)\n",
    "    for x in v['input_ids']:\n",
    "        all += x\n",
    "    print(len(all))\n",
    "print(all[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(np.array(all), 16, 10)\n",
    "print(x.shape, y.shape)\n",
    "print(x[0], y[0])\n",
    "print(tokenizer.decode(x[0]), tokenizer.decode(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
