{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp text.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "\n",
    "from torch.optim import SGD\n",
    "import torchtext; torchtext.disable_torchtext_deprecation_warning()\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from torch.utils.data import DataLoader, dataset, Dataset, random_split\n",
    "\n",
    "# pl\n",
    "from lightning import LightningDataModule, seed_everything\n",
    "\n",
    "\n",
    "# hf\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DefaultDataCollator, default_data_collator\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ui\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# param\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "# python\n",
    "from typing import Dict, List, Tuple, Optional, Set, Union\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from plum import dispatch\n",
    "import urllib\n",
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "# nimrod\n",
    "from nimrod.data.utils import DataModule, split_train_valid_test\n",
    "\n",
    "# conf\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from typing import Dict, List, Tuple, Optional, Set, Any\n",
    "\n",
    "# nimrod\n",
    "# from nimrod.models.lm import Vocab\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab\n",
    "Each row is a list of words (sentence). For each row, extract unique character and add to vocabulary. deals with special characters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Vocab:\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike, # path to text data file\n",
    "                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters\n",
    "                add_sentence_tokens=True, # add <bos> and <eos> tokens to each sentence\n",
    "                ):\n",
    "        # read data\n",
    "        df = pd.read_fwf(data_path, header=None, names=['text'])\n",
    "        if add_sentence_tokens:\n",
    "            df.loc[:, 'text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])\n",
    "        data = list(df['text'])\n",
    "        # count individual tokens\n",
    "        c = Counter()\n",
    "        for row in data:\n",
    "            for token in row:\n",
    "                c.update(token)\n",
    "        ordered_tuple = sorted(c.items(), key=lambda x:x[1], reverse=True)\n",
    "        dict = OrderedDict(ordered_tuple)        \n",
    "        # leverage torchtext vocab\n",
    "        self.voc = vocab(dict, specials=specials)\n",
    "        if '<unk>' in specials:\n",
    "            self.voc.set_default_index(self.voc['<unk>'])\n",
    "        else:\n",
    "            self.voc.set_default_index(-1)\n",
    "        self._stoi = self.voc.get_stoi()\n",
    "        self._itos = self.voc.get_itos()\n",
    "\n",
    "    @dispatch\n",
    "    def stoi(self, token:str)->int:\n",
    "        if len(token) > 1 and token not in ['<pad>', '<unk>', '<bos>', '<eos>']:\n",
    "            raise ValueError(\"input should be a token or list of tokens\")\n",
    "        return self._stoi[token]\n",
    "\n",
    "    @dispatch\n",
    "    # for list of characters\n",
    "    def stoi(self, tokens:List[str])->List[int]:\n",
    "        return [self._stoi[tok] for tok in tokens]\n",
    "    \n",
    "    # @dispatch #TODO\n",
    "    # def stoi(self, tokens:List[List[str]])->List[List[int]]:\n",
    "    #     return [self._stoi[u] for tok in tokens for ]\n",
    "    # TODO:\n",
    "    # support torch tensors\n",
    "\n",
    "    @dispatch    \n",
    "    def itos(self, index:int)->str:\n",
    "        return self._itos[index]\n",
    "    \n",
    "    @dispatch    \n",
    "    def itos(self, indices:List[int])->List[str]:\n",
    "        return [self._itos[index] for index in indices]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.voc)\n",
    "    \n",
    "    @property\n",
    "    def vocabulary(self)->Set:\n",
    "        return sorted(set([k for k,v in self._stoi.items()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "read text file into a pandas data framew with each row as a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Vocab('../data/text/tiny_shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '<', '<bos>', '<eos>', '<pad>', '<unk>', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "[12, 6, 17, 17, 5]\n",
      "['h', 'e', 'l', 'l', 'o']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[34, 12, 6, 4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(v.vocabulary)\n",
    "s = v.stoi([\"h\", \"e\", \"l\", \"l\", \"o\"])\n",
    "print(s)\n",
    "print(v.itos(s))\n",
    "v.stoi(['T', 'h', 'e', ' '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Dataset\n",
    "After Karpathy chatGPT tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['a', 'b'] c\n",
      "['b', 'c'] d\n"
     ]
    }
   ],
   "source": [
    "# [\"a\", \"b\", \"c\"]\n",
    "# a, b | c\n",
    "\n",
    "\n",
    "def test(row:List[str], n_context:int):\n",
    "    i = 0\n",
    "    maxi = len(row) - n_context\n",
    "    print(maxi)\n",
    "    while i < maxi:\n",
    "        print(row[i:i+n_context], row[i+n_context])\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "test([\"a\", \"b\", \"c\", \"d\"], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                data_path: str | os.PathLike, # path to the data file\n",
    "                context_length: int, # context length\n",
    "                vocab: Vocab, # vocab object\n",
    "                add_sentence_tokens: bool = True, # add special tokens to the data\n",
    "                verbose: bool = False, # print info\n",
    "                ):\n",
    "        \n",
    "        df = pd.read_fwf(data_path, header=None, names=['text'])\n",
    "\n",
    "        if add_sentence_tokens:\n",
    "            df['text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])\n",
    "\n",
    "        data = list(df['text'])\n",
    "        self.data = []\n",
    "        # flatten list of list of chars into one big list of chars with <bos> and <eos>\n",
    "        for row in data:\n",
    "            self.data.extend(row)\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.v = vocab\n",
    "        self.vocab_size = len(self.v)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        i = random.randint(0, len(self.data) - (self.context_length + 1))\n",
    "        chunk = self.data[i : i + self.context_length + 1]\n",
    "        dix = self.v.stoi(chunk)\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "    def to_tokens(self, message: str) -> torch.Tensor:\n",
    "        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)\n",
    "\n",
    "    def from_tokens(self, tokens: torch.Tensor) -> str:\n",
    "        return \"\".join([self.v.itos(int(i)) for i in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with urllib.request.urlopen(\"https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\") as f:\n",
    "#     text = f.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  me in my course.<eos><bos>Why I descend into th \n",
      "y: me in my course.<eos><bos>Why I descend into thi\n"
     ]
    }
   ],
   "source": [
    "block_size = 40 #context_length\n",
    "ds = CharDataset('../data/text/tiny_shakespeare.txt', block_size, v)\n",
    "x,y = ds[2]\n",
    "print(\"x:\",  ds.from_tokens(x), \"\\ny:\", ds.from_tokens(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1140888\n",
      "[912710, 114088, 114090]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset>,\n",
       " <torch.utils.data.dataset.Subset>,\n",
       " <torch.utils.data.dataset.Subset>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ds))\n",
    "t = len(ds)*torch.tensor((0.8, 0.1, 0.1))\n",
    "lengths = [int(p * len(ds)) for p in (0.8, 0.1, 0.1)]\n",
    "lengths[-1] = len(ds) - sum(lengths[:-1])\n",
    "print(lengths)\n",
    "\n",
    "random_split(ds, lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class CharDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: str | os.PathLike,\n",
    "            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),\n",
    "            context_size: int = 3,\n",
    "            batch_size: int = 32,\n",
    "            num_workers: int = 0,\n",
    "            pin_memory: bool = False,\n",
    "            persistent_workers: bool = False,\n",
    "            ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.train_ds: Optional[Dataset] = None\n",
    "        self.val_ds: Optional[Dataset] = None\n",
    "        self.test_ds: Optional[Dataset] = None\n",
    "        self.ds: Optional[Dataset] = None\n",
    "        # we extract vocab from the full char dataset\n",
    "        # TODO: add option to pass in vocab\n",
    "        self.v = Vocab(data_path)\n",
    "\n",
    "        if sum(train_val_test_split) != 1.0:\n",
    "            raise ValueError(\"train_val_test_split must sum to 1.0\")\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # run in main process. download, tokenize, etc.\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        # run in each GPU process. define, split DS, etc.\n",
    "        self.ds = CharDataset(self.hparams.data_path, self.hparams.context_size, self.v)\n",
    "        lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]\n",
    "        lengths[-1] = len(self.ds) - sum(lengths[:-1])\n",
    "\n",
    "        self.train_ds, self.val_ds, self.test_ds = random_split(self.ds, lengths)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    " \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_ds,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "            persistent_workers=self.hparams.persistent_workers\n",
    "        )\n",
    " \n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        return super().teardown(stage)\n",
    "\n",
    "    def state_dict(self) -> Dict[str, Any]:\n",
    "        return super().state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n",
    "        return super().load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return super().forward(x)\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n",
    "        return super().training_step(batch, batch_idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = CharDataModule(\n",
    "    \"../data/text/tiny_shakespeare.txt\",\n",
    "    batch_size=64,\n",
    "    context_size=20,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    )\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (B,T):  torch.Size([64, 20]) X:  tensor([11, 17, 13,  6, 11, 18, 21, 49,  4,  5, 13,  4, 15,  7,  4, 15,  8,  4,\n",
      "        24,  6]) chars:  already? or is it fe\n",
      "Y (B):  torch.Size([64, 20]) Y:  tensor([17, 13,  6, 11, 18, 21, 49,  4,  5, 13,  4, 15,  7,  4, 15,  8,  4, 24,\n",
      "         6, 11]) chars:  lready? or is it fea\n"
     ]
    }
   ],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init from config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = OmegaConf.load('../config/text/data/tinyshakespeare.yaml')\n",
    "# print(cfg)\n",
    "# dm = instantiate(cfg)\n",
    "# dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (B,T):  torch.Size([64, 20]) X:  tensor([12,  6,  4, 28,  5,  5, 13,  4, 16,  6,  6,  8, 17,  6, 22,  4,  8, 12,\n",
      "        11,  8]) chars:  he poor beetle, that\n",
      "Y (B):  torch.Size([64, 20]) Y:  tensor([ 6,  4, 28,  5,  5, 13,  4, 16,  6,  6,  8, 17,  6, 22,  4,  8, 12, 11,\n",
      "         8,  4]) chars:  e poor beetle, that \n"
     ]
    }
   ],
   "source": [
    "test_dl = dm.test_dataloader()\n",
    "X,Y = next(iter(test_dl))\n",
    "print(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\n",
    "print( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face\n",
    "https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 40000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files=\"../data/text/tiny_shakespeare.txt\") #, split=['train','dev','test'])\n",
    "print(dataset)\n",
    "full = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 32000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_test = full.train_test_split(train_size=0.8)\n",
    "test_valid = train_test['test'].train_test_split(train_size=0.5)\n",
    "shake = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "print(shake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"There's no more to be said, but he is banish'd,\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shake['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization / Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  50257\n",
      "text row 0:  There's no more to be said, but he is banish'd,\n",
      "tokens of row 0:  ['There', \"'s\", 'Ġno', 'Ġmore', 'Ġto', 'Ġbe', 'Ġsaid', ',', 'Ġbut', 'Ġhe', 'Ġis', 'Ġban', 'ish', \"'d\", ',']\n",
      "context block & padding for lm:  {'input_ids': [[1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], [318, 3958, 680, 1549, 11]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'length': [10, 5], 'overflow_to_sample_mapping': [0, 0]}\n",
      "decode single input_id:  oth\n",
      "[\"There's no more to be said, but he\", \" is banish'd,\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "print(\"vocab size: \", len(tokenizer))\n",
    "print(\"text row 0: \", shake['test'][0]['text'])\n",
    "tokens = tokenizer.tokenize(shake['test'][0]['text'])\n",
    "print(\"tokens of row 0: \", tokens)\n",
    "\n",
    "context_length = 10\n",
    "padded = tokenizer(shake['test'][0]['text'], max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "print(\"context block & padding for lm: \", padded)\n",
    "# print(padded.keys())\n",
    "print('decode single input_id: ', tokenizer.decode(849))\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize whole dataset using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"context_length\": 10,\n",
    "    \"truncation\": True,\n",
    "    \"return_length\": True,\n",
    "    \"return_overflowing_tokens\": True,\n",
    "}\n",
    "\n",
    "cfg = OmegaConf.create(cfg)\n",
    "\n",
    "# tokenizer function called via dataset map\n",
    "def tokenize_function(examples:List[dict[str,str]], cfg:OmegaConf=cfg) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=cfg.context_length,\n",
    "        truncation=cfg.truncation,\n",
    "        return_length=cfg.return_length,\n",
    "        return_overflowing_tokens=cfg.return_overflowing_tokens\n",
    "        )\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], [318, 3958, 680, 1549, 11]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'length': [10, 5], 'overflow_to_sample_mapping': [0, 0], 'word_ids': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 11, 12, 13]]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(shake['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shake_toked = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1858, 338, 645, 517, 284, 307, 531, 11, 475, 339], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'length': 10, 'overflow_to_sample_mapping': 0, 'word_ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}\n",
      "['There', \"'s\", ' no', ' more', ' to', ' be', ' said', ',', ' but', ' he']\n",
      "There's no more to be said, but he\n"
     ]
    }
   ],
   "source": [
    "print(shake_toked['test'][0])\n",
    "print([tokenizer.decode(x) for x in shake_toked['test'][0]['input_ids']])\n",
    "print(tokenizer.decode(shake_toked['test'][0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 42263\n",
      "})\n",
      "test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 5276\n",
      "})\n",
      "valid Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping', 'word_ids'],\n",
      "    num_rows: 5295\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for split, dset in shake_toked.items():\n",
    "    print(split, dset)\n",
    "    arr_len = np.sum(dset['length'], dtype=np.uint64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token, tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769],\n",
      "        [   13,   314, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'labels': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769],\n",
      "        [   13,   314,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator([shake_toked[\"test\"]['input_ids'][i] for i in range(5)])\n",
    "# out = default_data_collator(shake_toked['test']['input_ids'][0])\n",
    "print(out)\n",
    "# for key in out:\n",
    "#     print(f\"{key} shape: {out[key].shape}\")\n",
    "# print('inputs: ', out['input_ids'])\n",
    "# print('labels: ', out['labels'])\n",
    "\n",
    "# data_collator = DefaultDataCollator(tokenizer)\n",
    "# out = data_collator([shake_toked[\"test\"][i] for i in range(5)])\n",
    "# print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(examples, block_size: int, **kwargs):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There's no more to be said, but he is banish'd,\"]\n"
     ]
    }
   ],
   "source": [
    "example = shake['test'][0]\n",
    "# concatenated_examples = {k: sum(example[k], []) for k in example.keys()}\n",
    "print([example[k] for k in example.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769]]), 'labels': tensor([[ 1858,   338,   645,   517,   284,   307,   531,    11,   475,   339],\n",
      "        [  318,  3958,   680,  1549,    11,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   40,  8406,   606,   326,   750,  1842,   511,  1499,   338,   922],\n",
      "        [   45, 12321,    11, 35695,   502,   284, 11906, 10846,   290, 37769]])}\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([shake_toked['test']['input_ids'][i] for i in range(4)])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', \"'s\", ' no', ' more', ' to', ' be', ' said', ',', ' but', ' he']\n",
      "[' is', ' ban', 'ish', \"'d\", ',', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "['I', ' bid', ' them', ' that', ' did', ' love', ' their', ' country', \"'s\", ' good']\n",
      "['N', 'urse', ',', ' commend', ' me', ' to', ' thy', ' lady', ' and', ' mistress']\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print([tokenizer.decode(x) for x in out['input_ids'][i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(\n",
    "    shake_toked['test']['input_ids'],\n",
    "    batch_size=128,\n",
    "    collate_fn=data_collator,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!head ../data/text/tiny_shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "tensor([  318,  3958,   680,  1549,    11, 50256, 50256, 50256, 50256, 50256])\n",
      "tensor([ 318, 3958,  680, 1549,   11, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(test_dl))\n",
    "print(b['input_ids'].shape)\n",
    "print(b['input_ids'][1])\n",
    "print(b['labels'][1])\n",
    "# for i in range(128):\n",
    "#     print([tokenizer.decode(x) for x in b['input_ids'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_target_': 'nimrod.text.datasets.CharDataModule', 'text_file': '../data/text/tiny_shakespeare.txt', 'train_val_test_split': [0.8, 0.1, 0.1], 'batch_size': 64, 'num_workers': 0, 'pin_memory': True, 'persistent_workers': False}\n",
      "size of data: 123933 size of splits:  [99146, 12393, 12394]\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "# # from config\n",
    "# conf = OmegaConf.load(\"../recipes/lm/config/train.yaml\")\n",
    "# pprint(conf.datamodule)\n",
    "# dm = instantiate(conf.datamodule)\n",
    "# dm.prepare_data()\n",
    "# dm.setup()\n",
    "# print('size of data:', len(dm.dataset), \"size of splits: \", [len(x) for x in [dm.data_train, dm.data_val, dm.data_test]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikitext-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4358 <class 'datasets.arrow_dataset.Dataset'> Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4358\n",
      "})\n",
      "{'text': ' Du Fu \\'s popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him . While there was never another Du Fu , individual poets followed in the traditions of specific aspects of his work : Bai Juyi \\'s concern for the poor , Lu You \\'s patriotism , and Mei Yaochen \\'s reflections on the quotidian are a few examples . More broadly , Du Fu \\'s work in transforming the lǜshi from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre . \\n'}\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset), type(dataset), dataset)\n",
    "print(dataset[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' Du Fu \\'s popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him . While there was never another Du Fu , individual poets followed in the traditions of specific aspects of his work : Bai Juyi \\'s concern for the poor , Lu You \\'s patriotism , and Mei Yaochen \\'s reflections on the quotidian are a few examples . More broadly , Du Fu \\'s work in transforming the lǜshi from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre . \\n'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source from torchtext\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = WikiText2(root='../data/text', split='test')\n",
    "tokenizer = get_tokenizer('basic_english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# vocab.set_default_index(vocab['<unk>'])\n",
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab(tokenizer('this is a test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate all sentences together\n",
    "def data_process(raw_text_iter) -> torch.Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(train_iter):\n",
    "#     print(idx, i)\n",
    "#     print(vocab(tokenizer(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, val_iter, test_iter = WikiText2()\n",
    "# train_data = data_process(train_iter)\n",
    "# val_data = data_process(val_iter)\n",
    "# test_data = data_process(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data: torch.Tensor, bsz: int) -> torch.Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = batchify(test_data, 10)\n",
    "# print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: torch.Tensor, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = get_batch(test_data, 0)\n",
    "# print(\"x: \", x[:2])\n",
    "# print(\"y: \", y[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-based tokenization "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "# # vocab.set_default_index(vocab['<unk>'])\n",
    "# tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n",
    "# fn_kwargs={'tokenizer': tokenizer})\n",
    "# print(tokenized_dataset['train'][88]['tokens'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Du', 'Fu', \"'\", 's', 'popularity', 'grew', 'to', 'such', 'an', 'extent', 'that', 'it', 'is', 'as', 'hard', 'to', 'measure', 'his', 'influence', 'as', 'that', 'of', 'Shakespeare', 'in', 'England', ':', 'it', 'was', 'hard', 'for', 'any', 'Chinese', 'poet', 'not', 'to', 'be', 'influenced', 'by', 'him', '.', 'While', 'there', 'was', 'never', 'another', 'Du', 'Fu', ',', 'individual', 'poets', 'followed', 'in', 'the', 'traditions', 'of', 'specific', 'aspects', 'of', 'his', 'work', ':', 'Bai', 'Ju', '##yi', \"'\", 's', 'concern', 'for', 'the', 'poor', ',', 'Lu', 'You', \"'\", 's', 'pat', '##riot', '##ism', ',', 'and', 'Mei', 'Yao', '##chen', \"'\", 's', 'reflections', 'on', 'the', 'q', '##uo', '##ti', '##dian', 'are', 'a', 'few', 'examples', '.', 'More', 'broadly', ',', 'Du', 'Fu', \"'\", 's', 'work', 'in', 'transforming', 'the', '[UNK]', 'from', 'mere', 'word', 'play', 'into', '\"', 'a', 'vehicle', 'for', 'serious', 'poetic', 'utter', '##ance', '\"', 'set', 'the', 'stage', 'for', 'every', 'subsequent', 'writer', 'in', 'the', 'genre', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokens = tokenizer.tokenize(dataset[100]['text'])\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n",
    "# min_freq=3) \n",
    "# vocab.insert_token('<unk>', 0)           \n",
    "# vocab.insert_token('<eos>', 1)            \n",
    "# vocab.set_default_index(vocab['<unk>'])   \n",
    "# print(len(vocab))                         \n",
    "# print(vocab.get_itos()[:10])  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12786, 14763, 112, 188, 5587, 2580, 1106, 1216, 1126, 6102, 1115, 1122, 1110, 1112, 1662, 1106, 4929, 1117, 2933, 1112, 1115, 1104, 7647, 1107, 1652, 131, 1122, 1108, 1662, 1111, 1251, 1922, 4225, 1136, 1106, 1129, 4401, 1118, 1140, 119, 1799, 1175, 1108, 1309, 1330, 12786, 14763, 117, 2510, 11587, 1723, 1107, 1103, 7181, 1104, 2747, 5402, 1104, 1117, 1250, 131, 27900, 23915, 10279, 112, 188, 4517, 1111, 1103, 2869, 117, 14557, 1192, 112, 188, 26227, 23326, 1863, 117, 1105, 24563, 27762, 10415, 112, 188, 26906, 1113, 1103, 186, 11848, 3121, 10359, 1132, 170, 1374, 5136, 119, 3046, 14548, 117, 12786, 14763, 112, 188, 1250, 1107, 20892, 1103, 100, 1121, 8574, 1937, 1505, 1154, 107, 170, 3686, 1111, 3021, 15751, 15462, 3923, 107, 1383, 1103, 2016, 1111, 1451, 4194, 2432, 1107, 1103, 6453, 119]\n",
      "Du Fu's popularity grew to such an extent that it is as hard to measure his influence as that of Shakespeare in England : it was hard for any Chinese poet not to be influenced by him. While there was never another Du Fu, individual poets followed in the traditions of specific aspects of his work : Bai Juyi's concern for the poor, Lu You's patriotism, and Mei Yaochen's reflections on the quotidian are a few examples. More broadly, Du Fu's work in transforming the [UNK] from mere word play into \" a vehicle for serious poetic utterance \" set the stage for every subsequent writer in the genre.\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face for LM without intermediary steps\n",
    "https://huggingface.co/course/chapter7/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1142, 1110, 102], [101, 170, 3087, 102], [101, 119, 102], [101, 1137, 1177, 102], [101, 1122, 3093, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], 'length': [4, 4, 3, 4, 4], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'length', 'overflow_to_sample_mapping'])\n",
      "['[CLS] this is [SEP]', '[CLS] a text [SEP]', '[CLS]. [SEP]', '[CLS] or so [SEP]', '[CLS] it seems [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# directly without intermediary steps\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "text = [\"this is a text.\", \"or so it seems\"]\n",
    "padded = tokenizer(text, max_length=4, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "\n",
    "print(padded)\n",
    "print(padded.keys())\n",
    "print([tokenizer.decode(x) for x in padded['input_ids']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 4])\n",
      "labels shape: torch.Size([5, 4])\n",
      "tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102,    0],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]]) tensor([[ 101, 1142, 1110,  102],\n",
      "        [ 101,  170, 3087,  102],\n",
      "        [ 101,  119,  102, -100],\n",
      "        [ 101, 1137, 1177,  102],\n",
      "        [ 101, 1122, 3093,  102]])\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "out = data_collator(padded['input_ids'])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")\n",
    "\n",
    "print(out['input_ids'], out['labels'])\n",
    "# Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "Concatenate all data into one large string of text and then chunk it into context length chunks\n",
    "- https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf\n",
    "- https://www.youtube.com/watch?v=ma1TrR7gE7I&t=273s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:                                      \n",
    "            tokens = example['tokens'].append('<eos>')             \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024\n",
    "# train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "# test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_dataset.shape)\n",
    "# print(tokenized_dataset['train']['tokens'][88])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modeling dataset\n",
    "\n",
    "Basically concatenate all data into one big array of ids and then create block_lengths inputs. shift for corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 32000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(shake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples:List[dict[str,str]]) -> dict[str, List[List[int]]]:\n",
    "    result = tokenizer(examples[\"text\"]) #, max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized = shake.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    "    num_proc = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 32000\n",
      "}) <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(tokenized['train'], type(tokenized['train']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 32000\n",
      "})\n",
      "238064\n",
      "test Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "267859\n",
      "valid Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'word_ids'],\n",
      "    num_rows: 4000\n",
      "})\n",
      "298027\n",
      "[8496, 674, 826, 46258, 2988, 318, 1716, 13, 1870, 373, 379, 938, 503, 12, 24903]\n"
     ]
    }
   ],
   "source": [
    "all = []\n",
    "for k,v in tokenized.items():\n",
    "    print(k, v)\n",
    "    for x in v['input_ids']:\n",
    "        all += x\n",
    "    print(len(all))\n",
    "print(all[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10]) torch.Size([16, 10])\n",
      "tensor([  790, 32460, 33769,   314,   787,   817,   272,   428,  4939,  3211]) tensor([32460, 33769,   314,   787,   817,   272,   428,  4939,  3211,    25])\n",
      " every tedious stride I makeThan this weak arm  tedious stride I makeThan this weak arm:\n"
     ]
    }
   ],
   "source": [
    "x, y = get_batch(np.array(all), 16, 10)\n",
    "print(x.shape, y.shape)\n",
    "print(x[0], y[0])\n",
    "print(tokenizer.decode(x[0]), tokenizer.decode(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
