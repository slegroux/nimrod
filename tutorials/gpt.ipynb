{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniGPT LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from nimrod.text.tokenizers import CharTokenizer\n",
    "from nimrod.text.datasets import SimpleCharDataset\n",
    "from nimrod.models.transformer import TransformerBlock\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we \n",
      "vocabulary size: 65\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n",
      "length of data str: 1115384\n"
     ]
    }
   ],
   "source": [
    "text = Path('../data/text/tiny_shakespeare.txt').read_text()\n",
    "print(text[:25])\n",
    "tok = CharTokenizer.from_text(text)\n",
    "print(f\"vocabulary size: {len(tok)}\")\n",
    "encoded = tok.encode(text)\n",
    "print(encoded[:10])\n",
    "ds = SimpleCharDataset(encoded, 10)\n",
    "print(f\"length of data str: {len(ds)}\")\n",
    "n_train = int(0.8 * len(ds))\n",
    "n_eval = int(0.1 * len(ds))\n",
    "n_test = len(ds) - n_train - n_eval\n",
    "train_ds, eval_ds, test_ds = random_split(ds, lengths=(n_train, n_eval, n_test))\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "eval_dl = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
    "test_dl = DataLoader(train_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size:int,\n",
    "        embed_dim:int,\n",
    "        n_head:int,\n",
    "        context_length:int,\n",
    "        dropout:float,\n",
    "        n_blocks:int\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(context_length, embed_dim)\n",
    "        block_params = [embed_dim, n_head, context_length, dropout]\n",
    "        self.blocks = [TransformerBlock(*block_params) for _ in range(n_blocks)]\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x:torch.LongTensor # (B,T) list of token IDS\n",
    "        ):\n",
    "        B,T = x.shape\n",
    "        tok_embed = self.tok_embed(x) # B, T, C\n",
    "        pos_embed = self.pos_embed(torch.arange(T, device=x.device)) # B, T, C\n",
    "        x = tok_embed + pos_embed\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        x:torch.LongTensor, # tok ids (B, T)\n",
    "        max_tokens: int # max size of sequence gen\n",
    "    ):  \n",
    "        self.eval()\n",
    "        for i in range(max_tokens):\n",
    "            if x.size(1) >= self.context_length:\n",
    "                break\n",
    "            with torch.no_grad():\n",
    "                logits = self.forward(x) # (B, T, n_classes)\n",
    "                # look at last time step only\n",
    "                logits = logits[:, -1, :]\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                pred = torch.multinomial(probs, num_samples=1)\n",
    "                x = torch.cat([x, pred], dim=1) #(T+1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size:int\n",
    "    embed_dim:int = 16\n",
    "    n_head:int = 2\n",
    "    context_length:int = 10\n",
    "    n_blocks:int = 2\n",
    "    dropout:float = 0.1\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.embed_dim % self.n_head == 0, \"embed_dim must be divisible by n_head\"\n",
    "        assert self.dropout >= 0 and self.dropout <= 1, \"dropout must be between 0 and 1\"\n",
    "\n",
    "cfg = GPTConfig(vocab_size=len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGPT(\n",
      "  (tok_embed): Embedding(65, 16)\n",
      "  (pos_embed): Embedding(10, 16)\n",
      "  (blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (ln1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x AttentionHead(\n",
      "            (key): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (ln1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      (sa): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-1): 2 x AttentionHead(\n",
      "            (key): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (query): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (value): Linear(in_features=16, out_features=8, bias=False)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=16, bias=True)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Linear(in_features=16, out_features=65, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = MiniGPT(**asdict(cfg))\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 65])\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "B, T, C = 5, cfg.context_length, cfg.embed_dim\n",
    "x = torch.randint(0, cfg.vocab_size, (B,T), dtype=torch.long).to(device)\n",
    "m = m.to(device)\n",
    "y = m(x) # B,T, n_classes\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46, 43, 50, 50, 53,  2]])\n",
      "tensor([[46, 43, 50, 50, 53,  2, 17, 41, 23, 48]])\n",
      "hello!EcKj\n"
     ]
    }
   ],
   "source": [
    "prompt = \"hello!\"\n",
    "encoded = tok.encode(prompt).unsqueeze(dim=0)\n",
    "print(encoded)\n",
    "res = m.generate(encoded, max_tokens=25)\n",
    "print(res)\n",
    "print(tok.decode(res.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'\n",
    "cfg = GPTConfig(vocab_size=len(tok))\n",
    "cfg.embed_dim:int = 16\n",
    "cfg.n_head:int = 1\n",
    "cfg.context_length:int = 10\n",
    "cfg.n_blocks:int = 1\n",
    "\n",
    "m = MiniGPT(**asdict(cfg)).to(device)\n",
    "\n",
    "n_epochs = 2\n",
    "lr = 3e-3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=lr)\n",
    "\n",
    "liveplot = PlotLosses()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    m.train()\n",
    "    train_running_loss = 0\n",
    "    val_running_loss = 0\n",
    "    train_loss_hist = []\n",
    "    eval_loss_hist = []\n",
    "\n",
    "    for step, batch in enumerate(train_dl):\n",
    "        optim.zero_grad()\n",
    "        x, y = batch\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        B, T = x.shape\n",
    "        logits = m(x) # B, T, V\n",
    "        B,T,V = logits.shape\n",
    "        logits = logits.view(B*T, V)\n",
    "        y = y.view(B*T)\n",
    "        loss = criterion(logits, y)\n",
    "        train_running_loss += loss.item()\n",
    "        # if not step%1000:\n",
    "        #     print(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    avg_train_loss = train_running_loss / len(train_dl)\n",
    "    # print(f\"epoch: {epoch} train loss: {avg_train_loss}\")\n",
    "\n",
    "    for step, batch in enumerate(eval_dl):\n",
    "        optim.zero_grad()\n",
    "        x, y = batch[0].to(device), batch[1].to(device)\n",
    "        B, T = x.shape\n",
    "        logits = m(x)\n",
    "        B, T, V = logits.shape\n",
    "        logits = logits.view(B*T, V)\n",
    "        y = y.view(B*T)\n",
    "        val_loss = criterion(logits, y)\n",
    "        val_running_loss += val_loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    avg_val_loss = val_running_loss / len(eval_dl)\n",
    "    print(f\"epoch: {epoch} train_loss: {avg_train_loss} val_loss {avg_val_loss}\")\n",
    "    liveplot.update({\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss\n",
    "    })\n",
    "    liveplot.send()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtok\u001b[49m\u001b[38;5;241m.\u001b[39mencode(prompt)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded)\n\u001b[1;32m      4\u001b[0m res \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mgenerate(encoded, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tok' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"hello!\"\n",
    "encoded = tok.encode(prompt).unsqueeze(dim=0).to(device)\n",
    "print(encoded)\n",
    "res = m.generate(encoded, max_tokens=4)\n",
    "print(res)\n",
    "print(tok.decode(res.squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
