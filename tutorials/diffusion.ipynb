{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from nimrod.models.diffusion import DiffusorX\n",
    "from nimrod.models.core import lr_finder, train_one_cycle\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from hydra.utils import instantiate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "[20:25:50] INFO - Init ImageDataModule for mnist\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transforms' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transforms'])`.\n",
      "[20:25:55] INFO - loading dataset mnist with args () from split train\n",
      "[20:25:55] INFO - loading dataset mnist from split train\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[20:25:58] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:25:58] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[20:25:58] INFO - Found cached dataset mnist (/Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:25:58] INFO - Loading Dataset info from /Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:26:03] INFO - loading dataset mnist with args () from split test\n",
      "[20:26:03] INFO - loading dataset mnist from split test\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "[20:26:05] INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:26:05] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "Found cached dataset mnist (/Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "[20:26:05] INFO - Found cached dataset mnist (/Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n",
      "Loading Dataset info from /Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:26:05] INFO - Loading Dataset info from /Users/slegroux/Projects/nimrod/tutorials/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n",
      "[20:26:06] INFO - split train into train/val [0.8, 0.2]\n",
      "[20:26:06] INFO - train: 48000 val: 12000, test: 10000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "cfg = OmegaConf.load('../config/data/image/mnist.yaml')\n",
    "tfs = transforms.Compose([transforms.ToTensor(), transforms.Resize(32), transforms.Normalize((0.5,), (0.5,))])  \n",
    "dm = instantiate(cfg, batch_size=BATCH_SIZE, transforms=tfs, num_workers=0)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dm.dim\n",
    "dm.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = partial(torch.optim.AdamW, lr=3e-4)\n",
    "\n",
    "nnet = UNet2DModel(\n",
    "    sample_size=H,\n",
    "    in_channels=C,\n",
    "    out_channels=C,\n",
    "    block_out_channels=(32, 64, 128, 256)\n",
    "    )\n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"steps: {noise_scheduler.config.num_train_timesteps}\")\n",
    "model = DiffusorX(nnet, noise_scheduler, optimizer)\n",
    "x = torch.randn((B, C, H, W))\n",
    "\n",
    "t = torch.randint(0, noise_scheduler.config.num_train_timesteps, (B,))\n",
    "print(f\"x:{x.shape}, t: {t.shape}, {type(t)}\")\n",
    "noise = nnet(x, t).sample\n",
    "print(noise.shape)\n",
    "print(model(x, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[20:26:59] INFO - Regressor: init\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n",
      "[20:26:59] INFO - DiffusionX: init\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslegroux\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/wandb/run-20250213_202700-krprb4eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/slegroux/MNIST-Diffusion/runs/krprb4eq' target=\"_blank\">DiffusorX-bs:512-epochs:5</a></strong> to <a href='https://wandb.ai/slegroux/MNIST-Diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/slegroux/MNIST-Diffusion' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/slegroux/MNIST-Diffusion/runs/krprb4eq' target=\"_blank\">https://wandb.ai/slegroux/MNIST-Diffusion/runs/krprb4eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[20:27:03] INFO - Regressor: configure_optimizers\n",
      "[20:27:03] INFO - Optimizer: <class 'torch.optim.adamw.AdamW'>\n",
      "[20:27:03] INFO - Scheduler: <class 'torch.optim.lr_scheduler.OneCycleLR'>\n",
      "\n",
      "  | Name         | Type             | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | nnet         | UNet2DModel      | 15.9 M | train\n",
      "1 | criterion    | MSELoss          | 0      | train\n",
      "2 | train_mse    | MeanSquaredError | 0      | train\n",
      "3 | val_mse      | MeanSquaredError | 0      | train\n",
      "4 | test_mse     | MeanSquaredError | 0      | train\n",
      "5 | val_mse_best | MinMetric        | 0      | train\n",
      "6 | train_loss   | MeanMetric       | 0      | train\n",
      "7 | val_loss     | MeanMetric       | 0      | train\n",
      "8 | test_loss    | MeanMetric       | 0      | train\n",
      "----------------------------------------------------------\n",
      "15.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.9 M    Total params\n",
      "63.563    Total estimated model params size (MB)\n",
      "360       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e7599b01b64b0899b197f1bb870301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01d36e7438945eaad418839827432ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/amp.py:79\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:241\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[0;34m(loss)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:213\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[0;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:73\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[0;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1097\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[0;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     suggested_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-4\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ONE-CYCLE TRAINING\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m trained_model, best_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_cycle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuggested_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger_cb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_logger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/nimrod/nimrod/models/core.py:480\u001b[0m, in \u001b[0;36mtrain_one_cycle\u001b[0;34m(model, datamodule, max_lr, weight_decay, n_epochs, project_name, tags, test, run_name, model_summary, logger_cb, precision)\u001b[0m\n\u001b[1;32m    477\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(datamodule\u001b[38;5;241m.\u001b[39mtrain_dataloader()))\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(summary(model\u001b[38;5;241m.\u001b[39mnnet, input_size\u001b[38;5;241m=\u001b[39mxb\u001b[38;5;241m.\u001b[39mshape, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 480\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test:\n\u001b[1;32m    482\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtest(model, datamodule\u001b[38;5;241m.\u001b[39mtest_dataloader())\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "project_name = \"MNIST-Diffusion\"\n",
    "\n",
    "N_EPOCHS = 5\n",
    "do_lr_finder = False\n",
    "exp_logger = 'wandb'\n",
    "precision = \"16-mixed\" # 16-mixed, 32-true\n",
    "\n",
    "block_out_channels = [32, 64, 128, 256]  # channel/feature expansion\n",
    "\n",
    "\n",
    "cfg = OmegaConf.load('../config/model/image/diffusorx.yaml')\n",
    "cfg.nnet.block_out_channels = block_out_channels  # channel/feature expansion\n",
    "model = instantiate(cfg) #partial\n",
    "\n",
    "tags = [\n",
    "    f\"precision:{precision}\",\n",
    "    f\"block_chan:{block_out_channels}\",\n",
    "    f\"bs:{dm.batch_size}\",\n",
    "    f\"epochs:{N_EPOCHS}\"\n",
    "    ]\n",
    "\n",
    "# LR Finder\n",
    "if do_lr_finder:\n",
    "    suggested_lr = lr_finder(model, dm, plot=True)\n",
    "    print(f\"Suggested learning rate: {suggested_lr}\")\n",
    "else:\n",
    "    suggested_lr = 3e-4\n",
    "\n",
    "# ONE-CYCLE TRAINING\n",
    "trained_model, best_ckpt_path = train_one_cycle(\n",
    "    model,\n",
    "    dm,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    max_lr=suggested_lr,\n",
    "    project_name=project_name,\n",
    "    tags=tags,\n",
    "    model_summary=False,\n",
    "    logger_cb=exp_logger,\n",
    "    precision=precision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
