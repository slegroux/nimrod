{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: allows to leverage preliminary data prep from lhotse recipes\n",
    "output-file: data.utils.lhotse.html\n",
    "title: Lhotse support for datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS Lhotse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/modules.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Decoder\n",
       "\n",
       ">      Decoder ()\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/modules.py#L20){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Decoder\n",
       "\n",
       ">      Decoder ()\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/modules.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Encoder\n",
       "\n",
       ">      Encoder ()\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/modules.py#L11){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Encoder\n",
       "\n",
       ">      Encoder ()\n",
       "\n",
       "Base class for all neural network modules.\n",
       "\n",
       "Your models should also subclass this class.\n",
       "\n",
       "Modules can also contain other Modules, allowing to nest them in\n",
       "a tree structure. You can assign the submodules as regular attributes::\n",
       "\n",
       "    import torch.nn as nn\n",
       "    import torch.nn.functional as F\n",
       "\n",
       "    class Model(nn.Module):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
       "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
       "\n",
       "        def forward(self, x):\n",
       "            x = F.relu(self.conv1(x))\n",
       "            return F.relu(self.conv2(x))\n",
       "\n",
       "Submodules assigned in this way will be registered, and will have their\n",
       "parameters converted too when you call :meth:`to`, etc.\n",
       "\n",
       ".. note::\n",
       "    As per the example above, an ``__init__()`` call to the parent class\n",
       "    must be made before assignment on the child.\n",
       "\n",
       ":ivar training: Boolean represents whether this module is in training or\n",
       "                evaluation mode.\n",
       ":vartype training: bool"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Encoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download data and load into Lhotse cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "download_ljspeech('/data/en/LJSpeech')\n",
    "# skip this step already done\n",
    "ljspeech = prepare_ljspeech('/data/en/LJSpeech/LJSpeech-1.1', '../recipes/tts/ljspeech/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "cut_set = CutSet.from_manifests(**ljspeech)\n",
    "subset = cut_set.subset(first=3)\n",
    "subset.to_file('../data/en/LJSpeech-1.1/first_3.jsonl.gz')\n",
    "reload_subset = CutSet.from_file('../data/en/LJSpeech-1.1/first_3.jsonl.gz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encodec feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "encodec_extractor = EncoDecExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# torch.set_num_threads(1)\n",
    "# torch.set_num_interop_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfe18679547494b8f0665a54c80b035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting and storing features:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: fix bug for n_jobs >1\n",
    "cuts = subset.compute_and_store_features(\n",
    "    extractor=encodec_extractor,\n",
    "    storage_path=\"../data/en/LJSpeech-1.1/encodec/encodec\",\n",
    "    num_jobs=1,\n",
    "    # storage_type=NumpyHdf5Writer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n"
     ]
    }
   ],
   "source": [
    "print(cuts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuts.to_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\n",
    "cuts[0]\n",
    "reload_cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\n",
    "reload_cuts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input File     : '/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav'\n",
      "Channels       : 1\n",
      "Sample Rate    : 22050\n",
      "Precision      : 16-bit\n",
      "Duration       : 00:00:09.66 = 212893 samples ~ 724.126 CDDA sectors\n",
      "File Size      : 426k\n",
      "Bit Rate       : 353k\n",
      "Sample Encoding: 16-bit Signed Integer PCM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cuts[0].recording\n",
    "!soxi '/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([725, 8]), torch.Size([725, 8]), torch.Size([725, 8])]\n",
      "[724, 142, 725]\n",
      "torch.Size([3, 725, 8]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "strategy = PrecomputedFeatures()\n",
    "feats, feats_len = strategy(cuts)\n",
    "\n",
    "# print([(f\"feat: {feat.shape}\", f\"len: {feat_len}\") for feat in feats for feat_len in feats_len])\n",
    "print([feat.shape for feat in feats])\n",
    "print([int(feat_len) for feat_len in feats_len])\n",
    "print(feats.shape, feats_len.shape)\n",
    "# TODO: debug OnTheFlyFeature case\n",
    "# strategy = OnTheFlyFeatures(extractor=encodec_extractor)\n",
    "# feats, feats_len = strategy(cuts)\n",
    "# print(feats, feats_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text normalization, tokenization and numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "cleaner = TTSTextNormalizer()\n",
    "tokenizer = Phonemizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\n",
      "printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən\n",
      "in being comparatively modern.\n",
      "in being comparatively modern.\n",
      "ɪn biːɪŋ kəmpæɹətɪvli mɑːdɚn.\n",
      "For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process\n",
      "for although the chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the netherlands, by a similar process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fɔːɹ ɔːlðoʊ ðə tʃaɪniːz tʊk ɪmpɹɛʃənz fɹʌm wʊd blɑːks ɛŋɡɹeɪvd ɪn ɹᵻliːf fɔːɹ sɛntʃɚɹiz bᵻfoːɹ ðə wʊdkʌɾɚz ʌvðə nɛðɜːləndz, baɪ ɐ sɪmɪlɚ pɹɑːsɛs\n"
     ]
    }
   ],
   "source": [
    "n_jobs = 1\n",
    "unique_phonemes = set()\n",
    "with CutSet.open_writer('../data/en/LJSpeech-1.1/first_3.final.jsonl.gz', overwrite=True) as writer:\n",
    "    for cut in cuts:\n",
    "        text = cut.supervisions[0].text\n",
    "        print(text)\n",
    "        normalized = cleaner(text)\n",
    "        print(normalized)\n",
    "        phonemes = tokenizer(text)\n",
    "        print(phonemes)\n",
    "        cut.custom = {'normalized': normalized, 'phonemes': phonemes}\n",
    "        writer.write(cut, flush=True)\n",
    "        unique_phonemes.update(list(phonemes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export phoneme lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'normalized': 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition', 'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})\n",
      "{0: ' ', 1: ',', 2: '.', 3: 'a', 4: 'b', 5: 'd', 6: 'e', 7: 'f', 8: 'i', 9: 'k', 10: 'l', 11: 'm', 12: 'n', 13: 'o', 14: 'p', 15: 's', 16: 't', 17: 'v', 18: 'w', 19: 'z', 20: 'æ', 21: 'ð', 22: 'ŋ', 23: 'ɐ', 24: 'ɑ', 25: 'ɔ', 26: 'ə', 27: 'ɚ', 28: 'ɛ', 29: 'ɜ', 30: 'ɡ', 31: 'ɪ', 32: 'ɹ', 33: 'ɾ', 34: 'ʃ', 35: 'ʊ', 36: 'ʌ', 37: 'ː', 38: 'ᵻ', 39: '<eps>'} 40\n"
     ]
    }
   ],
   "source": [
    "cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.final.jsonl.gz\")\n",
    "print(cuts[0])\n",
    "map = {}\n",
    "unique_syms = set()\n",
    "for cut in cuts:\n",
    "    unique_syms.update(list(cut.custom['phonemes']))\n",
    "for (i, v) in enumerate(sorted(list(unique_syms))):\n",
    "    map[i] = v\n",
    "map[len(map)] = \"<eps>\"\n",
    "print(map, len(map))\n",
    "\n",
    "json_map = json.dumps(map)\n",
    "with open(\"../data/en/LJSpeech-1.1/lexicon/map.json\",\"w\") as f:\n",
    "    f.write(json_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': ' ', '1': ',', '2': '.', '3': 'a', '4': 'b', '5': 'd', '6': 'e', '7': 'f', '8': 'i', '9': 'k', '10': 'l', '11': 'm', '12': 'n', '13': 'o', '14': 'p', '15': 's', '16': 't', '17': 'v', '18': 'w', '19': 'z', '20': 'æ', '21': 'ð', '22': 'ŋ', '23': 'ɐ', '24': 'ɑ', '25': 'ɔ', '26': 'ə', '27': 'ɚ', '28': 'ɛ', '29': 'ɜ', '30': 'ɡ', '31': 'ɪ', '32': 'ɹ', '33': 'ɾ', '34': 'ʃ', '35': 'ʊ', '36': 'ʌ', '37': 'ː', '38': 'ᵻ', '39': '<eps>'}\n"
     ]
    }
   ],
   "source": [
    "with open('../data/en/LJSpeech-1.1/lexicon/map.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/utils/lhotse.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PhonemeCollater\n",
       "\n",
       ">      PhonemeCollater (cuts:lhotse.cut.set.CutSet, add_eos:bool=True,\n",
       ">                       add_bos:bool=True, pad_symbol:str='<pad>',\n",
       ">                       bos_symbol:str='<bos>', eos_symbol:str='<eos>',\n",
       ">                       unk_symbol:str='<unk>')\n",
       "\n",
       "Collate list of tokens\n",
       "\n",
       "Map sentences to integers. Sentences are padded to equal length.\n",
       "Beginning and end-of-sequence symbols can be added.\n",
       "Call .inverse(tokens_batch, tokens_lens) to reconstruct batch as string sentences.\n",
       "\n",
       "Example:\n",
       "    >>> token_collater = TokenCollater(cuts)\n",
       "    >>> tokens_batch, tokens_lens = token_collater(cuts.subset(first=32))\n",
       "    >>> original_sentences = token_collater.inverse(tokens_batch, tokens_lens)\n",
       "\n",
       "Returns:\n",
       "    tokens_batch: IntTensor of shape (B, L)\n",
       "        B: batch dimension, number of input sentences\n",
       "        L: length of the longest sentence\n",
       "    tokens_lens: IntTensor of shape (B,)\n",
       "        Length of each sentence after adding <eos> and <bos>\n",
       "        but before padding."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/utils/lhotse.py#L46){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PhonemeCollater\n",
       "\n",
       ">      PhonemeCollater (cuts:lhotse.cut.set.CutSet, add_eos:bool=True,\n",
       ">                       add_bos:bool=True, pad_symbol:str='<pad>',\n",
       ">                       bos_symbol:str='<bos>', eos_symbol:str='<eos>',\n",
       ">                       unk_symbol:str='<unk>')\n",
       "\n",
       "Collate list of tokens\n",
       "\n",
       "Map sentences to integers. Sentences are padded to equal length.\n",
       "Beginning and end-of-sequence symbols can be added.\n",
       "Call .inverse(tokens_batch, tokens_lens) to reconstruct batch as string sentences.\n",
       "\n",
       "Example:\n",
       "    >>> token_collater = TokenCollater(cuts)\n",
       "    >>> tokens_batch, tokens_lens = token_collater(cuts.subset(first=32))\n",
       "    >>> original_sentences = token_collater.inverse(tokens_batch, tokens_lens)\n",
       "\n",
       "Returns:\n",
       "    tokens_batch: IntTensor of shape (B, L)\n",
       "        B: batch dimension, number of input sentences\n",
       "        L: length of the longest sentence\n",
       "    tokens_lens: IntTensor of shape (B,)\n",
       "        Length of each sentence after adding <eos> and <bos>\n",
       "        but before padding."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(PhonemeCollater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'normalized': 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition', 'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 18,  4, 36,  4, 35,  4, 16,  4, 20,  4, 35,  4, 26,  4,  5,  4,  4,\n",
      "          4, 35,  4, 16,  4, 25,  4, 35,  4,  4,  4, 17,  4, 39,  4, 16,  4, 14,\n",
      "          4, 12,  4,  4,  4, 19,  4, 32,  4, 16,  4, 19,  4,  4,  4, 22,  4, 35,\n",
      "          4, 25,  4,  4,  4, 22,  4, 35,  4, 20,  4, 38,  4,  4,  4, 22,  4, 12,\n",
      "          4, 41,  4,  4,  4, 28,  4, 41,  4, 36,  4,  4,  4, 24,  4, 20,  4,  4,\n",
      "          4, 18,  4, 36,  4, 32,  4, 23,  4, 30,  4, 16,  4, 20,  4,  4,  4, 13,\n",
      "          4, 30,  4, 16,  4, 19,  4, 33,  4, 41,  4, 16,  4,  9,  4,  5,  4,  4,\n",
      "          4,  9,  4, 35,  4, 11,  4, 31,  4, 23,  4,  4,  4, 11,  4, 36,  4, 40,\n",
      "          4, 15,  4,  4,  4, 15,  4, 17,  4, 39,  4, 19,  4, 20,  4,  4,  4, 35,\n",
      "          4, 11,  4,  4,  4, 16,  4, 28,  4, 41,  4, 20,  4,  4,  4, 11,  4, 36,\n",
      "          4, 40,  4, 15,  4,  4,  4, 29,  4, 41,  4, 14,  4,  4,  4, 25,  4, 35,\n",
      "          4,  4,  4, 28,  4, 41,  4, 36,  4, 20,  4, 19,  4,  4,  4, 24,  4, 16,\n",
      "          4,  9,  4,  4,  4, 13,  4, 36,  4, 24,  4, 11,  4, 20,  4, 19,  4,  4,\n",
      "          4, 36,  4, 32,  4, 18,  4, 36,  4, 42,  4, 23,  4, 32,  4, 16,  4, 20,\n",
      "          4, 42,  4,  9,  4,  4,  4, 35,  4, 16,  4, 25,  4, 35,  4,  4,  4, 32,\n",
      "          4, 13,  4, 19,  4, 35,  4,  8,  4, 35,  4, 38,  4, 30,  4, 16,  3,  0,\n",
      "          0],\n",
      "        [ 2, 35,  4, 16,  4,  4,  4,  8,  4, 12,  4, 41,  4, 35,  4, 26,  4,  4,\n",
      "          4, 13,  4, 30,  4, 15,  4, 18,  4, 24,  4, 36,  4, 30,  4, 20,  4, 35,\n",
      "          4, 21,  4, 14,  4, 12,  4,  4,  4, 15,  4, 28,  4, 41,  4,  9,  4, 31,\n",
      "          4, 16,  4,  6,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [ 2, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 29,  4, 41,  4, 14,  4, 25,\n",
      "          4, 17,  4, 39,  4,  4,  4, 25,  4, 30,  4,  4,  4, 20,  4, 38,  4,  7,\n",
      "          4, 35,  4, 16,  4, 12,  4, 41,  4, 23,  4,  4,  4, 20,  4, 39,  4, 13,\n",
      "          4,  4,  4, 35,  4, 15,  4, 18,  4, 36,  4, 32,  4, 38,  4, 30,  4, 16,\n",
      "          4, 23,  4,  4,  4, 11,  4, 36,  4, 40,  4, 15,  4,  4,  4, 22,  4, 39,\n",
      "          4,  9,  4,  4,  4,  8,  4, 14,  4, 28,  4, 41,  4, 13,  4, 19,  4,  4,\n",
      "          4, 32,  4, 26,  4, 34,  4, 36,  4, 10,  4, 35,  4, 21,  4,  9,  4,  4,\n",
      "          4, 35,  4, 16,  4,  4,  4, 36,  4, 42,  4, 14,  4, 12,  4, 41,  4, 11,\n",
      "          4,  4,  4, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 19,  4, 32,  4, 16,\n",
      "          4, 20,  4, 38,  4, 31,  4, 36,  4, 12,  4, 23,  4,  4,  4,  8,  4, 42,\n",
      "          4, 11,  4, 17,  4, 41,  4, 36,  4,  4,  4, 25,  4, 30,  4,  4,  4, 22,\n",
      "          4, 39,  4,  9,  4, 13,  4, 40,  4, 37,  4, 31,  4, 23,  4,  4,  4, 40,\n",
      "          4, 21,  4, 25,  4, 30,  4,  4,  4, 16,  4, 32,  4, 25,  4, 33,  4, 41,\n",
      "          4, 14,  4, 30,  4, 16,  4,  9,  4, 23,  4,  5,  4,  4,  4,  8,  4,  7,\n",
      "          4, 35,  4,  4,  4, 27,  4,  4,  4, 19,  4, 35,  4, 15,  4, 35,  4, 14,\n",
      "          4, 31,  4,  4,  4, 18,  4, 36,  4, 28,  4, 41,  4, 19,  4, 32,  4, 19,\n",
      "          3]]) tensor([287,  59, 289], dtype=torch.int32)\n",
      "['p ɹ ɪ n t ɪ ŋ ,   ɪ n ð ɪ   o ʊ n l i   s ɛ n s   w ɪ ð   w ɪ t ʃ   w i ː   ɑ ː ɹ   æ t   p ɹ ɛ z ə n t   k ə n s ɜ ː n d ,   d ɪ f ɚ z   f ɹ ʌ m   m o ʊ s t   ɪ f   n ɑ ː t   f ɹ ʌ m   ɔ ː l   ð ɪ   ɑ ː ɹ t s   æ n d   k ɹ æ f t s   ɹ ɛ p ɹ ᵻ z ɛ n t ᵻ d   ɪ n ð ɪ   ɛ k s ɪ b ɪ ʃ ə n', 'ɪ n   b i ː ɪ ŋ   k ə m p æ ɹ ə t ɪ v l i   m ɑ ː d ɚ n .', 'f ɔ ː ɹ   ɔ ː l ð o ʊ   ð ə   t ʃ a ɪ n i ː z   t ʊ k   ɪ m p ɹ ɛ ʃ ə n z   f ɹ ʌ m   w ʊ d   b l ɑ ː k s   ɛ ŋ ɡ ɹ e ɪ v d   ɪ n   ɹ ᵻ l i ː f   f ɔ ː ɹ   s ɛ n t ʃ ɚ ɹ i z   b ᵻ f o ː ɹ   ð ə   w ʊ d k ʌ ɾ ɚ z   ʌ v ð ə   n ɛ ð ɜ ː l ə n d z ,   b a ɪ   ɐ   s ɪ m ɪ l ɚ   p ɹ ɑ ː s ɛ s']\n"
     ]
    }
   ],
   "source": [
    "pc = PhonemeCollater(cuts)\n",
    "tokens, tokens_len = pc(cuts)\n",
    "print(tokens, tokens_len)\n",
    "print(pc.inverse(tokens, tokens_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class ValleDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cuts:CutSet,\n",
    "            strategy:BatchIO=PrecomputedFeatures()\n",
    "        ):\n",
    "        self.extractor = strategy\n",
    "        self.tokenizer = PhonemeCollater(cuts)\n",
    "\n",
    "    def __getitem__(self, cuts: CutSet) -> Dict[str, torch.Tensor]:\n",
    "        cuts = cuts.sort_by_duration()\n",
    "        feats, feat_lens = self.extractor(cuts)\n",
    "        tokens, token_lens = self.tokenizer(cuts)\n",
    "        return {\"feats_pad\": feats, \"feats_lens\": feat_lens, \"tokens_pad\": tokens, \"tokens_lens\": token_lens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feats_pad': tensor([[[ 160.0000,  909.0000,  956.0117,  ...,  594.9853,  432.9870,\n",
      "           962.9949],\n",
      "         [ 438.0000,  876.0039,  486.0096,  ...,  602.0046,  997.9940,\n",
      "           262.0071],\n",
      "         [ 935.0078,  927.9921,  956.0148,  ...,  371.9996,  338.9874,\n",
      "           228.0006],\n",
      "         ...,\n",
      "         [ 475.0099,  856.9933,  653.0055,  ...,   95.9989,  853.0098,\n",
      "           467.0154],\n",
      "         [ 105.9963,  544.0138,  785.9864,  ...,  938.9966,  627.9919,\n",
      "           899.0155],\n",
      "         [ 474.9892,  913.0139,  981.9944,  ...,   40.9858,  771.9880,\n",
      "          1012.0151]]]), 'feats_lens': tensor([725], dtype=torch.int32), 'tokens_pad': tensor([[ 2, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 29,  4, 41,  4, 14,  4, 25,\n",
      "          4, 17,  4, 39,  4,  4,  4, 25,  4, 30,  4,  4,  4, 20,  4, 38,  4,  7,\n",
      "          4, 35,  4, 16,  4, 12,  4, 41,  4, 23,  4,  4,  4, 20,  4, 39,  4, 13,\n",
      "          4,  4,  4, 35,  4, 15,  4, 18,  4, 36,  4, 32,  4, 38,  4, 30,  4, 16,\n",
      "          4, 23,  4,  4,  4, 11,  4, 36,  4, 40,  4, 15,  4,  4,  4, 22,  4, 39,\n",
      "          4,  9,  4,  4,  4,  8,  4, 14,  4, 28,  4, 41,  4, 13,  4, 19,  4,  4,\n",
      "          4, 32,  4, 26,  4, 34,  4, 36,  4, 10,  4, 35,  4, 21,  4,  9,  4,  4,\n",
      "          4, 35,  4, 16,  4,  4,  4, 36,  4, 42,  4, 14,  4, 12,  4, 41,  4, 11,\n",
      "          4,  4,  4, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 19,  4, 32,  4, 16,\n",
      "          4, 20,  4, 38,  4, 31,  4, 36,  4, 12,  4, 23,  4,  4,  4,  8,  4, 42,\n",
      "          4, 11,  4, 17,  4, 41,  4, 36,  4,  4,  4, 25,  4, 30,  4,  4,  4, 22,\n",
      "          4, 39,  4,  9,  4, 13,  4, 40,  4, 37,  4, 31,  4, 23,  4,  4,  4, 40,\n",
      "          4, 21,  4, 25,  4, 30,  4,  4,  4, 16,  4, 32,  4, 25,  4, 33,  4, 41,\n",
      "          4, 14,  4, 30,  4, 16,  4,  9,  4, 23,  4,  5,  4,  4,  4,  8,  4,  7,\n",
      "          4, 35,  4,  4,  4, 27,  4,  4,  4, 19,  4, 35,  4, 15,  4, 35,  4, 14,\n",
      "          4, 31,  4,  4,  4, 18,  4, 36,  4, 28,  4, 41,  4, 19,  4, 32,  4, 19,\n",
      "          3]]), 'tokens_lens': tensor([289], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "ds = ValleDataset(cuts)\n",
    "# train_sampler = BucketingSampler(cuts, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\n",
    "train_sampler = DynamicBucketingSampler(cuts, max_duration=300, shuffle=True, num_buckets=2)\n",
    "dl = DataLoader(ds, sampler=train_sampler, batch_size=None, num_workers=1)\n",
    "print(next(iter(dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
