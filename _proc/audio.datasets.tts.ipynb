{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: TTS datasets\n",
    "output-file: audio.datasets.tts.html\n",
    "title: Audio TTS Datasets\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-To-Speech"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/datasets.py#L251){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TTSDataset\n",
       "\n",
       ">      TTSDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n",
       ">                  num_mel_bins:int=80)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | TokenCollater |  | text tokenizer |\n",
       "| num_mel_bins | int | 80 | number of mel spectrogram bins |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/datasets.py#L251){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TTSDataset\n",
       "\n",
       ">      TTSDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n",
       ">                  num_mel_bins:int=80)\n",
       "\n",
       "An abstract class representing a :class:`Dataset`.\n",
       "\n",
       "All datasets that represent a map from keys to data samples should subclass\n",
       "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
       "data sample for a given key. Subclasses could also optionally overwrite\n",
       ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
       ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
       "of :class:`~torch.utils.data.DataLoader`.\n",
       "\n",
       ".. note::\n",
       "  :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
       "  sampler that yields integral indices.  To make it work with a map-style\n",
       "  dataset with non-integral indices/keys, a custom sampler must be provided.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | TokenCollater |  | text tokenizer |\n",
       "| num_mel_bins | int | 80 | number of mel spectrogram bins |"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TTSDataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LJSpeech DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# #| export\n",
    "# class LJSpeechDataModule(LightningDataModule):\n",
    "#     def __init__(self,\n",
    "#         target_dir=\"/data/en\", # where data will be saved / retrieved\n",
    "#         dataset_parts=\"mini_librispeech\", # either full librispeech or mini subset\n",
    "#         output_dir=\"../recipes/tts/ljspeech/data\" # where to save manifest\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters(logger=False)\n",
    "\n",
    "#     def prepare_data(self,) -> None:\n",
    "#         download_librispeech(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)\n",
    "\n",
    "#     def setup(self, stage = None):\n",
    "#         libri = prepare_librispeech(corpus_dir=Path(self.hparams.target_dir) / \"LibriSpeech\", output_dir=self.hparams.output_dir)\n",
    "#         self.cuts_train = CutSet.from_manifests(**libri[\"train-clean-5\"])\n",
    "#         self.cuts_test = CutSet.from_manifests(**libri[\"dev-clean-2\"])\n",
    "#         self.tokenizer = TokenCollater(self.cuts_train)\n",
    "#         self.tokenizer(self.cuts_test.subset(first=2))\n",
    "#         self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\n",
    "#         return DataLoader(STTDataset(self.tokenizer), sampler=train_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False, bucket_method=\"equal_duration\")\n",
    "#         return DataLoader(STTDataset(self.tokenizer), sampler=test_sampler, batch_size=None, num_workers=2)\n",
    "\n",
    "#     @property\n",
    "#     def model_kwargs(self):\n",
    "#         return {\n",
    "#             \"odim\": len(self.tokenizer.idx2token),\n",
    "#         }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LibriTTS DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/datasets.py#L305){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LibriTTSDataModule\n",
       "\n",
       ">      LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n",
       ">                          clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n",
       ">                          mrod/recipes/libritts/data', num_jobs=1)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
       "advantage is consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| target_dir | str | /data/en/libriTTS | where data will be saved / retrieved |\n",
       "| dataset_parts | list | ['dev-clean', 'test-clean'] | either full libritts or subset |\n",
       "| output_dir | str | /home/syl20/slg/nimrod/recipes/libritts/data | where to save manifest |\n",
       "| num_jobs | int | 1 | num_jobs depending on number of cpus available |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/data/datasets.py#L305){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### LibriTTSDataModule\n",
       "\n",
       ">      LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n",
       ">                          clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n",
       ">                          mrod/recipes/libritts/data', num_jobs=1)\n",
       "\n",
       "A DataModule standardizes the training, val, test splits, data preparation and transforms. The main\n",
       "advantage is consistent data splits, data preparation and transforms across models.\n",
       "\n",
       "Example::\n",
       "\n",
       "    class MyDataModule(LightningDataModule):\n",
       "        def __init__(self):\n",
       "            super().__init__()\n",
       "        def prepare_data(self):\n",
       "            # download, split, etc...\n",
       "            # only called on 1 GPU/TPU in distributed\n",
       "        def setup(self, stage):\n",
       "            # make assignments here (val/train/test split)\n",
       "            # called on every process in DDP\n",
       "        def train_dataloader(self):\n",
       "            train_split = Dataset(...)\n",
       "            return DataLoader(train_split)\n",
       "        def val_dataloader(self):\n",
       "            val_split = Dataset(...)\n",
       "            return DataLoader(val_split)\n",
       "        def test_dataloader(self):\n",
       "            test_split = Dataset(...)\n",
       "            return DataLoader(test_split)\n",
       "        def teardown(self):\n",
       "            # clean up after fit or test\n",
       "            # called on every process in DDP\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| target_dir | str | /data/en/libriTTS | where data will be saved / retrieved |\n",
       "| dataset_parts | list | ['dev-clean', 'test-clean'] | either full libritts or subset |\n",
       "| output_dir | str | /home/syl20/slg/nimrod/recipes/libritts/data | where to save manifest |\n",
       "| num_jobs | int | 1 | num_jobs depending on number of cpus available |"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(LibriTTSDataModule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "dm = LibriTTSDataModule(\n",
    "    target_dir=\"../data/en\", \n",
    "    dataset_parts=\"test-clean\",\n",
    "    output_dir=\"../data/en/LibriTTS/test-clean\",\n",
    "    num_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# skip download and use local data folder\n",
    "# dm.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 95it [00:00, 4875.90it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]00:00<00:00, 48.20it/s]\n",
      "Scanning audio files (*.wav): 0it [00:00, ?it/s]\n",
      "Preparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 50.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dm.setup(stage='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
