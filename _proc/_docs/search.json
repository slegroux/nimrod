[
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "nimrod",
    "section": "Description",
    "text": "Description\nThis is a repo with minimal tooling, modules, models and recipes to get easily get started with deep learning training and experimentation"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nimrod",
    "section": "Install",
    "text": "Install\npip install nimrod"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "nimrod",
    "section": "Usage",
    "text": "Usage\nCheck recipes in recipes/ folder. For instance:\ncd recipes/autoencoder/\npython train.py"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "nimrod",
    "section": "Authors",
    "text": "Authors\n2023 Sylvain Le Groux slegroux@ccrma.stanford.edu"
  },
  {
    "objectID": "text.normalizers.html",
    "href": "text.normalizers.html",
    "title": "Text Normalizers",
    "section": "",
    "text": "source\n\n\n\n TTSTextNormalizer (language='en')\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncleaner = TTSTextNormalizer()\nprint(cleaner.en_normalize_numbers(\"$350\"))\nprint(cleaner.expand_time_english(\"12:05pm\"))\nprint(cleaner(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))\nprint(cleaner([\"Oh my dear! this is $5 too soon...\", \"It's 1:04 am!\"]))\n\nthree hundred fifty dollars\ntwelve oh five p m\noh my dear! this is five dollars too soon... it's one oh four a m!\n['oh my dear! this is five dollars too soon...', \"it's one oh four a m!\"]\n\n\n\nsource\n\n\n\n\n Punctuation (puncs:str=';:,.!?¡¿—…\"«»“”')\n\nHandle punctuations in text.\nJust strip punctuations from text or strip and restore them later.\nArgs: puncs (str): The punctuations to be processed. Defaults to _DEF_PUNCS.\nExample: >>> punc = Punctuation() >>> punc.strip(“This is. example !”) ‘This is example’\n>>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n>>> ' '.join(text_striped)\n'This is example'\n\n>>> text_restored = punc.restore(text_striped, punc_map)\n>>> text_restored[0]\n'This is. example !'\n\nsource\n\n\n\n\n PuncPosition (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nEnum for the punctuations positions\n\npunc = Punctuation()\ntext = \"This is. This is, example!\"\nprint(punc.strip(text))\nsplit_text, puncs = punc.strip_to_restore(text)\nprint(split_text, \" ---- \", puncs)\nrestored_text = punc.restore(split_text, puncs)\nprint(restored_text)\n\nThis is This is example\n['This is', 'This is', 'example']  ----  [_punc_index(punc='. ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc=', ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc='!', position=<PuncPosition.END: 1>)]\n['This is. This is, example!']"
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "source\n\n\n\n Decoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\n Encoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\ntorch.Size([10, 3])\n\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)\n\ntorch.Size([10, 784])"
  },
  {
    "objectID": "models.aligners.html",
    "href": "models.aligners.html",
    "title": "Aligners",
    "section": "",
    "text": "source\n\n\n\n AlignerWAV2VEC2 (text_normalizer, device='cuda')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Point (token_index:int, time_index:int, score:float)\n\n\nsource\n\n\n\n\n Segment (label:str, start:int, end:int, score:float)"
  },
  {
    "objectID": "models.aligners.html#usage",
    "href": "models.aligners.html#usage",
    "title": "Aligners",
    "section": "Usage",
    "text": "Usage\n\ntext_normalizer = TTSTextNormalizer().english_cleaners\naligner = AlignerWAV2VEC2(text_normalizer, device='cpu') # for CI on cpu\nwav_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.wav\"\ntxt_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.original.txt\"\nwav, sr = torchaudio.load(wav_path)\nwith open(txt_path, 'r') as f: txt = f.read()\nalignments = aligner.get_alignments(wav, txt)\n\nWord: HE, Confidence: 1.00, Start:0.121,  End: 0.242 sec\nWord: TRIED, Confidence: 0.91, Start:0.323,  End: 0.625 sec\nWord: TO, Confidence: 1.00, Start:0.686,  End: 0.787 sec\nWord: THINK, Confidence: 0.93, Start:0.948,  End: 1.311 sec\nWord: HOW, Confidence: 0.91, Start:1.473,  End: 1.675 sec\nWord: IT, Confidence: 0.71, Start:1.755,  End: 1.836 sec\nWord: COULD, Confidence: 0.75, Start:1.917,  End: 2.118 sec\nWord: BE, Confidence: 1.00, Start:2.239,  End: 2.461 sec"
  },
  {
    "objectID": "audio.utils.html",
    "href": "audio.utils.html",
    "title": "Audio Utilities",
    "section": "",
    "text": "source\n\nplot_waveform\n\n plot_waveform (waveform, sample_rate, title='Waveform', xlim=None,\n                ylim=None)\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nplot_waveform(wav,sr)"
  },
  {
    "objectID": "audio.embeddings.html",
    "href": "audio.embeddings.html",
    "title": "Audio Embedders",
    "section": "",
    "text": "source\n\n\n\n EncoDec (device:str='cpu')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\n# wav, sr = torch.rand((1, 24000)), 24000\n# wav, sr = np.random.random((1, 24000)), 24000\n\nencodec = EncoDec(device='cpu')\ncodes = encodec(wav,sr)\nprint(f\"wav: {wav.shape}, code: {codes.shape} \")\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.xlabel('frames')\nplt.ylabel('quantization')\nplt.imshow(codes.squeeze().cpu().numpy())\ndecoded = encodec.decode(codes)\nplot_waveform(decoded.detach().cpu().squeeze(0), encodec.sample_rate)\n\nwav: torch.Size([1, 102400]), code: torch.Size([1, 8, 480]) \n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n EncoDecExtractor (config=EncoDecConfig(frame_shift=0.013333333333333334,\n                   n_q=8))\n\nThe base class for all feature extractors in Lhotse. It is initialized with a config object, specific to a particular feature extraction method. The config is expected to be a dataclass so that it can be easily serialized.\nAll derived feature extractors must implement at least the following:\n\na name class attribute (how are these features called, e.g. ‘mfcc’)\na config_type class attribute that points to the configuration dataclass type\nthe extract method,\nthe frame_shift property.\n\nFeature extractors that support feature-domain mixing should additionally specify two static methods:\n\ncompute_energy, and\nmix.\n\nBy itself, the FeatureExtractor offers the following high-level methods that are not intended for overriding:\n\nextract_from_samples_and_store\nextract_from_recording_and_store\n\nThese methods run a larger feature extraction pipeline that involves data augmentation and disk storage.\n\nsource\n\n\n\n\n EncoDecConfig (frame_shift:float=0.013333333333333334, n_q:int=8)\n\n\nencodec_extractor = EncoDecExtractor()\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.jsonl.gz\")\nprint(cuts[0])\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\n# torch.set_num_threads(1)\n# torch.set_num_interop_threads(1)\n\n\n# TODO: make it work with num_jobs>1\ncuts = cuts.compute_and_store_features(\n    extractor=encodec_extractor,\n    storage_path=\"../data/en/LJSpeech-1.1/encodec\",\n    num_jobs=1,\n)\ncuts.to_file(\"../data/en/LJSpeech-1.1/cuts_encodec.jsonl.gz\")\nprint(cuts[0])\ncuts[0].plot_features()\n\n\n\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/cuts_encodec.jsonl.gz\")"
  },
  {
    "objectID": "audio.embeddings.html#audiolm",
    "href": "audio.embeddings.html#audiolm",
    "title": "Audio Embedders",
    "section": "AudioLM",
    "text": "AudioLM\n\n# TO DO"
  },
  {
    "objectID": "text.tokenizers.html",
    "href": "text.tokenizers.html",
    "title": "Text Tokenizers",
    "section": "",
    "text": "Assumes espeak backend is installed via apt-get install espeak\n\nsource\n\n\n\n Phonemizer (separator=<phonemizer.separator.Separator object at\n             0x7f53bfca0b50>, language='en-us', backend='espeak',\n             strip=True, preserve_punctuation=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\np = Phonemizer()\ntext = \"oh shoot I missed my train\"\nprint(p(text))\ntext = [\"Oh Dear, you'll be fine!\", \"this is it\"]\nprint(p(text))\n\noʊ ʃuːt aɪ mɪst maɪ tɹeɪn\n['oʊ dɪɹ, juːl biː faɪn!', 'ðɪs ɪz ɪt']\n\n\n\n\n\n\nphon = Phonemizer()\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.jsonl.gz\")\n# cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/ljspeech_fbank_all.jsonl.gz\")\nunique_phonemes = set()\nprint(cuts[0])\nwith CutSet.open_writer('../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz', overwrite=True) as writer:\n    for cut in tqdm(cuts):\n        phonemes = phon(cut.supervisions[0].text, n_jobs=1)\n        cut.custom = {\"phonemes\": phonemes}\n        writer.write(cut, flush=True)\n        unique_phonemes.update(list(phonemes))\nprint(unique_phonemes, len(unique_phonemes))\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\n\n\n{'ɪ', 'e', 'ɔ', 'ɑ', 'ə', 'ɛ', 'p', 'o', 'ŋ', 'z', 'a', 'ɾ', 'b', 'ð', 'd', 's', 'm', 'v', 'l', 'k', 'ᵻ', 'ʊ', 'i', ' ', 'ɡ', 'ː', 'n', 't', 'w', 'ɜ', 'ɚ', 'ʃ', 'f', 'ɹ', ',', 'ɐ', 'æ', 'ʌ', '.'} 39\n\n\n\nprocessed_cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz\")\nprint(processed_cuts[0])\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})\n\n\n\nmap = {}\nunique_syms = set()\nfor cut in processed_cuts:\n    unique_syms.update(list(cut.custom['phonemes']))\n\nfor (i, v) in enumerate(sorted(list(unique_syms))):\n    map[i] = v\nprint(map, len(map))\n\n{0: ' ', 1: ',', 2: '.', 3: 'a', 4: 'b', 5: 'd', 6: 'e', 7: 'f', 8: 'i', 9: 'k', 10: 'l', 11: 'm', 12: 'n', 13: 'o', 14: 'p', 15: 's', 16: 't', 17: 'v', 18: 'w', 19: 'z', 20: 'æ', 21: 'ð', 22: 'ŋ', 23: 'ɐ', 24: 'ɑ', 25: 'ɔ', 26: 'ə', 27: 'ɚ', 28: 'ɛ', 29: 'ɜ', 30: 'ɡ', 31: 'ɪ', 32: 'ɹ', 33: 'ɾ', 34: 'ʃ', 35: 'ʊ', 36: 'ʌ', 37: 'ː', 38: 'ᵻ'} 39"
  },
  {
    "objectID": "text.tokenizers.html#tokenizer",
    "href": "text.tokenizers.html#tokenizer",
    "title": "Text Tokenizers",
    "section": "Tokenizer",
    "text": "Tokenizer\nRequires download of spacy specific language e.g. python -m spacy download en\n\nsource\n\nTokenizer\n\n Tokenizer (backend='spacy', language='en')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#usage-1",
    "href": "text.tokenizers.html#usage-1",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\ntok = Tokenizer()\n\n\n# str -> List[str]\ns = \"Oh, yeah I don't know dude...\"\ntokenized = tok(s)\nprint(s)\nprint(tokenized)\nprint(tok.inverse(tokenized))\n\n# List[str]->List[List[str]]\ns = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\ntokenized = tok(s)\nprint(tokenized)\nprint(tok.inverse(tokenized))\n\n# Iterable -> Iterable\nds = AG_NEWS(split='test') # data pipe\nsample = next(iter(ds)) # (label, text)\n# print(sample)\nit = tok(ds)\ntokens = [token for token in it]\nprint(tokens[:2])\n\nOh, yeah I don't know dude...\n['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...']\nOh , yeah I do n't know dude ...\n[['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...'], ['this', 'is', 'a', 'test']]\n[\"Oh , yeah I do n't know dude ...\", 'this is a test']\n[['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['The', 'Race', 'is', 'On', ':', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'TORONTO', ',', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', ' ', '#', '36;10', 'million', 'Ansari', 'X', 'Prize', ',', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight', ',', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket', '.']]"
  },
  {
    "objectID": "text.tokenizers.html#numericalizer",
    "href": "text.tokenizers.html#numericalizer",
    "title": "Text Tokenizers",
    "section": "Numericalizer",
    "text": "Numericalizer\n\nsource\n\nNumericalizer\n\n Numericalizer (tokens_iter:Iterable, specials=['<pad>', '<unk>', '<bos>',\n                '<eos>'])\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#usage-2",
    "href": "text.tokenizers.html#usage-2",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\ntok = Tokenizer()\n# In the case of agnews, dataset is: [(index, text)]\ndef token_iterator(data_iter:Iterable)->Iterable:\n    for _, text in data_iter:\n        yield tok(text)\ntok_it= token_iterator(ds)\n# initialize numericalizer based on token iterator\nnum = Numericalizer(tok_it)\n\n\nprint(num('<pad>'), num('<unk>'))\n\n0 1\n\n\n\nprint(num.vocab['the'])\nprint(num('the'))\nprint(num(['<bos>', '<pad>', '<unk>', 'a', 'this', 'the', 'lkjsdf']))\nprint(num.inverse(0))\nprint(num.inverse([6,55]))\nprint(num([['<bos>', '<pad>'], ['<unk>', 'a', 'this', 'the', 'lkjsdf']]))\n\n4\n4\n[2, 0, 1, 9, 58, 4, 1]\n<pad>\n['.', 'Monday']\n[[2, 0], [1, 9, 58, 4, 1]]\n\n\n\ntokens = tok([\"here we go. asdflkj\", \"it was time...\"])\nprint(tokens)\nprint([num(tok) for tok in tokens])\nprint(num(tokens))\n\n[['here', 'we', 'go', '.', 'asdflkj'], ['it', 'was', 'time', '...']]\n[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n\n\n\nsource\n\nTextCollater\n\n TextCollater (tokenizer, numericalizer, padding_value:int=0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntexts = [\"this is it...\", \"this is the second sentence.\"]\nt = tok(texts)\nprint(t)\ntt = num(t)\nprint(tt)\nttt= [torch.Tensor(t) for t in tt]\n[t.shape[0] for t in ttt]\ncollater = TextCollater(tok, num)\nprint(collater.collate_list(texts))\ndl = DataLoader(dataset=ds, batch_size=2, shuffle=True, collate_fn=collater.collate_agnews)\n\n[['this', 'is', 'it', '...'], ['this', 'is', 'the', 'second', 'sentence', '.']]\n[[58, 27, 34, 67], [58, 27, 4, 95, 3714, 6]]\n(tensor([[  58,   27,   34,   67,    0,    0],\n        [  58,   27,    4,   95, 3714,    6]]), tensor([4, 6]))\n\n\n\nb = next(iter(dl))\nprint('batch: ', b)\ntokens, lens = b[0], b[1]\nfor token, len in zip(tokens, lens):\n    print(token[:len].tolist())\n    print(num.inverse(token[:len].tolist()))\n\nbatch:  (tensor([[ 3894,  6448,    13,   532,   179,  9855,  1683,    14,   932,    17,\n          3586,    18,   453,    11,  3894,   371,  2260,     5,     4,   100,\n            16,    19,   171,   284,     8,   312,  1820,     5,  1406,    42,\n            22,   454,   401,  2514,    28,     4,    63,    27,  2297,  3327,\n            21,    41,  2455,  2275,   797,    69],\n        [ 3592,  6422,  3288,     7,  1359,   928,    15,  2846,    17,    36,\n            18,     8,   212,   119,  2290,    71,   157,    15,  3202,  1163,\n          8066,    60,    13,  7023,   971,    20,    15,  1673,    10,  2735,\n          6044,    39,  2951, 10519,   194,     4,    15,   873,  8398,  1748,\n             6,     0,     0,     0,     0,     0]]), tensor([46, 41]))\n[3894, 6448, 13, 532, 179, 9855, 1683, 14, 932, 17, 3586, 18, 453, 11, 3894, 371, 2260, 5, 4, 100, 16, 19, 171, 284, 8, 312, 1820, 5, 1406, 42, 22, 454, 401, 2514, 28, 4, 63, 27, 2297, 3327, 21, 41, 2455, 2275, 797, 69]\n['Vodafone', 'Drops', 'on', 'Report', 'It', 'Supports', 'Bid', 'for', 'Sprint', '(', 'Update2', ')', 'Shares', 'in', 'Vodafone', 'Group', 'Plc', ',', 'the', 'world', '#', '39;s', 'largest', 'mobile', '-', 'phone', 'operator', ',', 'dropped', 'after', 'The', 'Wall', 'Street', 'Journal', 'said', 'the', 'company', 'is', 'considering', 'bidding', 'with', 'US', 'partner', 'Verizon', 'Communications', 'Inc.']\n[3592, 6422, 3288, 7, 1359, 928, 15, 2846, 17, 36, 18, 8, 212, 119, 2290, 71, 157, 15, 3202, 1163, 8066, 60, 13, 7023, 971, 20, 15, 1673, 10, 2735, 6044, 39, 2951, 10519, 194, 4, 15, 873, 8398, 1748, 6]\n['Winter', 'Concerns', 'Push', 'to', 'Record', 'High', ' ', 'SINGAPORE', '(', 'Reuters', ')', '-', 'Oil', 'prices', 'broke', 'into', 'record', ' ', 'territory', 'above', '\\\\$52', 'Thursday', 'on', 'heightened', 'concerns', 'that', ' ', 'supplies', 'of', 'heating', 'fuels', 'will', 'prove', 'inadequate', 'during', 'the', ' ', 'northern', 'hemisphere', 'winter', '.']"
  },
  {
    "objectID": "data.utils.lhotse.html",
    "href": "data.utils.lhotse.html",
    "title": "Lhotse support for datasets",
    "section": "",
    "text": "source\n\n\n\n Decoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\n Encoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\n\n\ndownload_ljspeech('/data/en/LJSpeech')\n# skip this step already done\nljspeech = prepare_ljspeech('/data/en/LJSpeech/LJSpeech-1.1', '../recipes/tts/ljspeech/data')\n\n\ncut_set = CutSet.from_manifests(**ljspeech)\nsubset = cut_set.subset(first=3)\nsubset.to_file('../data/en/LJSpeech-1.1/first_3.jsonl.gz')\nreload_subset = CutSet.from_file('../data/en/LJSpeech-1.1/first_3.jsonl.gz')\n\n\n\n\n\nencodec_extractor = EncoDecExtractor()\n\n\n# torch.set_num_threads(1)\n# torch.set_num_interop_threads(1)\n\n\n# TODO: fix bug for n_jobs >1\ncuts = subset.compute_and_store_features(\n    extractor=encodec_extractor,\n    storage_path=\"../data/en/LJSpeech-1.1/encodec/encodec\",\n    num_jobs=1,\n    # storage_type=NumpyHdf5Writer\n)\n\n\n\n\n\nprint(cuts[0])\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\ncuts.to_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\ncuts[0]\nreload_cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\nreload_cuts[0]\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\n# cuts[0].recording\n!soxi '/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav'\n\n\nInput File     : '/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav'\nChannels       : 1\nSample Rate    : 22050\nPrecision      : 16-bit\nDuration       : 00:00:09.66 = 212893 samples ~ 724.126 CDDA sectors\nFile Size      : 426k\nBit Rate       : 353k\nSample Encoding: 16-bit Signed Integer PCM\n\n\n\n\nstrategy = PrecomputedFeatures()\nfeats, feats_len = strategy(cuts)\n\n# print([(f\"feat: {feat.shape}\", f\"len: {feat_len}\") for feat in feats for feat_len in feats_len])\nprint([feat.shape for feat in feats])\nprint([int(feat_len) for feat_len in feats_len])\nprint(feats.shape, feats_len.shape)\n# TODO: debug OnTheFlyFeature case\n# strategy = OnTheFlyFeatures(extractor=encodec_extractor)\n# feats, feats_len = strategy(cuts)\n# print(feats, feats_len)\n\n[torch.Size([725, 8]), torch.Size([725, 8]), torch.Size([725, 8])]\n[724, 142, 725]\ntorch.Size([3, 725, 8]) torch.Size([3])\n\n\n\n\n\n\ncleaner = TTSTextNormalizer()\ntokenizer = Phonemizer()\n\n\nn_jobs = 1\nunique_phonemes = set()\nwith CutSet.open_writer('../data/en/LJSpeech-1.1/first_3.final.jsonl.gz', overwrite=True) as writer:\n    for cut in cuts:\n        text = cut.supervisions[0].text\n        print(text)\n        normalized = cleaner(text)\n        print(normalized)\n        phonemes = tokenizer(text)\n        print(phonemes)\n        cut.custom = {'normalized': normalized, 'phonemes': phonemes}\n        writer.write(cut, flush=True)\n        unique_phonemes.update(list(phonemes))\n\nPrinting, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition\nprinting, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition\n\n\nWARNING:phonemizer:words count mismatch on 200.0% of the lines (2/1)\n\n\npɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən\nin being comparatively modern.\nin being comparatively modern.\nɪn biːɪŋ kəmpæɹətɪvli mɑːdɚn.\nFor although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process\nfor although the chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the netherlands, by a similar process\n\n\nWARNING:phonemizer:words count mismatch on 100.0% of the lines (1/1)\n\n\nfɔːɹ ɔːlðoʊ ðə tʃaɪniːz tʊk ɪmpɹɛʃənz fɹʌm wʊd blɑːks ɛŋɡɹeɪvd ɪn ɹᵻliːf fɔːɹ sɛntʃɚɹiz bᵻfoːɹ ðə wʊdkʌɾɚz ʌvðə nɛðɜːləndz, baɪ ɐ sɪmɪlɚ pɹɑːsɛs\n\n\n\n\n\n\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.final.jsonl.gz\")\nprint(cuts[0])\nmap = {}\nunique_syms = set()\nfor cut in cuts:\n    unique_syms.update(list(cut.custom['phonemes']))\nfor (i, v) in enumerate(sorted(list(unique_syms))):\n    map[i] = v\nmap[len(map)] = \"<eps>\"\nprint(map, len(map))\n\njson_map = json.dumps(map)\nwith open(\"../data/en/LJSpeech-1.1/lexicon/map.json\",\"w\") as f:\n    f.write(json_map)\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'normalized': 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition', 'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})\n{0: ' ', 1: ',', 2: '.', 3: 'a', 4: 'b', 5: 'd', 6: 'e', 7: 'f', 8: 'i', 9: 'k', 10: 'l', 11: 'm', 12: 'n', 13: 'o', 14: 'p', 15: 's', 16: 't', 17: 'v', 18: 'w', 19: 'z', 20: 'æ', 21: 'ð', 22: 'ŋ', 23: 'ɐ', 24: 'ɑ', 25: 'ɔ', 26: 'ə', 27: 'ɚ', 28: 'ɛ', 29: 'ɜ', 30: 'ɡ', 31: 'ɪ', 32: 'ɹ', 33: 'ɾ', 34: 'ʃ', 35: 'ʊ', 36: 'ʌ', 37: 'ː', 38: 'ᵻ', 39: '<eps>'} 40\n\n\n\nwith open('../data/en/LJSpeech-1.1/lexicon/map.json', 'r') as f:\n    data = json.load(f)\n\nprint(data)\n\n{'0': ' ', '1': ',', '2': '.', '3': 'a', '4': 'b', '5': 'd', '6': 'e', '7': 'f', '8': 'i', '9': 'k', '10': 'l', '11': 'm', '12': 'n', '13': 'o', '14': 'p', '15': 's', '16': 't', '17': 'v', '18': 'w', '19': 'z', '20': 'æ', '21': 'ð', '22': 'ŋ', '23': 'ɐ', '24': 'ɑ', '25': 'ɔ', '26': 'ə', '27': 'ɚ', '28': 'ɛ', '29': 'ɜ', '30': 'ɡ', '31': 'ɪ', '32': 'ɹ', '33': 'ɾ', '34': 'ʃ', '35': 'ʊ', '36': 'ʌ', '37': 'ː', '38': 'ᵻ', '39': '<eps>'}\n\n\n\n\n\n\nfrom lhotse.dataset.collation import TokenCollater\nfrom typing import Tuple\nimport numpy as np\n\n\nsource\n\n\n\n\n\n PhonemeCollater (cuts:lhotse.cut.set.CutSet, add_eos:bool=True,\n                  add_bos:bool=True, pad_symbol:str='<pad>',\n                  bos_symbol:str='<bos>', eos_symbol:str='<eos>',\n                  unk_symbol:str='<unk>')\n\nCollate list of tokens\nMap sentences to integers. Sentences are padded to equal length. Beginning and end-of-sequence symbols can be added. Call .inverse(tokens_batch, tokens_lens) to reconstruct batch as string sentences.\nExample: >>> token_collater = TokenCollater(cuts) >>> tokens_batch, tokens_lens = token_collater(cuts.subset(first=32)) >>> original_sentences = token_collater.inverse(tokens_batch, tokens_lens)\nReturns: tokens_batch: IntTensor of shape (B, L) B: batch dimension, number of input sentences L: length of the longest sentence tokens_lens: IntTensor of shape (B,) Length of each sentence after adding  and  but before padding.\n\ncuts[0]\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='encodec', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'normalized': 'printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the exhibition', 'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})\n\n\n\npc = PhonemeCollater(cuts)\ntokens, tokens_len = pc(cuts)\nprint(tokens, tokens_len)\nprint(pc.inverse(tokens, tokens_len))\n\ntensor([[ 2, 18,  4, 36,  4, 35,  4, 16,  4, 20,  4, 35,  4, 26,  4,  5,  4,  4,\n          4, 35,  4, 16,  4, 25,  4, 35,  4,  4,  4, 17,  4, 39,  4, 16,  4, 14,\n          4, 12,  4,  4,  4, 19,  4, 32,  4, 16,  4, 19,  4,  4,  4, 22,  4, 35,\n          4, 25,  4,  4,  4, 22,  4, 35,  4, 20,  4, 38,  4,  4,  4, 22,  4, 12,\n          4, 41,  4,  4,  4, 28,  4, 41,  4, 36,  4,  4,  4, 24,  4, 20,  4,  4,\n          4, 18,  4, 36,  4, 32,  4, 23,  4, 30,  4, 16,  4, 20,  4,  4,  4, 13,\n          4, 30,  4, 16,  4, 19,  4, 33,  4, 41,  4, 16,  4,  9,  4,  5,  4,  4,\n          4,  9,  4, 35,  4, 11,  4, 31,  4, 23,  4,  4,  4, 11,  4, 36,  4, 40,\n          4, 15,  4,  4,  4, 15,  4, 17,  4, 39,  4, 19,  4, 20,  4,  4,  4, 35,\n          4, 11,  4,  4,  4, 16,  4, 28,  4, 41,  4, 20,  4,  4,  4, 11,  4, 36,\n          4, 40,  4, 15,  4,  4,  4, 29,  4, 41,  4, 14,  4,  4,  4, 25,  4, 35,\n          4,  4,  4, 28,  4, 41,  4, 36,  4, 20,  4, 19,  4,  4,  4, 24,  4, 16,\n          4,  9,  4,  4,  4, 13,  4, 36,  4, 24,  4, 11,  4, 20,  4, 19,  4,  4,\n          4, 36,  4, 32,  4, 18,  4, 36,  4, 42,  4, 23,  4, 32,  4, 16,  4, 20,\n          4, 42,  4,  9,  4,  4,  4, 35,  4, 16,  4, 25,  4, 35,  4,  4,  4, 32,\n          4, 13,  4, 19,  4, 35,  4,  8,  4, 35,  4, 38,  4, 30,  4, 16,  3,  0,\n          0],\n        [ 2, 35,  4, 16,  4,  4,  4,  8,  4, 12,  4, 41,  4, 35,  4, 26,  4,  4,\n          4, 13,  4, 30,  4, 15,  4, 18,  4, 24,  4, 36,  4, 30,  4, 20,  4, 35,\n          4, 21,  4, 14,  4, 12,  4,  4,  4, 15,  4, 28,  4, 41,  4,  9,  4, 31,\n          4, 16,  4,  6,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n          0],\n        [ 2, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 29,  4, 41,  4, 14,  4, 25,\n          4, 17,  4, 39,  4,  4,  4, 25,  4, 30,  4,  4,  4, 20,  4, 38,  4,  7,\n          4, 35,  4, 16,  4, 12,  4, 41,  4, 23,  4,  4,  4, 20,  4, 39,  4, 13,\n          4,  4,  4, 35,  4, 15,  4, 18,  4, 36,  4, 32,  4, 38,  4, 30,  4, 16,\n          4, 23,  4,  4,  4, 11,  4, 36,  4, 40,  4, 15,  4,  4,  4, 22,  4, 39,\n          4,  9,  4,  4,  4,  8,  4, 14,  4, 28,  4, 41,  4, 13,  4, 19,  4,  4,\n          4, 32,  4, 26,  4, 34,  4, 36,  4, 10,  4, 35,  4, 21,  4,  9,  4,  4,\n          4, 35,  4, 16,  4,  4,  4, 36,  4, 42,  4, 14,  4, 12,  4, 41,  4, 11,\n          4,  4,  4, 11,  4, 29,  4, 41,  4, 36,  4,  4,  4, 19,  4, 32,  4, 16,\n          4, 20,  4, 38,  4, 31,  4, 36,  4, 12,  4, 23,  4,  4,  4,  8,  4, 42,\n          4, 11,  4, 17,  4, 41,  4, 36,  4,  4,  4, 25,  4, 30,  4,  4,  4, 22,\n          4, 39,  4,  9,  4, 13,  4, 40,  4, 37,  4, 31,  4, 23,  4,  4,  4, 40,\n          4, 21,  4, 25,  4, 30,  4,  4,  4, 16,  4, 32,  4, 25,  4, 33,  4, 41,\n          4, 14,  4, 30,  4, 16,  4,  9,  4, 23,  4,  5,  4,  4,  4,  8,  4,  7,\n          4, 35,  4,  4,  4, 27,  4,  4,  4, 19,  4, 35,  4, 15,  4, 35,  4, 14,\n          4, 31,  4,  4,  4, 18,  4, 36,  4, 28,  4, 41,  4, 19,  4, 32,  4, 19,\n          3]]) tensor([287,  59, 289], dtype=torch.int32)\n['p ɹ ɪ n t ɪ ŋ ,   ɪ n ð ɪ   o ʊ n l i   s ɛ n s   w ɪ ð   w ɪ t ʃ   w i ː   ɑ ː ɹ   æ t   p ɹ ɛ z ə n t   k ə n s ɜ ː n d ,   d ɪ f ɚ z   f ɹ ʌ m   m o ʊ s t   ɪ f   n ɑ ː t   f ɹ ʌ m   ɔ ː l   ð ɪ   ɑ ː ɹ t s   æ n d   k ɹ æ f t s   ɹ ɛ p ɹ ᵻ z ɛ n t ᵻ d   ɪ n ð ɪ   ɛ k s ɪ b ɪ ʃ ə n', 'ɪ n   b i ː ɪ ŋ   k ə m p æ ɹ ə t ɪ v l i   m ɑ ː d ɚ n .', 'f ɔ ː ɹ   ɔ ː l ð o ʊ   ð ə   t ʃ a ɪ n i ː z   t ʊ k   ɪ m p ɹ ɛ ʃ ə n z   f ɹ ʌ m   w ʊ d   b l ɑ ː k s   ɛ ŋ ɡ ɹ e ɪ v d   ɪ n   ɹ ᵻ l i ː f   f ɔ ː ɹ   s ɛ n t ʃ ɚ ɹ i z   b ᵻ f o ː ɹ   ð ə   w ʊ d k ʌ ɾ ɚ z   ʌ v ð ə   n ɛ ð ɜ ː l ə n d z ,   b a ɪ   ɐ   s ɪ m ɪ l ɚ   p ɹ ɑ ː s ɛ s']"
  },
  {
    "objectID": "audio.datasets.stt.html",
    "href": "audio.datasets.stt.html",
    "title": "Speech to Text Datasets",
    "section": "",
    "text": "source\n\n\n\n STTDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins"
  },
  {
    "objectID": "audio.datasets.stt.html#librispeech-datamodule",
    "href": "audio.datasets.stt.html#librispeech-datamodule",
    "title": "Speech to Text Datasets",
    "section": "LibriSpeech DataModule",
    "text": "LibriSpeech DataModule\n\nsource\n\nLibriSpeechDataModule\n\n LibriSpeechDataModule (target_dir='/data/en',\n                        dataset_parts='mini_librispeech',\n                        output_dir='../recipes/stt/librispeech/data',\n                        num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en\nwhere data will be saved / retrieved\n\n\ndataset_parts\nstr\nmini_librispeech\neither full librispeech or mini subset\n\n\noutput_dir\nstr\n../recipes/stt/librispeech/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available"
  },
  {
    "objectID": "audio.datasets.stt.html#usage",
    "href": "audio.datasets.stt.html#usage",
    "title": "Speech to Text Datasets",
    "section": "Usage",
    "text": "Usage\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech/dev-clean-2\",\n    num_jobs=1\n)\n\n\n# skip this at export time to not waste time\n# download\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nDataset parts: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n\n\n\nrecs = RecordingSet.from_file(\"../data/en/LibriSpeech/dev-clean-2/librispeech_recordings_dev-clean-2.jsonl.gz\")\nsup = SupervisionSet(\"../data/en/LibriSpeech/dev-clean-2/librispeech_supervisions_dev-clean-2.jsonl.gz\")\nprint(len(recs),len(sup))\n\n25 80\n\n\n\ntest_dl = dm.test_dataloader()\nb = next(iter(test_dl))\nprint(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\n# plt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n\n# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])\n\ntorch.Size([1, 1113, 80]) torch.Size([1, 163]) torch.Size([1])\n\n\n\nprint(dm.cuts_test)\ncut = dm.cuts_test[0]\n# pprint(cut.to_dict())\ncut.plot_audio()\n\nCutSet(len=25) [underlying data type: <class 'dict'>]\n\n\n<AxesSubplot: >"
  },
  {
    "objectID": "models.autoencoders.html",
    "href": "models.autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "source\n\nAutoEncoder\n\n AutoEncoder (encoder:nimrod.modules.Encoder,\n              decoder:nimrod.modules.Decoder)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nType\nDetails\n\n\n\n\nencoder\nEncoder\nEncoder layer\n\n\ndecoder\nDecoder\nDecoder layer\n\n\n\n\nenc = Encoder()\ndec = Decoder()\na = AutoEncoder(enc, dec)\nbatch = torch.rand((10, 28*28))\ny = a(batch)\nprint(y.shape)\n\ntorch.Size([10, 784])\n\n\n\nds = MNISTDataset()\ndl = DataLoader(ds)\nb = next(iter(dl))\nprint(len(b), b[0].shape, b[1].shape)\n\n2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n\n\n\nsource\n\n\nAutoEncoderPL\n\n AutoEncoderPL (autoencoder:__main__.AutoEncoder)\n\nHooks to be used in LightningModule.\n\nautoencoder_pl = AutoEncoderPL(a)\nb = torch.rand((5,28*28))\ny = autoencoder_pl(b)\nprint(y.shape)\n\ntorch.Size([5, 784])"
  },
  {
    "objectID": "image.datasets.html",
    "href": "image.datasets.html",
    "title": "Image Datasets",
    "section": "",
    "text": "source\n\n\n\n ImageDataset ()\n\nBase class for image datasets providing visualization of (image, label) samples"
  },
  {
    "objectID": "image.datasets.html#mnist",
    "href": "image.datasets.html#mnist",
    "title": "Image Datasets",
    "section": "MNIST",
    "text": "MNIST\n\nMNIST dataset\n\nsource\n\n\nMNISTDataset\n\n MNISTDataset (data_dir:str='~/Data', train=True, transform:<module'torchv\n               ision.transforms.transforms'from'/home/syl20/anaconda3/envs\n               /nimrod/lib/python3.9/site-\n               packages/torchvision/transforms/transforms.py'>=ToTensor())\n\nMNIST digit dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_dir\nstr\n~/Data\npath where data is saved\n\n\ntrain\nbool\nTrue\ntrain or test dataset\n\n\ntransform\ntorchvision.transforms.transforms\nToTensor()\ndata formatting\n\n\n\n\n\nUsage\n\nds = MNISTDataset('~/Data', train=False)\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX, y = ds[0]\nprint(X.shape, y, X.type())\nds.show_idx(0)\ntrain, dev = ds.train_dev_split(0.8)\n\nNumber of samples in the dataset: 10000\ntorch.Size([1, 28, 28]) 7 torch.FloatTensor\n\n\n\n\n\n\n\nMNIST DataModule\n\nsource\n\n\nMNISTDataModule\n\n MNISTDataModule (data_dir:str='~/Data/',\n                  train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n                  batch_size:int=64, num_workers:int=0,\n                  pin_memory:bool=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\nUsage\n\ndm = MNISTDataModule(\n    data_dir=\"~/Data/\",train_val_test_split=[0.8, 0.1, 0.1],\n    batch_size = 64,\n    num_workers = 0,\n    pin_memory= False\n)\ndm.prepare_data()\ndm.setup()\ntest_dl = dm.test_dataloader()\nlen(dm.data_test[0])\nimgs = [dm.data_test[i][0] for i in range(5)]\nImageDataset.show_grid(imgs)"
  },
  {
    "objectID": "audio.datasets.tts.html",
    "href": "audio.datasets.tts.html",
    "title": "Audio TTS Datasets",
    "section": "",
    "text": "https://github.com/Lightning-AI/lightning/issues/10358 https://colab.research.google.com/drive/1HKSYPsWx_HoCdrnLpaPdYj5zwlPsM3NH\n\nsource\n\n\n\n\n LhotseTTSDataset (tokenizer=<class\n                   'lhotse.dataset.collation.TokenCollater'>, extractor=<l\n                   hotse.dataset.input_strategies.OnTheFlyFeatures object\n                   at 0x7ff2a6dabdf0>)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\ntype\nTokenCollater\ntext tokenizer\n\n\nextractor\nOnTheFlyFeatures\n<lhotse.dataset.input_strategies.OnTheFlyFeatures object at 0x7ff2a6dabdf0>\nfeature extractor\n\n\n\n\n# tok = TokenCollater()\n# ds = LhotseTTSDataset(tok)\n\nTypeError: __init__() missing 1 required positional argument: 'cuts'\n\n\n\n\n\n\nsource\n\n\n\n\n TTSDataset (tokenizer, num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins\n\n\n\n\n\n\n\n#(Waveform, Sample_rate, Original_text, Normalized_text, Speaker_ID, Chapter_ID, Utterance_ID)\nds = LIBRITTS(\"../data/en\", 'test-clean')\nprint(ds[0])\n\n(tensor([[0.0007, 0.0008, 0.0012,  ..., 0.0039, 0.0042, 0.0042]]), 24000, 'He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour-fattened sauce. Stuff it into you, his belly counselled him.', 'He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce. Stuff it into you, his belly counselled him.', 1089, 134686, '1089_134686_000001_000001')\n\n\n7\n\n\n\nsource\n\n\n\n\n LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n                     clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n                     mrod/recipes/libritts/data', num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en/libriTTS\nwhere data will be saved / retrieved\n\n\ndataset_parts\nlist\n[‘dev-clean’, ‘test-clean’]\neither full libritts or subset\n\n\noutput_dir\nstr\n/home/syl20/slg/nimrod/recipes/libritts/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available\n\n\n\n\n\n\n\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 95it [00:00, 5596.18it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<00:00, 46.46it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nPreparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 53.83it/s]"
  }
]