[
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "nimrod",
    "section": "Description",
    "text": "Description\nThis is a repo with minimal tooling, modules, models and recipes to get easily get started with deep learning training and experimentation"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nimrod",
    "section": "Install",
    "text": "Install\npip install nimrod"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "nimrod",
    "section": "Usage",
    "text": "Usage\nCheck recipes in recipes/ folder. For instance:\ncd recipes/autoencoder/\npython train.py"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "nimrod",
    "section": "Authors",
    "text": "Authors\n2023 Sylvain Le Groux slegroux@ccrma.stanford.edu"
  },
  {
    "objectID": "text.normalizers.html",
    "href": "text.normalizers.html",
    "title": "Text Normalizers",
    "section": "",
    "text": "source\n\n\n\n TTSTextNormalizer ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncleaner = TTSTextNormalizer()\nprint(cleaner.en_normalize_numbers(\"$350\"))\nprint(cleaner.expand_time_english(\"12:05pm\"))\nprint(cleaner.english_cleaners(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))\n\nthree hundred fifty dollars\ntwelve oh five p m\noh my dear! this is five dollars too soon... it's one oh four a m!\n\n\n\nsource\n\n\n\n\n Punctuation (puncs:str=';:,.!?¡¿—…\"«»“”')\n\nHandle punctuations in text.\nJust strip punctuations from text or strip and restore them later.\nArgs: puncs (str): The punctuations to be processed. Defaults to _DEF_PUNCS.\nExample: >>> punc = Punctuation() >>> punc.strip(“This is. example !”) ‘This is example’\n>>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n>>> ' '.join(text_striped)\n'This is example'\n\n>>> text_restored = punc.restore(text_striped, punc_map)\n>>> text_restored[0]\n'This is. example !'\n\nsource\n\n\n\n\n PuncPosition (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nEnum for the punctuations positions\n\npunc = Punctuation()\ntext = \"This is. This is, example!\"\nprint(punc.strip(text))\nsplit_text, puncs = punc.strip_to_restore(text)\nprint(split_text, \" ---- \", puncs)\nrestored_text = punc.restore(split_text, puncs)\nprint(restored_text)\n\nThis is This is example\n['This is', 'This is', 'example']  ----  [_punc_index(punc='. ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc=', ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc='!', position=<PuncPosition.END: 1>)]\n['This is. This is, example!']"
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "source\n\nDecoder\n\n Decoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nEncoder\n\n Encoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\ntorch.Size([10, 3])\n\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)\n\ntorch.Size([10, 784])"
  },
  {
    "objectID": "models.aligners.html",
    "href": "models.aligners.html",
    "title": "Aligners",
    "section": "",
    "text": "source\n\n\n\n AlignerWAV2VEC2 (text_normalizer, device='cuda')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Point (token_index:int, time_index:int, score:float)\n\n\nsource\n\n\n\n\n Segment (label:str, start:int, end:int, score:float)"
  },
  {
    "objectID": "models.aligners.html#usage",
    "href": "models.aligners.html#usage",
    "title": "Aligners",
    "section": "Usage",
    "text": "Usage\n\ntext_normalizer = TTSTextNormalizer().english_cleaners\naligner = AlignerWAV2VEC2(text_normalizer)\nwav_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.wav\"\ntxt_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.original.txt\"\nwav, sr = torchaudio.load(wav_path)\nwith open(txt_path, 'r') as f: txt = f.read()\nalignments = aligner.get_alignments(wav, txt)\n\nWord: HE, Confidence: 1.00, Start:0.121,  End: 0.242 sec\nWord: TRIED, Confidence: 0.91, Start:0.323,  End: 0.625 sec\nWord: TO, Confidence: 1.00, Start:0.686,  End: 0.787 sec\nWord: THINK, Confidence: 0.93, Start:0.948,  End: 1.311 sec\nWord: HOW, Confidence: 0.91, Start:1.473,  End: 1.675 sec\nWord: IT, Confidence: 0.71, Start:1.755,  End: 1.836 sec\nWord: COULD, Confidence: 0.75, Start:1.917,  End: 2.118 sec\nWord: BE, Confidence: 1.00, Start:2.239,  End: 2.461 sec"
  },
  {
    "objectID": "audio.utils.html",
    "href": "audio.utils.html",
    "title": "Audio Utilities",
    "section": "",
    "text": "source\n\nplot_waveform\n\n plot_waveform (waveform, sample_rate, title='Waveform', xlim=None,\n                ylim=None)\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nplot_waveform(wav,sr)"
  },
  {
    "objectID": "audio.embeddings.html",
    "href": "audio.embeddings.html",
    "title": "Audio Embedders",
    "section": "",
    "text": "source\n\n\n\n EncoDec (device='cpu')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "audio.embeddings.html#usage",
    "href": "audio.embeddings.html#usage",
    "title": "Audio Embedders",
    "section": "Usage",
    "text": "Usage\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nencodec = EncoDec(device='cpu')\ncodes = encodec(wav,sr)\nplt.rcParams[\"figure.figsize\"] = (10,10)\nplt.xlabel('frames')\nplt.ylabel('quantization')\nplt.imshow(codes.squeeze().cpu().numpy())\ndecoded = encodec.decode(codes)\nplot_waveform(decoded.detach().cpu().squeeze(0), encodec.sample_rate)\n\n\n\n\n\n\n\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nplt.imshow(codes.squeeze().numpy())\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\nprint(codes)\nprint(encoded_frames)\n\ntensor([[[ 408,  106,  876,  ...,  876,  738, 1017],\n         [ 937,  363,  544,  ...,  544,  544,  913],\n         [  36,  915,  989,  ...,  989,  989,  821],\n         ...,\n         [ 903,  982,  939,  ...,  851,  683,  986],\n         [ 900,  977,  748,  ...,  772,  834,  772],\n         [ 975,  948,  828,  ...,  701,  975,  975]]])\n[(tensor([[[ 408,  106,  876,  ...,  876,  738, 1017],\n         [ 937,  363,  544,  ...,  544,  544,  913],\n         [  36,  915,  989,  ...,  989,  989,  821],\n         ...,\n         [ 903,  982,  939,  ...,  851,  683,  986],\n         [ 900,  977,  748,  ...,  772,  834,  772],\n         [ 975,  948,  828,  ...,  701,  975,  975]]]), None)]\n\n\n\nframes_from_code = [(codes, None)]\nprint(frames_from_code)\ndecoded = model.decode(encoded_frames=frames_from_code)\n# decoded.shape\n\n[(tensor([[[ 408,  106,  876,  ...,  876,  738, 1017],\n         [ 937,  363,  544,  ...,  544,  544,  913],\n         [  36,  915,  989,  ...,  989,  989,  821],\n         ...,\n         [ 903,  982,  939,  ...,  851,  683,  986],\n         [ 900,  977,  748,  ...,  772,  834,  772],\n         [ 975,  948,  828,  ...,  701,  975,  975]]]), None)]\n\n\n\nplot_waveform(decoded.detach().squeeze(0), model.sample_rate)\n\n\n\n\n\nipd.Audio(decoded.detach().squeeze(0).numpy(), rate=model.sample_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "audio.embeddings.html#audiolm",
    "href": "audio.embeddings.html#audiolm",
    "title": "Audio Embedders",
    "section": "AudioLM",
    "text": "AudioLM\n\n# TO DO"
  },
  {
    "objectID": "text.tokenizers.html",
    "href": "text.tokenizers.html",
    "title": "Text Tokenizers",
    "section": "",
    "text": "Assumes espeak backend is installed via apt-get install espeak\n\nsource\n\n\n\n Phonemizer (separator=<phonemizer.separator.Separator object at\n             0x7fb6d88c8370>, language='en-us', backend='espeak',\n             strip=True, preserve_punctuation=True)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#usage",
    "href": "text.tokenizers.html#usage",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\np = Phonemizer()\ntext = \"Oh Dear! This suck...\\n We'll be fine!\"\nprint(p(text))\n\noʊ dɪɹ! ðɪs sʌk...\nwiːl biː faɪn!"
  },
  {
    "objectID": "text.tokenizers.html#tokenizer",
    "href": "text.tokenizers.html#tokenizer",
    "title": "Text Tokenizers",
    "section": "Tokenizer",
    "text": "Tokenizer\nRequires download of spacy specific language e.g. python -m spacy download en\n\nsource\n\nTokenizer\n\n Tokenizer (backend='spacy', language='en')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#numericalizer",
    "href": "text.tokenizers.html#numericalizer",
    "title": "Text Tokenizers",
    "section": "Numericalizer",
    "text": "Numericalizer\n\nsource\n\nNumericalizer\n\n Numericalizer (tokenizer:__main__.Tokenizer)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n# TODO: collate text + add special characters & 0 != unk"
  },
  {
    "objectID": "text.tokenizers.html#usage-1",
    "href": "text.tokenizers.html#usage-1",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\ntok = Tokenizer()\ntokenized = tok(\"Oh, yeah\\n I don't know dude...\")\nds = AG_NEWS(split='test') # data pipe\nsample = next(iter(ds)) # (label, text)\nprint(sample)\ntokenized_ds = tok.tokenize_iter(ds)\nsample = next(iter(tokenized_ds))\nprint(sample)\n\n/home/syl20/anaconda3/envs/nimrod/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n  warnings.warn(\n\n\n(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.']\n\n\n\nnum = Numericalizer(tok)\nmapper = num.build_map_from_iter(ds)\nprint(mapper[\"<unk>\"])\nprint(mapper(tok(\"here we go. asdflkj\")))\n# text_pipeline = lambda x: voc(tokenizer(x))\nprint(mapper(tokenized))\n\n0\n[531, 1037, 307, 3, 0]\n[7808, 2, 0, 0, 296, 378, 255, 1324, 0, 64]\n\n\n\na = mapper(tok(\"here we go. asdflkj\"))\nprint(len(a))\nb = mapper(tok(\"Oh, yeah\\n I don't know dude...\"))\nprint(len(b))\nmini_batch = [a, b]\nx = [torch.LongTensor(x_i) for x_i in mini_batch]\nprint(x)\nx_padded = pad_sequence(x, batch_first=True, padding_value=0)\nprint(x_padded)\n\n5\n10\n[tensor([ 531, 1037,  307,    3,    0]), tensor([7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64])]\ntensor([[ 531, 1037,  307,    3,    0,    0,    0,    0,    0,    0],\n        [7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64]])\n\n\n\ndef text_collate(batch):\n    # batch: [(label, text), ]\n    # from ipdb import set_trace; set_trace()\n    texts = [row[1] for row in batch]\n    tokens = [torch.LongTensor(mapper(tok(row))) for row in texts]\n    text_lens = [len(token) for token in tokens]\n    text_pad = pad_sequence(tokens, batch_first=True, padding_value=-1)\n    return text_pad, text_lens\n\n\ndl = DataLoader(dataset=ds, batch_size=5, shuffle=True, collate_fn=text_collate)\n\n\nb = next(iter(dl))\nprint(b)\n\n(tensor([[11991, 29434, 24121, 10187,  2896,   147,     6,   238,    17,    28,\n            81,  4305,    20,    12,    13, 16249,    13,    76,    27,    26,\n             1,  2065,  3871,     7, 17562,     2, 11991,  1048,    24,     4,\n           885,     1, 11025,     7, 10187,    20,     6,  5517,     3,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1],\n        [23259,    11,     6, 17457,  1195, 12240,     6,  9439, 13543,     1,\n          2814,  2657,     7,  6523,  4490,    22, 15031,    82,  1383,    21,\n         21972,    22,     2, 14020, 23182,    11,     1,  4490,   907,     2,\n            56,    72,  1076,     1,  2196,     5,  2173, 10377,     3,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1],\n        [ 3891,  1535,   165,    18,   151,  3243,  1111,  3891,    28,  1295,\n           139,    11,   161,    43,   145,     5,  1143,  4333,    11,  1545,\n             4,   206,  7221,    50,    23,  1592,   383,   919,     3,  3891,\n         22706,     3,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1],\n        [ 9843,   922, 11719,  3058,   464,     5,   274,    14,    32,    15,\n            32,     5,   922,  2454,  5026,  5014,   214,    34,    47,   703,\n          1902,     2,     6,    48,     5,   203,   502,     2,     9,  2692,\n            65,   203,     8,   413,  1651,     4,   644,   687,     8,     1,\n          5529,     7,     1,  1773,  2040,     5,  1235,  5943,    18,    35,\n           464,     5,   274,  4055,     7,     1,  2136,  3058,    10,    52,\n             3],\n        [19542,  4688,  1316,    50,   661, 19541, 10660,  5047,  4688,  1813,\n            63,    87,    10,    34,   173,  1315,     4,  2205,    34,  3847,\n          1308,  1235,     3,  4688,     2,    73,  2182,     1,   173,   311,\n             7,     1,  3012,   776,  2775,    11,  4983,     9, 14848,     6,\n         29237,  1717,     4,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1]]), [39, 39, 32, 61, 43])\n\n\n\ntorch.sum(torch.Tensor([17564, 24659, 29258, 26399,    13, 16230,  1350,  1321,     4,     6,\n          1649,    20,  1471,     7,   386, 10675,     6,   784,   648,     8,\n          1734,    58,     1,   351,  4033,     7,   315,   217,  4224,  2494,\n            13,    16,  2880,     5,   363,   105,     3,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n            -1,    -1,    -1,    -1,    -1,    -1])!= -1)\n\ntensor(37)"
  },
  {
    "objectID": "models.autoencoders.html",
    "href": "models.autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "source\n\nAutoEncoder\n\n AutoEncoder (encoder:nimrod.modules.Encoder,\n              decoder:nimrod.modules.Decoder)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nType\nDetails\n\n\n\n\nencoder\nEncoder\nEncoder layer\n\n\ndecoder\nDecoder\nDecoder layer\n\n\n\n\nenc = Encoder()\ndec = Decoder()\na = AutoEncoder(enc, dec)\nbatch = torch.rand((10, 28*28))\ny = a(batch)\nprint(y.shape)\n\ntorch.Size([10, 784])\n\n\n\nds = MNISTDataset()\ndl = DataLoader(ds)\nb = next(iter(dl))\nprint(len(b), b[0].shape, b[1].shape)\n\n2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n\n\n\nsource\n\n\nAutoEncoderPL\n\n AutoEncoderPL (autoencoder:__main__.AutoEncoder)\n\nHooks to be used in LightningModule.\n\nautoencoder_pl = AutoEncoderPL(a)\nb = torch.rand((5,28*28))\ny = autoencoder_pl(b)\nprint(y.shape)\n\ntorch.Size([5, 784])"
  },
  {
    "objectID": "audio.datasets.stt.html",
    "href": "audio.datasets.stt.html",
    "title": "Speech to Text Datasets",
    "section": "",
    "text": "source\n\n\n\n STTDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins"
  },
  {
    "objectID": "audio.datasets.stt.html#librispeech-datamodule",
    "href": "audio.datasets.stt.html#librispeech-datamodule",
    "title": "Speech to Text Datasets",
    "section": "LibriSpeech DataModule",
    "text": "LibriSpeech DataModule\n\nsource\n\nLibriSpeechDataModule\n\n LibriSpeechDataModule (target_dir='/data/en',\n                        dataset_parts='mini_librispeech',\n                        output_dir='../recipes/stt/librispeech/data',\n                        num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en\nwhere data will be saved / retrieved\n\n\ndataset_parts\nstr\nmini_librispeech\neither full librispeech or mini subset\n\n\noutput_dir\nstr\n../recipes/stt/librispeech/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available"
  },
  {
    "objectID": "audio.datasets.stt.html#usage",
    "href": "audio.datasets.stt.html#usage",
    "title": "Speech to Text Datasets",
    "section": "Usage",
    "text": "Usage\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech/dev-clean-2\",\n    num_jobs=1\n)\n\n\n# skip this at export time to not waste time\n# download\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nDataset parts: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n\n\n\nrecs = RecordingSet.from_file(\"../data/en/LibriSpeech/dev-clean-2/librispeech_recordings_dev-clean-2.jsonl.gz\")\nsup = SupervisionSet(\"../data/en/LibriSpeech/dev-clean-2/librispeech_supervisions_dev-clean-2.jsonl.gz\")\nprint(len(recs),len(sup))\n\n25 80\n\n\n\ntest_dl = dm.test_dataloader()\nb = next(iter(test_dl))\nprint(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\n# plt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n\n# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])\n\ntorch.Size([1, 1113, 80]) torch.Size([1, 163]) torch.Size([1])\n\n\n\nprint(dm.cuts_test)\ncut = dm.cuts_test[0]\n# pprint(cut.to_dict())\ncut.plot_audio()\n\nCutSet(len=25) [underlying data type: <class 'dict'>]\n\n\n<AxesSubplot: >"
  },
  {
    "objectID": "image.datasets.html",
    "href": "image.datasets.html",
    "title": "Image Datasets",
    "section": "",
    "text": "source\n\n\n\n ImageDataset ()\n\nBase class for image datasets providing visualization of (image, label) samples"
  },
  {
    "objectID": "image.datasets.html#mnist",
    "href": "image.datasets.html#mnist",
    "title": "Image Datasets",
    "section": "MNIST",
    "text": "MNIST\n\nMNIST dataset\n\nsource\n\n\nMNISTDataset\n\n MNISTDataset (data_dir:str='~/Data', train=True, transform:<module'torchv\n               ision.transforms.transforms'from'/home/syl20/anaconda3/envs\n               /nimrod/lib/python3.9/site-\n               packages/torchvision/transforms/transforms.py'>=ToTensor())\n\nMNIST digit dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_dir\nstr\n~/Data\npath where data is saved\n\n\ntrain\nbool\nTrue\ntrain or test dataset\n\n\ntransform\ntorchvision.transforms.transforms\nToTensor()\ndata formatting\n\n\n\n\n\nUsage\n\nds = MNISTDataset('~/Data', train=False)\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX, y = ds[0]\nprint(X.shape, y, X.type())\nds.show_idx(0)\ntrain, dev = ds.train_dev_split(0.8)\n\nNumber of samples in the dataset: 10000\ntorch.Size([1, 28, 28]) 7 torch.FloatTensor\n\n\n\n\n\n\n\nMNIST DataModule\n\nsource\n\n\nMNISTDataModule\n\n MNISTDataModule (data_dir:str='~/Data/',\n                  train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n                  batch_size:int=64, num_workers:int=0,\n                  pin_memory:bool=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\nUsage\n\ndm = MNISTDataModule(\n    data_dir=\"~/Data/\",train_val_test_split=[0.8, 0.1, 0.1],\n    batch_size = 64,\n    num_workers = 0,\n    pin_memory= False\n)\ndm.prepare_data()\ndm.setup()\ntest_dl = dm.test_dataloader()\nlen(dm.data_test[0])\nimgs = [dm.data_test[i][0] for i in range(5)]\nImageDataset.show_grid(imgs)"
  },
  {
    "objectID": "audio.datasets.tts.html",
    "href": "audio.datasets.tts.html",
    "title": "Audio TTS Datasets",
    "section": "",
    "text": "source\n\n\n\n\n TTSDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins\n\n\n\n\n\n\n\n# #| export\n# class LJSpeechDataModule(LightningDataModule):\n#     def __init__(self,\n#         target_dir=\"/data/en\", # where data will be saved / retrieved\n#         dataset_parts=\"mini_librispeech\", # either full librispeech or mini subset\n#         output_dir=\"../recipes/tts/ljspeech/data\" # where to save manifest\n#     ):\n#         super().__init__()\n#         self.save_hyperparameters(logger=False)\n\n#     def prepare_data(self,) -> None:\n#         download_librispeech(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)\n\n#     def setup(self, stage = None):\n#         libri = prepare_librispeech(corpus_dir=Path(self.hparams.target_dir) / \"LibriSpeech\", output_dir=self.hparams.output_dir)\n#         self.cuts_train = CutSet.from_manifests(**libri[\"train-clean-5\"])\n#         self.cuts_test = CutSet.from_manifests(**libri[\"dev-clean-2\"])\n#         self.tokenizer = TokenCollater(self.cuts_train)\n#         self.tokenizer(self.cuts_test.subset(first=2))\n#         self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n\n#     def train_dataloader(self):\n#         train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\n#         return DataLoader(STTDataset(self.tokenizer), sampler=train_sampler, batch_size=None, num_workers=2)\n\n#     def test_dataloader(self):\n#         test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False, bucket_method=\"equal_duration\")\n#         return DataLoader(STTDataset(self.tokenizer), sampler=test_sampler, batch_size=None, num_workers=2)\n\n#     @property\n#     def model_kwargs(self):\n#         return {\n#             \"odim\": len(self.tokenizer.idx2token),\n#         }\n\n\n\n\n\nsource\n\n\n\n\n LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n                     clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n                     mrod/recipes/libritts/data', num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en/libriTTS\nwhere data will be saved / retrieved\n\n\ndataset_parts\nlist\n[‘dev-clean’, ‘test-clean’]\neither full libritts or subset\n\n\noutput_dir\nstr\n/home/syl20/slg/nimrod/recipes/libritts/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available\n\n\n\n\n\n\n\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 95it [00:00, 4875.90it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<00:00, 48.20it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nPreparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 50.96it/s]"
  }
]