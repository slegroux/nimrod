[
  {
    "objectID": "text.normalizers.html",
    "href": "text.normalizers.html",
    "title": "Text Normalizers",
    "section": "",
    "text": "source\n\n\n\n TTSTextNormalizer (language='en')\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncleaner = TTSTextNormalizer()\nprint(cleaner.en_normalize_numbers(\"$350\"))\nprint(cleaner.expand_time_english(\"12:05pm\"))\nprint(cleaner(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))\nprint(cleaner([\"Oh my dear! this is $5 too soon...\", \"It's 1:04 am!\"]))\n\nthree hundred fifty dollars\ntwelve oh five p m\noh my dear! this is five dollars too soon... it's one oh four a m!\n['oh my dear! this is five dollars too soon...', \"it's one oh four a m!\"]\n\n\n\nsource\n\n\n\n\n Punctuation (puncs:str=';:,.!?¡¿—…\"«»“”')\n\nHandle punctuations in text.\nJust strip punctuations from text or strip and restore them later.\nArgs: puncs (str): The punctuations to be processed. Defaults to _DEF_PUNCS.\nExample: >>> punc = Punctuation() >>> punc.strip(“This is. example !”) ‘This is example’\n>>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n>>> ' '.join(text_striped)\n'This is example'\n\n>>> text_restored = punc.restore(text_striped, punc_map)\n>>> text_restored[0]\n'This is. example !'\n\nsource\n\n\n\n\n PuncPosition (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nEnum for the punctuations positions\n\npunc = Punctuation()\ntext = \"This is. This is, example!\"\nprint(punc.strip(text))\nsplit_text, puncs = punc.strip_to_restore(text)\nprint(split_text, \" ---- \", puncs)\nrestored_text = punc.restore(split_text, puncs)\nprint(restored_text)\n\nThis is This is example\n['This is', 'This is', 'example']  ----  [_punc_index(punc='. ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc=', ', position=<PuncPosition.MIDDLE: 2>), _punc_index(punc='!', position=<PuncPosition.END: 1>)]\n['This is. This is, example!']"
  },
  {
    "objectID": "models.aligners.html",
    "href": "models.aligners.html",
    "title": "Aligners",
    "section": "",
    "text": "source\n\n\n\n AlignerWAV2VEC2 (text_normalizer, device='cuda')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Point (token_index:int, time_index:int, score:float)\n\n\nsource\n\n\n\n\n Segment (label:str, start:int, end:int, score:float)"
  },
  {
    "objectID": "models.aligners.html#usage",
    "href": "models.aligners.html#usage",
    "title": "Aligners",
    "section": "Usage",
    "text": "Usage\n\ntext_normalizer = TTSTextNormalizer().english_cleaners\naligner = AlignerWAV2VEC2(text_normalizer, device='cpu') # for CI on cpu\nwav_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.wav\"\ntxt_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.original.txt\"\nwav, sr = torchaudio.load(wav_path)\nwith open(txt_path, 'r') as f: txt = f.read()\nalignments = aligner.get_alignments(wav, txt)\n\nWord: HE, Confidence: 1.00, Start:0.121,  End: 0.242 sec\nWord: TRIED, Confidence: 0.91, Start:0.323,  End: 0.625 sec\nWord: TO, Confidence: 1.00, Start:0.686,  End: 0.787 sec\nWord: THINK, Confidence: 0.93, Start:0.948,  End: 1.311 sec\nWord: HOW, Confidence: 0.91, Start:1.473,  End: 1.675 sec\nWord: IT, Confidence: 0.71, Start:1.755,  End: 1.836 sec\nWord: COULD, Confidence: 0.75, Start:1.917,  End: 2.118 sec\nWord: BE, Confidence: 1.00, Start:2.239,  End: 2.461 sec"
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "source\n\n\n\n Decoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\n\n\n Encoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\ntorch.Size([10, 3])\n\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)\n\ntorch.Size([10, 784])"
  },
  {
    "objectID": "audio.utils.html",
    "href": "audio.utils.html",
    "title": "Audio Utilities",
    "section": "",
    "text": "source\n\nplot_waveform\n\n plot_waveform (waveform, sample_rate, title='Waveform', xlim=None,\n                ylim=None)\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nplot_waveform(wav,sr)"
  },
  {
    "objectID": "audio.embeddings.html",
    "href": "audio.embeddings.html",
    "title": "Audio Embedders",
    "section": "",
    "text": "source\n\n\n\n EncoDec (device:str='cpu')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\n# wav, sr = torch.rand((1, 24000)), 24000\n# wav, sr = np.random.random((1, 24000)), 24000\n\nencodec = EncoDec(device='cpu')\ncodes = encodec(wav,sr)\nprint(f\"wav: {wav.shape}, code: {codes.shape} \")\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.xlabel('frames')\nplt.ylabel('quantization')\nplt.imshow(codes.squeeze().cpu().numpy())\ndecoded = encodec.decode(codes)\nplot_waveform(decoded.detach().cpu().squeeze(0), encodec.sample_rate)\n\nwav: torch.Size([1, 102400]), code: torch.Size([1, 8, 480]) \n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n EncoDecExtractor (config=EncoDecConfig(frame_shift=0.013333333333333334,\n                   n_qs=8))\n\nThe base class for all feature extractors in Lhotse. It is initialized with a config object, specific to a particular feature extraction method. The config is expected to be a dataclass so that it can be easily serialized.\nAll derived feature extractors must implement at least the following:\n\na name class attribute (how are these features called, e.g. ‘mfcc’)\na config_type class attribute that points to the configuration dataclass type\nthe extract method,\nthe frame_shift property.\n\nFeature extractors that support feature-domain mixing should additionally specify two static methods:\n\ncompute_energy, and\nmix.\n\nBy itself, the FeatureExtractor offers the following high-level methods that are not intended for overriding:\n\nextract_from_samples_and_store\nextract_from_recording_and_store\n\nThese methods run a larger feature extraction pipeline that involves data augmentation and disk storage.\n\nsource\n\n\n\n\n EncoDecConfig (frame_shift:float=0.013333333333333334, n_qs:int=8)\n\n\nencodec_extractor = EncoDecExtractor()\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.jsonl.gz\")\nprint(cuts[0])\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\ntorch.set_num_threads(1)\ntorch.set_num_interop_threads(1)\n\n\n# TODO: make it work with num_jobs>1\ncuts = cuts.compute_and_store_features(\n    extractor=encodec_extractor,\n    storage_path=\"../data/en/LJSpeech-1.1/encodec\",\n    num_jobs=1,\n)\ncuts.to_file(\"../data/en/LJSpeech-1.1/cuts_encodec.jsonl.gz\")\nprint(cuts[0])\ncuts[0].plot_features()\n\n\n\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='None', num_frames=724, num_features=8, frame_shift=0.013333333333333334, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../data/en/LJSpeech-1.1/encodec.lca', storage_key='0,8029,3610', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n<matplotlib.image.AxesImage>"
  },
  {
    "objectID": "audio.embeddings.html#audiolm",
    "href": "audio.embeddings.html#audiolm",
    "title": "Audio Embedders",
    "section": "AudioLM",
    "text": "AudioLM\n\n# TO DO"
  },
  {
    "objectID": "audio.datasets.stt.html",
    "href": "audio.datasets.stt.html",
    "title": "Speech to Text Datasets",
    "section": "",
    "text": "source\n\n\n\n STTDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins"
  },
  {
    "objectID": "audio.datasets.stt.html#librispeech-datamodule",
    "href": "audio.datasets.stt.html#librispeech-datamodule",
    "title": "Speech to Text Datasets",
    "section": "LibriSpeech DataModule",
    "text": "LibriSpeech DataModule\n\nsource\n\nLibriSpeechDataModule\n\n LibriSpeechDataModule (target_dir='/data/en',\n                        dataset_parts='mini_librispeech',\n                        output_dir='../recipes/stt/librispeech/data',\n                        num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en\nwhere data will be saved / retrieved\n\n\ndataset_parts\nstr\nmini_librispeech\neither full librispeech or mini subset\n\n\noutput_dir\nstr\n../recipes/stt/librispeech/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available"
  },
  {
    "objectID": "audio.datasets.stt.html#usage",
    "href": "audio.datasets.stt.html#usage",
    "title": "Speech to Text Datasets",
    "section": "Usage",
    "text": "Usage\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech/dev-clean-2\",\n    num_jobs=1\n)\n\n\n# skip this at export time to not waste time\n# download\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nDataset parts: 100%|██████████| 1/1 [00:00<00:00, 31.98it/s]\n\n\n\nrecs = RecordingSet.from_file(\"../data/en/LibriSpeech/dev-clean-2/librispeech_recordings_dev-clean-2.jsonl.gz\")\nsup = SupervisionSet(\"../data/en/LibriSpeech/dev-clean-2/librispeech_supervisions_dev-clean-2.jsonl.gz\")\nprint(len(recs),len(sup))\n\n25 80\n\n\n\ntest_dl = dm.test_dataloader()\nb = next(iter(test_dl))\nprint(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\n# plt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n\n# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])\n\ntorch.Size([1, 1113, 80]) torch.Size([1, 163]) torch.Size([1])\n\n\n\nprint(dm.cuts_test)\ncut = dm.cuts_test[0]\n# pprint(cut.to_dict())\ncut.plot_audio()\n\nCutSet(len=25) [underlying data type: <class 'dict'>]\n\n\n<AxesSubplot: >"
  },
  {
    "objectID": "text.tokenizers.html",
    "href": "text.tokenizers.html",
    "title": "Text Tokenizers",
    "section": "",
    "text": "Assumes espeak backend is installed via apt-get install espeak\n\nsource\n\n\n\n Phonemizer (separator=<phonemizer.separator.Separator object at\n             0x7fb5601779a0>, language='en-us', backend='espeak',\n             strip=True, preserve_punctuation=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\np = Phonemizer()\ntext = \"oh shoot I missed my train\"\nprint(p(text))\ntext = [\"Oh Dear, you'll be fine!\", \"this is it\"]\nprint(p(text))\n\noʊ ʃuːt aɪ mɪst maɪ tɹeɪn\n['oʊ dɪɹ, juːl biː faɪn!', 'ðɪs ɪz ɪt']\n\n\n\n\n\n\nphon = Phonemizer()\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.jsonl.gz\")\nunique_phonemes = set()\nprint(cuts[0])\nwith CutSet.open_writer('../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz', overwrite=True) as writer:\n    for cut in cuts:\n        phonemes = phon(cut.supervisions[0].text)\n        cut.custom = {\"phonemes\": phonemes}\n        writer.write(cut, flush=True)\n        unique_phonemes.update(list(phonemes))\nprint(unique_phonemes, len(unique_phonemes))\nprint(cuts[0])\nprocessed_cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz\")\nprint(processed_cuts[0])\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n{'ɑ', 'p', 'æ', 'ʌ', 'ɾ', 'm', 'l', 'b', 'f', 'ɡ', 'ɜ', 'ð', 'i', 'k', 'a', 'e', 't', 'o', 'd', 'ŋ', 'ː', 'ʃ', 'ɪ', ' ', 'ɚ', 'ɐ', ',', 'ɔ', 'v', 's', 'n', '.', 'ɹ', 'ʊ', 'z', 'ɛ', 'w', 'ᵻ', 'ə'} 39\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})"
  },
  {
    "objectID": "text.tokenizers.html#tokenizer",
    "href": "text.tokenizers.html#tokenizer",
    "title": "Text Tokenizers",
    "section": "Tokenizer",
    "text": "Tokenizer\nRequires download of spacy specific language e.g. python -m spacy download en\n\nsource\n\nTokenizer\n\n Tokenizer (backend='spacy', language='en')\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#usage-1",
    "href": "text.tokenizers.html#usage-1",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\ntok = Tokenizer()\n\n\n# str -> List[str]\ns = \"Oh, yeah I don't know dude...\"\ntokenized = tok(s)\nprint(s)\nprint(tokenized)\nprint(tok.inverse(tokenized))\n\n# List[str]->List[List[str]]\ns = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\ntokenized = tok(s)\nprint(tokenized)\nprint(tok.inverse(tokenized))\n\n# Iterable -> Iterable\nds = AG_NEWS(split='test') # data pipe\nsample = next(iter(ds)) # (label, text)\n# print(sample)\nit = tok(ds)\ntokens = [token for token in it]\nprint(tokens[:2])\n\nOh, yeah I don't know dude...\n['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...']\nOh , yeah I do n't know dude ...\n[['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...'], ['this', 'is', 'a', 'test']]\n[\"Oh , yeah I do n't know dude ...\", 'this is a test']\n[['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['The', 'Race', 'is', 'On', ':', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'TORONTO', ',', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', ' ', '#', '36;10', 'million', 'Ansari', 'X', 'Prize', ',', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight', ',', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket', '.']]"
  },
  {
    "objectID": "text.tokenizers.html#numericalizer",
    "href": "text.tokenizers.html#numericalizer",
    "title": "Text Tokenizers",
    "section": "Numericalizer",
    "text": "Numericalizer\n\nsource\n\nNumericalizer\n\n Numericalizer (tokens_iter:Iterable, specials=['<pad>', '<unk>', '<bos>',\n                '<eos>'])\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "text.tokenizers.html#usage-2",
    "href": "text.tokenizers.html#usage-2",
    "title": "Text Tokenizers",
    "section": "Usage",
    "text": "Usage\n\ntok = Tokenizer()\n# In the case of agnews, dataset is: [(index, text)]\ndef token_iterator(data_iter:Iterable)->Iterable:\n    for _, text in data_iter:\n        yield tok(text)\ntok_it= token_iterator(ds)\n# initialize numericalizer based on token iterator\nnum = Numericalizer(tok_it)\n\n\nprint(num('<pad>'), num('<unk>'))\n\n0 1\n\n\n\nprint(num.vocab['the'])\nprint(num('the'))\nprint(num(['<bos>', '<pad>', '<unk>', 'a', 'this', 'the', 'lkjsdf']))\nprint(num.inverse(0))\nprint(num.inverse([6,55]))\nprint(num([['<bos>', '<pad>'], ['<unk>', 'a', 'this', 'the', 'lkjsdf']]))\n\n4\n4\n[2, 0, 1, 9, 58, 4, 1]\n<pad>\n['.', 'Monday']\n[[2, 0], [1, 9, 58, 4, 1]]\n\n\n\ntokens = tok([\"here we go. asdflkj\", \"it was time...\"])\nprint(tokens)\nprint([num(tok) for tok in tokens])\nprint(num(tokens))\n\n[['here', 'we', 'go', '.', 'asdflkj'], ['it', 'was', 'time', '...']]\n[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n\n\n\nsource\n\nTextCollater\n\n TextCollater (tokenizer, numericalizer, padding_value:int=0)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntexts = [\"this is it...\", \"this is the second sentence.\"]\nt = tok(texts)\nprint(t)\ntt = num(t)\nprint(tt)\nttt= [torch.Tensor(t) for t in tt]\n[t.shape[0] for t in ttt]\ncollater = TextCollater(tok, num)\nprint(collater.collate_list(texts))\ndl = DataLoader(dataset=ds, batch_size=2, shuffle=True, collate_fn=collater.collate_agnews)\n\n[['this', 'is', 'it', '...'], ['this', 'is', 'the', 'second', 'sentence', '.']]\n[[58, 27, 34, 67], [58, 27, 4, 95, 3714, 6]]\n(tensor([[  58,   27,   34,   67,    0,    0],\n        [  58,   27,    4,   95, 3714,    6]]), tensor([4, 6]))\n\n\n\nb = next(iter(dl))\nprint('batch: ', b)\ntokens, lens = b[0], b[1]\nfor token, len in zip(tokens, lens):\n    print(token[:len].tolist())\n    print(num.inverse(token[:len].tolist()))\n\nbatch:  (tensor([[ 3894,  6448,    13,   532,   179,  9855,  1683,    14,   932,    17,\n          3586,    18,   453,    11,  3894,   371,  2260,     5,     4,   100,\n            16,    19,   171,   284,     8,   312,  1820,     5,  1406,    42,\n            22,   454,   401,  2514,    28,     4,    63,    27,  2297,  3327,\n            21,    41,  2455,  2275,   797,    69],\n        [ 3592,  6422,  3288,     7,  1359,   928,    15,  2846,    17,    36,\n            18,     8,   212,   119,  2290,    71,   157,    15,  3202,  1163,\n          8066,    60,    13,  7023,   971,    20,    15,  1673,    10,  2735,\n          6044,    39,  2951, 10519,   194,     4,    15,   873,  8398,  1748,\n             6,     0,     0,     0,     0,     0]]), tensor([46, 41]))\n[3894, 6448, 13, 532, 179, 9855, 1683, 14, 932, 17, 3586, 18, 453, 11, 3894, 371, 2260, 5, 4, 100, 16, 19, 171, 284, 8, 312, 1820, 5, 1406, 42, 22, 454, 401, 2514, 28, 4, 63, 27, 2297, 3327, 21, 41, 2455, 2275, 797, 69]\n['Vodafone', 'Drops', 'on', 'Report', 'It', 'Supports', 'Bid', 'for', 'Sprint', '(', 'Update2', ')', 'Shares', 'in', 'Vodafone', 'Group', 'Plc', ',', 'the', 'world', '#', '39;s', 'largest', 'mobile', '-', 'phone', 'operator', ',', 'dropped', 'after', 'The', 'Wall', 'Street', 'Journal', 'said', 'the', 'company', 'is', 'considering', 'bidding', 'with', 'US', 'partner', 'Verizon', 'Communications', 'Inc.']\n[3592, 6422, 3288, 7, 1359, 928, 15, 2846, 17, 36, 18, 8, 212, 119, 2290, 71, 157, 15, 3202, 1163, 8066, 60, 13, 7023, 971, 20, 15, 1673, 10, 2735, 6044, 39, 2951, 10519, 194, 4, 15, 873, 8398, 1748, 6]\n['Winter', 'Concerns', 'Push', 'to', 'Record', 'High', ' ', 'SINGAPORE', '(', 'Reuters', ')', '-', 'Oil', 'prices', 'broke', 'into', 'record', ' ', 'territory', 'above', '\\\\$52', 'Thursday', 'on', 'heightened', 'concerns', 'that', ' ', 'supplies', 'of', 'heating', 'fuels', 'will', 'prove', 'inadequate', 'during', 'the', ' ', 'northern', 'hemisphere', 'winter', '.']"
  },
  {
    "objectID": "models.autoencoders.html",
    "href": "models.autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "source\n\nAutoEncoder\n\n AutoEncoder (encoder:nimrod.modules.Encoder,\n              decoder:nimrod.modules.Decoder)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nType\nDetails\n\n\n\n\nencoder\nEncoder\nEncoder layer\n\n\ndecoder\nDecoder\nDecoder layer\n\n\n\n\nenc = Encoder()\ndec = Decoder()\na = AutoEncoder(enc, dec)\nbatch = torch.rand((10, 28*28))\ny = a(batch)\nprint(y.shape)\n\ntorch.Size([10, 784])\n\n\n\nds = MNISTDataset()\ndl = DataLoader(ds)\nb = next(iter(dl))\nprint(len(b), b[0].shape, b[1].shape)\n\n2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n\n\n\nsource\n\n\nAutoEncoderPL\n\n AutoEncoderPL (autoencoder:__main__.AutoEncoder)\n\nHooks to be used in LightningModule.\n\nautoencoder_pl = AutoEncoderPL(a)\nb = torch.rand((5,28*28))\ny = autoencoder_pl(b)\nprint(y.shape)\n\ntorch.Size([5, 784])"
  },
  {
    "objectID": "image.datasets.html",
    "href": "image.datasets.html",
    "title": "Image Datasets",
    "section": "",
    "text": "source\n\n\n\n ImageDataset ()\n\nBase class for image datasets providing visualization of (image, label) samples"
  },
  {
    "objectID": "image.datasets.html#mnist",
    "href": "image.datasets.html#mnist",
    "title": "Image Datasets",
    "section": "MNIST",
    "text": "MNIST\n\nMNIST dataset\n\nsource\n\n\nMNISTDataset\n\n MNISTDataset (data_dir:str='~/Data', train=True, transform:<module'torchv\n               ision.transforms.transforms'from'/home/syl20/anaconda3/envs\n               /nimrod/lib/python3.9/site-\n               packages/torchvision/transforms/transforms.py'>=ToTensor())\n\nMNIST digit dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_dir\nstr\n~/Data\npath where data is saved\n\n\ntrain\nbool\nTrue\ntrain or test dataset\n\n\ntransform\ntorchvision.transforms.transforms\nToTensor()\ndata formatting\n\n\n\n\n\nUsage\n\nds = MNISTDataset('~/Data', train=False)\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX, y = ds[0]\nprint(X.shape, y, X.type())\nds.show_idx(0)\ntrain, dev = ds.train_dev_split(0.8)\n\nNumber of samples in the dataset: 10000\ntorch.Size([1, 28, 28]) 7 torch.FloatTensor\n\n\n\n\n\n\n\nMNIST DataModule\n\nsource\n\n\nMNISTDataModule\n\n MNISTDataModule (data_dir:str='~/Data/',\n                  train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n                  batch_size:int=64, num_workers:int=0,\n                  pin_memory:bool=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\nUsage\n\ndm = MNISTDataModule(\n    data_dir=\"~/Data/\",train_val_test_split=[0.8, 0.1, 0.1],\n    batch_size = 64,\n    num_workers = 0,\n    pin_memory= False\n)\ndm.prepare_data()\ndm.setup()\ntest_dl = dm.test_dataloader()\nlen(dm.data_test[0])\nimgs = [dm.data_test[i][0] for i in range(5)]\nImageDataset.show_grid(imgs)"
  },
  {
    "objectID": "audio.datasets.tts.html",
    "href": "audio.datasets.tts.html",
    "title": "Audio TTS Datasets",
    "section": "",
    "text": "https://github.com/Lightning-AI/lightning/issues/10358 https://colab.research.google.com/drive/1HKSYPsWx_HoCdrnLpaPdYj5zwlPsM3NH\n\nsource\n\n\n\n\n LhotseTTSDataset (tokenizer=<class\n                   'lhotse.dataset.collation.TokenCollater'>, extractor=<l\n                   hotse.dataset.input_strategies.OnTheFlyFeatures object\n                   at 0x7fd2a1a71b80>)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\ntype\nTokenCollater\ntext tokenizer\n\n\nextractor\nOnTheFlyFeatures\n<lhotse.dataset.input_strategies.OnTheFlyFeatures object at 0x7fd2a1a71b80>\nfeature extractor\n\n\n\n\n# tok = TokenCollater()\n# ds = LhotseTTSDataset(tok)\n\nTypeError: __init__() missing 1 required positional argument: 'cuts'\n\n\n\n\n\n\nsource\n\n\n\n\n TTSDataset (tokenizer, num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\n\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins\n\n\n\n\n\n\n\n#(Waveform, Sample_rate, Original_text, Normalized_text, Speaker_ID, Chapter_ID, Utterance_ID)\nds = LIBRITTS(\"../data/en\", 'test-clean')\nprint(ds[0])\n\n(tensor([[0.0007, 0.0008, 0.0012,  ..., 0.0039, 0.0042, 0.0042]]), 24000, 'He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour-fattened sauce. Stuff it into you, his belly counselled him.', 'He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce. Stuff it into you, his belly counselled him.', 1089, 134686, '1089_134686_000001_000001')\n\n\n7\n\n\n\nsource\n\n\n\n\n LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n                     clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n                     mrod/recipes/libritts/data', num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en/libriTTS\nwhere data will be saved / retrieved\n\n\ndataset_parts\nlist\n[‘dev-clean’, ‘test-clean’]\neither full libritts or subset\n\n\noutput_dir\nstr\n/home/syl20/slg/nimrod/recipes/libritts/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available\n\n\n\n\n\n\n\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 95it [00:00, 5596.18it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<00:00, 46.46it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nPreparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 53.83it/s]"
  },
  {
    "objectID": "data.utils.lhotse.html",
    "href": "data.utils.lhotse.html",
    "title": "Prepare Datasets with Lhotse",
    "section": "",
    "text": "# from nbdev.showdoc import *"
  },
  {
    "objectID": "data.utils.lhotse.html#recordings-and-supervision-sets-from-file",
    "href": "data.utils.lhotse.html#recordings-and-supervision-sets-from-file",
    "title": "Prepare Datasets with Lhotse",
    "section": "recordings and supervision sets from file",
    "text": "recordings and supervision sets from file\n\n# recording & supervision set\nrecordings = CutSet.from_file('../recipes/tts/ljspeech/data/ljspeech_recordings_all.jsonl.gz')\nsupervisions = CutSet.from_file('../recipes/tts/ljspeech/data/ljspeech_supervisions_all.jsonl.gz')\n\n\ncut_set = CutSet.from_manifests(recordings=recordings, supervisions=supervisions)\nprint(cut_set[0])\nprint(len(cut_set))\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n13100\n\n\n\ncut_set = CutSet.from_manifests(\n                recordings=ljspeech[\"recordings\"],\n                supervisions=ljspeech[\"supervisions\"],\n            )\n# same as\ncut_set = CutSet.from_manifests(**ljspeech)\n\n\nmini = cut_set.subset(first=3)\nfor k, v in mini.data.items():\n    print(f\"key:{k}, value: {v}\")\n\nkey:LJ001-0001-0, value: MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\nkey:LJ001-0002-1, value: MonoCut(id='LJ001-0002-1', start=0, duration=1.899546485260771, channel=0, supervisions=[SupervisionSegment(id='LJ001-0002', recording_id='LJ001-0002', start=0.0, duration=1.899546485260771, channel=0, text='in being comparatively modern.', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0002', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0002.wav')], sampling_rate=22050, num_samples=41885, duration=1.899546485260771, channel_ids=[0], transforms=None), custom=None)\nkey:LJ001-0003-2, value: MonoCut(id='LJ001-0003-2', start=0, duration=9.666621315192744, channel=0, supervisions=[SupervisionSegment(id='LJ001-0003', recording_id='LJ001-0003', start=0.0, duration=9.666621315192744, channel=0, text='For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0003', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav')], sampling_rate=22050, num_samples=213149, duration=9.666621315192744, channel_ids=[0], transforms=None), custom=None)\n\n\n\nrec0 = recordings[0]\nprint(rec0)\n\nRecording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None)\n\n\n\ncuts = CutSet.from_manifests(**ljspeech)\ncuts_trimmed = cuts.trim_to_supervisions()\nmini_cut = cuts.subset(first=3)\nmini_cut.to_file('../data/en/LJSpeech-1.1/first_3.jsonl.gz')\nprint(mini_cut)\n\nCutSet(len=3) [underlying data type: <class 'dict'>]"
  },
  {
    "objectID": "data.utils.lhotse.html#feature-extraction",
    "href": "data.utils.lhotse.html#feature-extraction",
    "title": "Prepare Datasets with Lhotse",
    "section": "feature extraction",
    "text": "feature extraction\n\ntorch.set_num_threads(1)\ntorch.set_num_interop_threads(1)\n\n: \n\n\n: \n\n\n\nconf = FbankConfig(sampling_rate=22050, frame_shift=0.02)\nwith torch.no_grad():\n    cuts = mini_cut.compute_and_store_features(\n        extractor=Fbank(conf),\n        storage_path='../recipes/tts/ljspeech/data/feats',\n        num_jobs=100\n    )\ncuts.to_file(\"../recipes/tts/ljspeech/data/ljspeech_fbank_1000.jsonl.gz\")"
  },
  {
    "objectID": "data.utils.lhotse.html#supervision-segment-features-recording",
    "href": "data.utils.lhotse.html#supervision-segment-features-recording",
    "title": "Prepare Datasets with Lhotse",
    "section": "Supervision segment + Features + Recording",
    "text": "Supervision segment + Features + Recording\n\nfeats = CutSet.from_file(\"../recipes/tts/ljspeech/data/ljspeech_fbank_1000.jsonl.gz\")\nprint(feats[0])\nfeats[0].plot_features()\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=483, num_features=80, frame_shift=0.02, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../recipes/tts/ljspeech/data/feats/feats-0.lca', storage_key='0,44166', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\n# mvn_stats = cuts.compute_global_feature_stats('../recipes/tts/ljspeech/data/feats/mvn_stats.lca', max_cuts=100)\n\n\n# feats[0].supervisions[0].custom[\"tokens\"] = \"tutu\"\n\nTypeError: 'NoneType' object does not support item assignment\n\n\n\nfeats[0]\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=483, num_features=80, frame_shift=0.02, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../recipes/tts/ljspeech/data/feats/feats-0.lca', storage_key='0,44166', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n\n\n\nwith CutSet.open_writer('../recipes/tts/ljspeech/data/ljspeech_fbank_1000_ali.jsonl.gz', overwrite=False) as writer:\n    for cut in feats:\n        for idx, subcut in enumerate(cut.supervisions):\n            subcut.alignment = {\"word\": 0}\n            subcut.custom = {\"phoneme\": 'bloo'}\n            print(subcut)\n        writer.write(cut, flush=True)\n\nSupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom={'phoneme': 'bloo'}, alignment={'word': 0})\nSupervisionSegment(id='LJ001-0002', recording_id='LJ001-0002', start=0.0, duration=1.899546485260771, channel=0, text='in being comparatively modern.', language='English', speaker=None, gender='female', custom={'phoneme': 'bloo'}, alignment={'word': 0})\nSupervisionSegment(id='LJ001-0003', recording_id='LJ001-0003', start=0.0, duration=9.666621315192744, channel=0, text='For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process', language='English', speaker=None, gender='female', custom={'phoneme': 'bloo'}, alignment={'word': 0})\n\n\n\n# feats = CutSet.from_file(\"../recipes/tts/ljspeech/data/ljspeech_fbank_1000_ali.jsonl.gz\")\n# print(feats[0])\n\n\nfrom tqdm.notebook import tqdm\ncuts[0]\nfor c in tqdm(cuts):\n    text = c.supervisions[0].text\n    new_text = text.replace(\"”\", '\"').replace(\"“\", '\"').upper()\n    # c.supervisions[0].tokens = \"tutu\"\n    # text = text.replace(\"”\", '\"').replace(\"“\", '\"')\n    # phonemes = tokenize_text(text_tokenizer, text=text)\n    print(c)\n\n\n\n\nMonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=483, num_features=80, frame_shift=0.02, sampling_rate=22050, start=0, duration=9.65501134, storage_type='lilcom_chunky', storage_path='../recipes/tts/ljspeech/data/feats/feats-0.lca', storage_key='0,44166', recording_id='None', channels=0), recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\nMonoCut(id='LJ001-0002-1', start=0, duration=1.899546485260771, channel=0, supervisions=[SupervisionSegment(id='LJ001-0002', recording_id='LJ001-0002', start=0.0, duration=1.899546485260771, channel=0, text='in being comparatively modern.', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=95, num_features=80, frame_shift=0.02, sampling_rate=22050, start=0, duration=1.89954649, storage_type='lilcom_chunky', storage_path='../recipes/tts/ljspeech/data/feats/feats-1.lca', storage_key='0,8710', recording_id='None', channels=0), recording=Recording(id='LJ001-0002', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0002.wav')], sampling_rate=22050, num_samples=41885, duration=1.899546485260771, channel_ids=[0], transforms=None), custom=None)\nMonoCut(id='LJ001-0003-2', start=0, duration=9.666621315192744, channel=0, supervisions=[SupervisionSegment(id='LJ001-0003', recording_id='LJ001-0003', start=0.0, duration=9.666621315192744, channel=0, text='For although the Chinese took impressions from wood blocks engraved in relief for centuries before the woodcutters of the Netherlands, by a similar process', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=483, num_features=80, frame_shift=0.02, sampling_rate=22050, start=0, duration=9.66662132, storage_type='lilcom_chunky', storage_path='../recipes/tts/ljspeech/data/feats/feats-2.lca', storage_key='0,43976', recording_id='None', channels=0), recording=Recording(id='LJ001-0003', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0003.wav')], sampling_rate=22050, num_samples=213149, duration=9.666621315192744, channel_ids=[0], transforms=None), custom=None)\n\n\n\ncut1 = MonoCut(\"c1\", start=0, duration=4.9, channel=0, recording=\"tutu.wav\")\nali1 = np.random.randint(500, size=(121,))\nwith TemporaryDirectory() as d, NumpyFilesWriter(d) as writer:\n    cut1.label_alignment = writer.store_array(\"c1\", ali1, frame_shift=0.04, temporal_dim=0)\n    cut1.toto = writer.store_array(\"c1\", ali1, frame_shift=0.04, temporal_dim=0)\n\nprint(cut1)\n\nMonoCut(id='c1', start=0, duration=4.9, channel=0, supervisions=[], features=None, recording='tutu.wav', custom={'label_alignment': TemporalArray(array=Array(storage_type='numpy_files', storage_path='/tmp/tmpp25xq2zp', storage_key='c1/c1.npy', shape=[121]), temporal_dim=0, frame_shift=0.04, start=0), 'toto': TemporalArray(array=Array(storage_type='numpy_files', storage_path='/tmp/tmpp25xq2zp', storage_key='c1/c1.npy', shape=[121]), temporal_dim=0, frame_shift=0.04, start=0)})"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "nimrod",
    "section": "Description",
    "text": "Description\nThis is a repo with minimal tooling, modules, models and recipes to get easily get started with deep learning training and experimentation"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nimrod",
    "section": "Install",
    "text": "Install\npip install nimrod"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "nimrod",
    "section": "Usage",
    "text": "Usage\nCheck recipes in recipes/ folder. For instance:\ncd recipes/autoencoder/\npython train.py"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "nimrod",
    "section": "Authors",
    "text": "Authors\n2023 Sylvain Le Groux slegroux@ccrma.stanford.edu"
  }
]