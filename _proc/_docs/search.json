[
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "nimrod",
    "section": "Description",
    "text": "Description\nThis is a repo with minimal tooling, modules, models and recipes to get easily get started with deep learning training and experimentation"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "nimrod",
    "section": "Install",
    "text": "Install\npip install nimrod"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "nimrod",
    "section": "Usage",
    "text": "Usage\nCheck recipes in recipes/ folder. For instance:\ncd recipes/autoencoder/\npython train.py"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "nimrod",
    "section": "Authors",
    "text": "Authors\n2023 Sylvain Le Groux slegroux@ccrma.stanford.edu"
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "source\n\nDecoder\n\n Decoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nEncoder\n\n Encoder ()\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\ntorch.Size([10, 3])\n\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)\n\ntorch.Size([10, 784])"
  },
  {
    "objectID": "audio.embeddings.html",
    "href": "audio.embeddings.html",
    "title": "Audio Embedders",
    "section": "",
    "text": "model = EncodecModel.encodec_model_24khz()\nmodel.set_target_bandwidth(6.0)\nnew_sr = 24000\n\n\n# Load and pre-process the audio waveform\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nwav = convert_audio(wav, sr, model.sample_rate, model.channels)\nwav = wav.unsqueeze(0)\n\n\nplot_waveform(wav.squeeze(0), new_sr)\n\n\n\n\n\nipd.Audio(wav.squeeze(0).numpy(), rate=new_sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Extract discrete codes from EnCodec\nwith torch.no_grad():\n    encoded_frames = model.encode(wav)\ncodes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n\n1 torch.Size([1, 8, 480])\n\n\n\nprint(encoded_frames[0][0].shape)\nprint(codes.shape)\n\ntorch.Size([1, 8, 480])\ntorch.Size([1, 8, 480])\n\n\n\nplt.rcParams[\"figure.figsize\"] = (10,10)\nplt.imshow(codes.squeeze().numpy())\n\n<matplotlib.image.AxesImage>\n\n\n\n\n\n\ndecoded = model.decode(encoded_frames=encoded_frames)\ndecoded.shape\n\ntorch.Size([1, 1, 153600])\n\n\n\nplot_waveform(decoded.detach().squeeze(0), new_sr)\n\n\n\n\n\nipd.Audio(decoded.detach().squeeze(0).numpy(), rate=new_sr)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "audio.utils.html",
    "href": "audio.utils.html",
    "title": "Audio Utilities",
    "section": "",
    "text": "source\n\nplot_waveform\n\n plot_waveform (waveform, sample_rate, title='Waveform', xlim=None,\n                ylim=None)\n\n\nwav, sr = torchaudio.load(\"../data/obama.wav\")\nplot_waveform(wav,sr)"
  },
  {
    "objectID": "models.autoencoders.html",
    "href": "models.autoencoders.html",
    "title": "Autoencoders",
    "section": "",
    "text": "source\n\nAutoEncoder\n\n AutoEncoder (encoder:nimrod.modules.Encoder,\n              decoder:nimrod.modules.Decoder)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nType\nDetails\n\n\n\n\nencoder\nEncoder\nEncoder layer\n\n\ndecoder\nDecoder\nDecoder layer\n\n\n\n\nenc = Encoder()\ndec = Decoder()\na = AutoEncoder(enc, dec)\nbatch = torch.rand((10, 28*28))\ny = a(batch)\nprint(y.shape)\n\ntorch.Size([10, 784])\n\n\n\nds = MNISTDataset()\ndl = DataLoader(ds)\nb = next(iter(dl))\nprint(len(b), b[0].shape, b[1].shape)\n\n2 torch.Size([1, 1, 28, 28]) torch.Size([1])\n\n\n\nsource\n\n\nAutoEncoderPL\n\n AutoEncoderPL (autoencoder:__main__.AutoEncoder)\n\nHooks to be used in LightningModule.\n\nautoencoder_pl = AutoEncoderPL(a)\nb = torch.rand((5,28*28))\ny = autoencoder_pl(b)\nprint(y.shape)\n\ntorch.Size([5, 784])"
  },
  {
    "objectID": "data.datasets.html",
    "href": "data.datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "source\n\n\n\n\n ImageDataset ()\n\nBase class for image datasets providing visualization of (image, label) samples\n\n\n\n\n\n\nsource\n\n\n\n\n\n MNISTDataset (data_dir:str='~/Data', train=True, transform:<module'torchv\n               ision.transforms.transforms'from'/home/syl20/anaconda3/envs\n               /nimrod/lib/python3.9/site-\n               packages/torchvision/transforms/transforms.py'>=ToTensor())\n\nMNIST digit dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_dir\nstr\n~/Data\npath where data is saved\n\n\ntrain\nbool\nTrue\ntrain or test dataset\n\n\ntransform\ntorchvision.transforms.transforms\nToTensor()\ndata formatting\n\n\n\n\nds = MNISTDataset('~/Data', train=False)\nprint(f\"Number of samples in the dataset: {len(ds)}\")\nX, y = ds[0]\nprint(X.shape, y, X.type())\nds.show_idx(0)\ntrain, dev = ds.train_dev_split(0.8)\n\nNumber of samples in the dataset: 10000\ntorch.Size([1, 28, 28]) 7 torch.FloatTensor\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n MNISTDataModule (data_dir:str='~/Data/',\n                  train_val_test_split:List[float]=[0.8, 0.1, 0.1],\n                  batch_size:int=64, num_workers:int=0,\n                  pin_memory:bool=False)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\ndm = MNISTDataModule(\n    data_dir=\"~/Data/\",train_val_test_split=[0.8, 0.1, 0.1],\n    batch_size = 64,\n    num_workers = 0,\n    pin_memory= False\n)\ndm.prepare_data()\ndm.setup()\ntest_dl = dm.test_dataloader()\nlen(dm.data_test[0])\nimgs = [dm.data_test[i][0] for i in range(5)]\nImageDataset.show_grid(imgs)"
  },
  {
    "objectID": "data.datasets.html#audio",
    "href": "data.datasets.html#audio",
    "title": "Datasets",
    "section": "Audio",
    "text": "Audio\n\nSpeech-To-Text\n\nBase class\n\nsource\n\n\n\nSTTDataset\n\n STTDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins\n\n\n\n\nLibriSpeech DataModule\n\nsource\n\n\n\nLibriSpeechDataModule\n\n LibriSpeechDataModule (target_dir='/data/en',\n                        dataset_parts='mini_librispeech',\n                        output_dir='../recipes/stt/librispeech/data',\n                        num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en\nwhere data will be saved / retrieved\n\n\ndataset_parts\nstr\nmini_librispeech\neither full librispeech or mini subset\n\n\noutput_dir\nstr\n../recipes/stt/librispeech/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available\n\n\n\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech/dev-clean-2\",\n    num_jobs=1\n)\n\n\n# skip this at export time to not waste time\n# download\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nDataset parts: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n\n\n\nrecs = RecordingSet.from_file(\"../data/en/LibriSpeech/dev-clean-2/librispeech_recordings_dev-clean-2.jsonl.gz\")\nlen(recs)\n\n25\n\n\n\nsup = SupervisionSet(\"../data/en/LibriSpeech/dev-clean-2/librispeech_supervisions_dev-clean-2.jsonl.gz\")\nlen(sup)\n\n80\n\n\n\ntest_dl = dm.test_dataloader()\nb = next(iter(test_dl))\nprint(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\n# plt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n\n# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])\n\ntorch.Size([1, 1113, 80]) torch.Size([1, 163]) torch.Size([1])\n\n\n\ndm.cuts_test\n\nCutSet(len=25) [underlying data type: <class 'dict'>]\n\n\n\ncut = dm.cuts_test[0]\n# pprint(cut.to_dict())\ncut.plot_audio()\n\n<AxesSubplot: >\n\n\n\n\n\n\n\nText-To-Speech\n\nBase Class\n\nsource\n\n\n\nTTSDataset\n\n TTSDataset (tokenizer:lhotse.dataset.collation.TokenCollater,\n             num_mel_bins:int=80)\n\nAn abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntokenizer\nTokenCollater\n\ntext tokenizer\n\n\nnum_mel_bins\nint\n80\nnumber of mel spectrogram bins\n\n\n\n\nLJSpeech DataModule\n\nsource\n\n\n\nLJSpeechDataModule\n\n LJSpeechDataModule (target_dir='/data/en',\n                     dataset_parts='mini_librispeech',\n                     output_dir='../recipes/tts/ljspeech/data')\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en\nwhere data will be saved / retrieved\n\n\ndataset_parts\nstr\nmini_librispeech\neither full librispeech or mini subset\n\n\noutput_dir\nstr\n../recipes/tts/ljspeech/data\nwhere to save manifest\n\n\n\n\nLibriTTS DataModule\n\nsource\n\n\n\nLibriTTSDataModule\n\n LibriTTSDataModule (target_dir='/data/en/libriTTS', dataset_parts=['dev-\n                     clean', 'test-clean'], output_dir='/home/syl20/slg/ni\n                     mrod/recipes/libritts/data', num_jobs=1)\n\nA DataModule standardizes the training, val, test splits, data preparation and transforms. The main advantage is consistent data splits, data preparation and transforms across models.\nExample::\nclass MyDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n    def prepare_data(self):\n        # download, split, etc...\n        # only called on 1 GPU/TPU in distributed\n    def setup(self, stage):\n        # make assignments here (val/train/test split)\n        # called on every process in DDP\n    def train_dataloader(self):\n        train_split = Dataset(...)\n        return DataLoader(train_split)\n    def val_dataloader(self):\n        val_split = Dataset(...)\n        return DataLoader(val_split)\n    def test_dataloader(self):\n        test_split = Dataset(...)\n        return DataLoader(test_split)\n    def teardown(self):\n        # clean up after fit or test\n        # called on every process in DDP\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_dir\nstr\n/data/en/libriTTS\nwhere data will be saved / retrieved\n\n\ndataset_parts\nlist\n[‘dev-clean’, ‘test-clean’]\neither full libritts or subset\n\n\noutput_dir\nstr\n/home/syl20/slg/nimrod/recipes/libritts/data\nwhere to save manifest\n\n\nnum_jobs\nint\n1\nnum_jobs depending on number of cpus available\n\n\n\n\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\ndm.setup(stage='test')\n\nScanning audio files (*.wav): 0it [00:00, ?it/s]00:00<?, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 95it [00:00, 10200.68it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nScanning audio files (*.wav): 0it [00:00, ?it/s]\nPreparing LibriTTS parts: 100%|██████████| 7/7 [00:00<00:00, 71.02it/s]"
  },
  {
    "objectID": "data.datasets.html#test-lhotse-stt-tts",
    "href": "data.datasets.html#test-lhotse-stt-tts",
    "title": "Datasets",
    "section": "Test Lhotse STT & TTS",
    "text": "Test Lhotse STT & TTS\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech/dev-clean-2\",\n    num_jobs=1\n)\n\n\ndm.setup(stage='test')\nprint(dm.libri)\n\nDataset parts: 100%|██████████| 1/1 [00:00<00:00, 34.49it/s]\n\n\n{'dev-clean-2': {'recordings': RecordingSet(len=25), 'supervisions': SupervisionSet(len=25)}}\n\n\n\n\n\n\nset_dev = CutSet.from_manifests(**dm.libri[\"dev-clean-2\"])\n\n\nprint(set_dev[0])\n\nMonoCut(id='1272-135031-0000-0', start=0, duration=10.885, channel=0, supervisions=[SupervisionSegment(id='1272-135031-0000', recording_id='1272-135031-0000', start=0.0, duration=10.885, channel=0, text='BECAUSE YOU WERE SLEEPING INSTEAD OF CONQUERING THE LOVELY ROSE PRINCESS HAS BECOME A FIDDLE WITHOUT A BOW WHILE POOR SHAGGY SITS THERE A COOING DOVE', language='English', speaker='1272', gender=None, custom=None, alignment=None)], features=None, recording=Recording(id='1272-135031-0000', sources=[AudioSource(type='file', channels=[0], source='../data/en/LibriSpeech/dev-clean-2/1272/135031/1272-135031-0000.flac')], sampling_rate=16000, num_samples=174160, duration=10.885, channel_ids=[0], transforms=None), custom=None)\n\n\n\nset_dev[1].plot_audio()\n\n<AxesSubplot: >\n\n\n\n\n\n\ntorch.set_num_threads(1) #root:num_jobs is > 1 and torch's number of threads is > 1 as well: For certain configs this can result in a never ending computation. If this happens, use torch.set_num_threads(1) to circumvent this.\nextractor = Fbank() # Mfcc()\nset_dev_feat = set_dev.compute_and_store_features(\n    extractor, \"/tmp/feats-train\", num_jobs=110\n)\nprint(set_dev_feat[0])\nset_dev_feat[0].plot_features()\n\nExtracting and storing features (chunks progress): 100%|██████████| 110/110 [00:04<00:00, 22.62it/s]\n\n\nMonoCut(id='1272-135031-0000-0', start=0, duration=10.885, channel=0, supervisions=[SupervisionSegment(id='1272-135031-0000', recording_id='1272-135031-0000', start=0.0, duration=10.885, channel=0, text='BECAUSE YOU WERE SLEEPING INSTEAD OF CONQUERING THE LOVELY ROSE PRINCESS HAS BECOME A FIDDLE WITHOUT A BOW WHILE POOR SHAGGY SITS THERE A COOING DOVE', language='English', speaker='1272', gender=None, custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=1089, num_features=80, frame_shift=0.01, sampling_rate=16000, start=0, duration=10.885, storage_type='lilcom_chunky', storage_path='/tmp/feats-train/feats-0.lca', storage_key='0,45657,45174,7993', recording_id='None', channels=0), recording=Recording(id='1272-135031-0000', sources=[AudioSource(type='file', channels=[0], source='../data/en/LibriSpeech/dev-clean-2/1272/135031/1272-135031-0000.flac')], sampling_rate=16000, num_samples=174160, duration=10.885, channel_ids=[0], transforms=None), custom=None)\n\n\n\n\n\n<matplotlib.image.AxesImage at 0x7ff34b6ca400>\n\n\n\n\n\n\nset_dev[0].play_audio()\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef text_normalizer(segment: SupervisionSegment) -> SupervisionSegment:\n    text = segment.text.upper()\n    text = re.sub(r'[^\\w !?]', '', text)\n    text = re.sub(r'^\\s+', '', text)\n    text = re.sub(r'\\s+$', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.lower()\n    return fastcopy(segment, text=text)\n\n\nset_dev_feat_norm = set_dev_feat.map_supervisions(text_normalizer)\n\nset_dev_feat_norm.to_json(Path('/tmp') / f'cuts.json.gz')\nljspeech_manifests = {}\nljspeech_manifests['cuts'] = set_dev_feat_norm\nset_dev_feat_norm[0]\n\n/home/syl20/anaconda3/envs/nimrod/lib/python3.9/site-packages/lhotse/lazy.py:413: UserWarning: A lambda was passed to LazyMapper: it may prevent you from forking this process. If you experience issues with num_workers > 0 in torch.utils.data.DataLoader, try passing a regular function instead.\n  warnings.warn(\n\n\nMonoCut(id='1272-135031-0000-0', start=0, duration=10.885, channel=0, supervisions=[SupervisionSegment(id='1272-135031-0000', recording_id='1272-135031-0000', start=0.0, duration=10.885, channel=0, text='because you were sleeping instead of conquering the lovely rose princess has become a fiddle without a bow while poor shaggy sits there a cooing dove', language='English', speaker='1272', gender=None, custom=None, alignment=None)], features=Features(type='kaldi-fbank', num_frames=1089, num_features=80, frame_shift=0.01, sampling_rate=16000, start=0, duration=10.885, storage_type='lilcom_chunky', storage_path='/tmp/feats-train/feats-0.lca', storage_key='0,45657,45174,7993', recording_id='None', channels=0), recording=Recording(id='1272-135031-0000', sources=[AudioSource(type='file', channels=[0], source='../data/en/LibriSpeech/dev-clean-2/1272/135031/1272-135031-0000.flac')], sampling_rate=16000, num_samples=174160, duration=10.885, channel_ids=[0], transforms=None), custom=None)\n\n\n\ndataset = SpeechSynthesisDataset(ljspeech_manifests['cuts'])\n\n\nprint(dataset[set_dev_feat_norm]['audio'].shape)\nprint(dataset[set_dev_feat_norm]['features'].shape)\nprint(dataset[set_dev_feat_norm]['tokens'].shape)\nprint(dataset[set_dev_feat_norm]['audio_lens'].shape)\nprint(dataset[set_dev_feat_norm]['features_lens'].shape)\nprint(dataset[set_dev_feat_norm]['tokens_lens'].shape)\n\ntorch.Size([25, 231520])\ntorch.Size([25, 1447, 80])\ntorch.Size([25, 214])\ntorch.Size([25])\ntorch.Size([25])\ntorch.Size([25])\n\n\n\ntrain_sampler = BucketingSampler(\n    set_dev,\n    shuffle=True,\n    max_duration=100.0,\n    num_buckets=10,\n)\n\n\ntrain_dataset = K2SpeechRecognitionDataset(\n    cut_transforms=[\n        PerturbSpeed(factors=[0.9, 1.1], p=2 / 3),\n        PerturbVolume(scale_low=0.125, scale_high=2.0, p=0.5),\n    ],\n    input_transforms=[\n        SpecAugment(),  # default configuration is well-tuned\n    ],\n    input_strategy=OnTheFlyFeatures(Fbank()),\n)\n\n\nds = train_dataset[set_dev]\n\n\nprint(ds['supervisions'].keys())\nprint(ds['inputs'].shape)\n\ndict_keys(['text', 'sequence_idx', 'start_frame', 'num_frames'])\ntorch.Size([25, 1608, 80])\n\n\n\ndl = DataLoader(\n    train_dataset,\n    sampler=train_sampler,\n    batch_size=None,\n    num_workers=110,  # For faster dataloading, use num_workers > 1\n)\n\n\nnext(iter(dl))\n\n{'inputs': tensor([[[-11.1812, -10.9739, -12.5455,  ...,  -5.5773,  -6.4572,  -5.3041],\n          [-10.3855,  -9.8523, -10.5708,  ...,  -6.3980,  -6.1359,  -4.6375],\n          [-10.9408,  -9.8240, -10.3110,  ...,  -6.6965,  -6.0713,  -5.5296],\n          ...,\n          [-10.8166,  -9.3418,  -8.9698,  ...,  -6.3168,  -7.0967,  -7.0158],\n          [-10.2726,  -9.3132, -13.9735,  ...,  -6.7106,  -7.1200,  -7.3640],\n          [-10.9128, -11.3037,  -9.5023,  ...,  -6.9619,  -7.6213,  -7.3471]]]),\n 'supervisions': {'text': ['HE HAS GONE AND GONE FOR GOOD ANSWERED POLYCHROME WHO HAD MANAGED TO SQUEEZE INTO THE ROOM BESIDE THE DRAGON AND HAD WITNESSED THE OCCURRENCES WITH MUCH INTEREST'],\n  'sequence_idx': tensor([0], dtype=torch.int32),\n  'start_frame': tensor([0], dtype=torch.int32),\n  'num_frames': tensor([1012], dtype=torch.int32)}}\n\n\n\nds['inputs'][0].shape\n\ntorch.Size([1608, 80])\n\n\n\nplt.imshow(torch.transpose(ds['inputs'][0],0,1))\n\n<matplotlib.image.AxesImage at 0x7ff34647d670>\n\n\n\n\n\n\nsample = next(iter(dl))\n\n\nsample.keys()\n\ndict_keys(['inputs', 'supervisions'])\n\n\n\nsample['inputs'].shape\n\ntorch.Size([1, 1012, 80])\n\n\n\nplt.imshow(torch.transpose(sample['inputs'].squeeze(0), 0, 1))\n\n<matplotlib.image.AxesImage at 0x7ff346245190>\n\n\n\n\n\n\nsample['supervisions']\n\n{'text': ['HE HAS GONE AND GONE FOR GOOD ANSWERED POLYCHROME WHO HAD MANAGED TO SQUEEZE INTO THE ROOM BESIDE THE DRAGON AND HAD WITNESSED THE OCCURRENCES WITH MUCH INTEREST'],\n 'sequence_idx': tensor([0], dtype=torch.int32),\n 'start_frame': tensor([0], dtype=torch.int32),\n 'num_frames': tensor([1012], dtype=torch.int32)}\n\n\n\nlen(\"HE HAS GONE AND GONE FOR GOOD ANSWERED POLYCHROME WHO HAD MANAGED TO SQUEEZE INTO THE ROOM BESIDE THE DRAGON AND HAD WITNESSED THE OCCURRENCES WITH MUCH INTEREST\")\n\n161"
  }
]