{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: text.tokenizers.html\n",
    "title: Text Tokenizers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Phonemizer\n",
       "\n",
       ">      Phonemizer (separator=<phonemizer.separator.Separator object at\n",
       ">                  0x7fb6d88c8370>, language='en-us', backend='espeak',\n",
       ">                  strip=True, preserve_punctuation=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L16){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Phonemizer\n",
       "\n",
       ">      Phonemizer (separator=<phonemizer.separator.Separator object at\n",
       ">                  0x7fb6d88c8370>, language='en-us', backend='espeak',\n",
       ">                  strip=True, preserve_punctuation=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Phonemizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oʊ dɪɹ! ðɪs sʌk...\n",
      "wiːl biː faɪn!\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = \"Oh Dear! This suck...\\n We'll be fine!\"\n",
    "print(p(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires download of spacy specific language e.g. `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Tokenizer\n",
       "\n",
       ">      Tokenizer (backend='spacy', language='en')\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Tokenizer\n",
       "\n",
       ">      Tokenizer (backend='spacy', language='en')\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Numericalizer\n",
       "\n",
       ">      Numericalizer (tokenizer:__main__.Tokenizer)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L69){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Numericalizer\n",
       "\n",
       ">      Numericalizer (tokenizer:__main__.Tokenizer)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Numericalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# TODO: collate text + add special characters & 0 != unk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syl20/anaconda3/envs/nimrod/lib/python3.9/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.\")\n",
      "['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tokenized = tok(\"Oh, yeah\\n I don't know dude...\")\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "print(sample)\n",
    "tokenized_ds = tok.tokenize_iter(ds)\n",
    "sample = next(iter(tokenized_ds))\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[531, 1037, 307, 3, 0]\n",
      "[7808, 2, 0, 0, 296, 378, 255, 1324, 0, 64]\n"
     ]
    }
   ],
   "source": [
    "num = Numericalizer(tok)\n",
    "mapper = num.build_map_from_iter(ds)\n",
    "print(mapper[\"<unk>\"])\n",
    "print(mapper(tok(\"here we go. asdflkj\")))\n",
    "# text_pipeline = lambda x: voc(tokenizer(x))\n",
    "print(mapper(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "[tensor([ 531, 1037,  307,    3,    0]), tensor([7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64])]\n",
      "tensor([[ 531, 1037,  307,    3,    0,    0,    0,    0,    0,    0],\n",
      "        [7808,    2,    0,    0,  296,  378,  255, 1324,    0,   64]])\n"
     ]
    }
   ],
   "source": [
    "a = mapper(tok(\"here we go. asdflkj\"))\n",
    "print(len(a))\n",
    "b = mapper(tok(\"Oh, yeah\\n I don't know dude...\"))\n",
    "print(len(b))\n",
    "mini_batch = [a, b]\n",
    "x = [torch.LongTensor(x_i) for x_i in mini_batch]\n",
    "print(x)\n",
    "x_padded = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "print(x_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def text_collate(batch):\n",
    "    # batch: [(label, text), ]\n",
    "    # from ipdb import set_trace; set_trace()\n",
    "    texts = [row[1] for row in batch]\n",
    "    tokens = [torch.LongTensor(mapper(tok(row))) for row in texts]\n",
    "    text_lens = [len(token) for token in tokens]\n",
    "    text_pad = pad_sequence(tokens, batch_first=True, padding_value=-1)\n",
    "    return text_pad, text_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset=ds, batch_size=5, shuffle=True, collate_fn=text_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[11991, 29434, 24121, 10187,  2896,   147,     6,   238,    17,    28,\n",
      "            81,  4305,    20,    12,    13, 16249,    13,    76,    27,    26,\n",
      "             1,  2065,  3871,     7, 17562,     2, 11991,  1048,    24,     4,\n",
      "           885,     1, 11025,     7, 10187,    20,     6,  5517,     3,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [23259,    11,     6, 17457,  1195, 12240,     6,  9439, 13543,     1,\n",
      "          2814,  2657,     7,  6523,  4490,    22, 15031,    82,  1383,    21,\n",
      "         21972,    22,     2, 14020, 23182,    11,     1,  4490,   907,     2,\n",
      "            56,    72,  1076,     1,  2196,     5,  2173, 10377,     3,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [ 3891,  1535,   165,    18,   151,  3243,  1111,  3891,    28,  1295,\n",
      "           139,    11,   161,    43,   145,     5,  1143,  4333,    11,  1545,\n",
      "             4,   206,  7221,    50,    23,  1592,   383,   919,     3,  3891,\n",
      "         22706,     3,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1],\n",
      "        [ 9843,   922, 11719,  3058,   464,     5,   274,    14,    32,    15,\n",
      "            32,     5,   922,  2454,  5026,  5014,   214,    34,    47,   703,\n",
      "          1902,     2,     6,    48,     5,   203,   502,     2,     9,  2692,\n",
      "            65,   203,     8,   413,  1651,     4,   644,   687,     8,     1,\n",
      "          5529,     7,     1,  1773,  2040,     5,  1235,  5943,    18,    35,\n",
      "           464,     5,   274,  4055,     7,     1,  2136,  3058,    10,    52,\n",
      "             3],\n",
      "        [19542,  4688,  1316,    50,   661, 19541, 10660,  5047,  4688,  1813,\n",
      "            63,    87,    10,    34,   173,  1315,     4,  2205,    34,  3847,\n",
      "          1308,  1235,     3,  4688,     2,    73,  2182,     1,   173,   311,\n",
      "             7,     1,  3012,   776,  2775,    11,  4983,     9, 14848,     6,\n",
      "         29237,  1717,     4,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "            -1]]), [39, 39, 32, 61, 43])\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(37)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.Tensor([17564, 24659, 29258, 26399,    13, 16230,  1350,  1321,     4,     6,\n",
    "          1649,    20,  1471,     7,   386, 10675,     6,   784,   648,     8,\n",
    "          1734,    58,     1,   351,  4033,     7,   315,   217,  4224,  2494,\n",
    "            13,    16,  2880,     5,   363,   105,     3,    -1,    -1,    -1,\n",
    "            -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
    "            -1,    -1,    -1,    -1,    -1,    -1])!= -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
