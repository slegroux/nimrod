{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: text.tokenizers.html\n",
    "title: Text Tokenizers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes espeak backend is installed via `apt-get install espeak`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L25){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Phonemizer\n",
       "\n",
       ">      Phonemizer (separator=<phonemizer.separator.Separator object at\n",
       ">                  0x7fdf1c873070>, language='en-us', backend='espeak',\n",
       ">                  strip=True, preserve_punctuation=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L25){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Phonemizer\n",
       "\n",
       ">      Phonemizer (separator=<phonemizer.separator.Separator object at\n",
       ">                  0x7fdf1c873070>, language='en-us', backend='espeak',\n",
       ">                  strip=True, preserve_punctuation=True)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Phonemizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oʊ ʃuːt aɪ mɪst maɪ tɹeɪn\n",
      "['oʊ dɪɹ, juːl biː faɪn!', 'ðɪs ɪz ɪt']\n"
     ]
    }
   ],
   "source": [
    "p = Phonemizer()\n",
    "text = \"oh shoot I missed my train\"\n",
    "print(p(text))\n",
    "text = [\"Oh Dear, you'll be fine!\", \"this is it\"]\n",
    "print(p(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lhotse tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom=None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32091f5d2d645ff8e0fdb6af1e70f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ɪ', 'e', 'ɔ', 'ɑ', 'ə', 'ɛ', 'p', 'o', 'ŋ', 'z', 'a', 'ɾ', 'b', 'ð', 'd', 's', 'm', 'v', 'l', 'k', 'ᵻ', 'ʊ', 'i', ' ', 'ɡ', 'ː', 'n', 't', 'w', 'ɜ', 'ɚ', 'ʃ', 'f', 'ɹ', ',', 'ɐ', 'æ', 'ʌ', '.'} 39\n"
     ]
    }
   ],
   "source": [
    "phon = Phonemizer()\n",
    "cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.jsonl.gz\")\n",
    "# cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/ljspeech_fbank_all.jsonl.gz\")\n",
    "unique_phonemes = set()\n",
    "print(cuts[0])\n",
    "with CutSet.open_writer('../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz', overwrite=True) as writer:\n",
    "    for cut in tqdm(cuts):\n",
    "        phonemes = phon(cut.supervisions[0].text, n_jobs=1)\n",
    "        cut.custom = {\"phonemes\": phonemes}\n",
    "        writer.write(cut, flush=True)\n",
    "        unique_phonemes.update(list(phonemes))\n",
    "print(unique_phonemes, len(unique_phonemes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonoCut(id='LJ001-0001-0', start=0, duration=9.65501133786848, channel=0, supervisions=[SupervisionSegment(id='LJ001-0001', recording_id='LJ001-0001', start=0.0, duration=9.65501133786848, channel=0, text='Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', language='English', speaker=None, gender='female', custom=None, alignment=None)], features=None, recording=Recording(id='LJ001-0001', sources=[AudioSource(type='file', channels=[0], source='/data/en/LJSpeech/LJSpeech-1.1/wavs/LJ001-0001.wav')], sampling_rate=22050, num_samples=212893, duration=9.65501133786848, channel_ids=[0], transforms=None), custom={'phonemes': 'pɹɪntɪŋ, ɪnðɪ oʊnli sɛns wɪð wɪtʃ wiː ɑːɹ æt pɹɛzənt kənsɜːnd, dɪfɚz fɹʌm moʊst ɪf nɑːt fɹʌm ɔːl ðɪ ɑːɹts ænd kɹæfts ɹɛpɹᵻzɛntᵻd ɪnðɪ ɛksɪbɪʃən'})\n"
     ]
    }
   ],
   "source": [
    "processed_cuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.phon.jsonl.gz\")\n",
    "print(processed_cuts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ' ', 1: ',', 2: '.', 3: 'a', 4: 'b', 5: 'd', 6: 'e', 7: 'f', 8: 'i', 9: 'k', 10: 'l', 11: 'm', 12: 'n', 13: 'o', 14: 'p', 15: 's', 16: 't', 17: 'v', 18: 'w', 19: 'z', 20: 'æ', 21: 'ð', 22: 'ŋ', 23: 'ɐ', 24: 'ɑ', 25: 'ɔ', 26: 'ə', 27: 'ɚ', 28: 'ɛ', 29: 'ɜ', 30: 'ɡ', 31: 'ɪ', 32: 'ɹ', 33: 'ɾ', 34: 'ʃ', 35: 'ʊ', 36: 'ʌ', 37: 'ː', 38: 'ᵻ'} 39\n"
     ]
    }
   ],
   "source": [
    "map = {}\n",
    "unique_syms = set()\n",
    "for cut in processed_cuts:\n",
    "    unique_syms.update(list(cut.custom['phonemes']))\n",
    "\n",
    "for (i, v) in enumerate(sorted(list(unique_syms))):\n",
    "    map[i] = v\n",
    "print(map, len(map))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires download of spacy specific language e.g. `python -m spacy download en`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Tokenizer\n",
       "\n",
       ">      Tokenizer (backend='spacy', language='en')\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L79){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Tokenizer\n",
       "\n",
       ">      Tokenizer (backend='spacy', language='en')\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, yeah I don't know dude...\n",
      "['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...']\n",
      "Oh , yeah I do n't know dude ...\n",
      "[['Oh', ',', 'yeah', 'I', 'do', \"n't\", 'know', 'dude', '...'], ['this', 'is', 'a', 'test']]\n",
      "[\"Oh , yeah I do n't know dude ...\", 'this is a test']\n",
      "[['Fears', 'for', 'T', 'N', 'pension', 'after', 'talks', 'Unions', 'representing', 'workers', 'at', 'Turner', '  ', 'Newall', 'say', 'they', 'are', \"'\", 'disappointed', \"'\", 'after', 'talks', 'with', 'stricken', 'parent', 'firm', 'Federal', 'Mogul', '.'], ['The', 'Race', 'is', 'On', ':', 'Second', 'Private', 'Team', 'Sets', 'Launch', 'Date', 'for', 'Human', 'Spaceflight', '(', 'SPACE.com', ')', 'SPACE.com', '-', 'TORONTO', ',', 'Canada', '--', 'A', 'second\\\\team', 'of', 'rocketeers', 'competing', 'for', 'the', ' ', '#', '36;10', 'million', 'Ansari', 'X', 'Prize', ',', 'a', 'contest', 'for\\\\privately', 'funded', 'suborbital', 'space', 'flight', ',', 'has', 'officially', 'announced', 'the', 'first\\\\launch', 'date', 'for', 'its', 'manned', 'rocket', '.']]\n"
     ]
    }
   ],
   "source": [
    "# str -> List[str]\n",
    "s = \"Oh, yeah I don't know dude...\"\n",
    "tokenized = tok(s)\n",
    "print(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "\n",
    "# List[str]->List[List[str]]\n",
    "s = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\n",
    "tokenized = tok(s)\n",
    "print(tokenized)\n",
    "print(tok.inverse(tokenized))\n",
    "\n",
    "# Iterable -> Iterable\n",
    "ds = AG_NEWS(split='test') # data pipe\n",
    "sample = next(iter(ds)) # (label, text)\n",
    "# print(sample)\n",
    "it = tok(ds)\n",
    "tokens = [token for token in it]\n",
    "print(tokens[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Numericalizer\n",
       "\n",
       ">      Numericalizer (tokens_iter:Iterable, specials=['<pad>', '<unk>', '<bos>',\n",
       ">                     '<eos>'])\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L113){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Numericalizer\n",
       "\n",
       ">      Numericalizer (tokens_iter:Iterable, specials=['<pad>', '<unk>', '<bos>',\n",
       ">                     '<eos>'])\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Numericalizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "# In the case of agnews, dataset is: [(index, text)]\n",
    "def token_iterator(data_iter:Iterable)->Iterable:\n",
    "    for _, text in data_iter:\n",
    "        yield tok(text)\n",
    "tok_it= token_iterator(ds)\n",
    "# initialize numericalizer based on token iterator\n",
    "num = Numericalizer(tok_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "print(num('<pad>'), num('<unk>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "[2, 0, 1, 9, 58, 4, 1]\n",
      "<pad>\n",
      "['.', 'Monday']\n",
      "[[2, 0], [1, 9, 58, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(num.vocab['the'])\n",
    "print(num('the'))\n",
    "print(num(['<bos>', '<pad>', '<unk>', 'a', 'this', 'the', 'lkjsdf']))\n",
    "print(num.inverse(0))\n",
    "print(num.inverse([6,55]))\n",
    "print(num([['<bos>', '<pad>'], ['<unk>', 'a', 'this', 'the', 'lkjsdf']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['here', 'we', 'go', '.', 'asdflkj'], ['it', 'was', 'time', '...']]\n",
      "[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n",
      "[[534, 1040, 310, 6, 1], [34, 40, 101, 67]]\n"
     ]
    }
   ],
   "source": [
    "tokens = tok([\"here we go. asdflkj\", \"it was time...\"])\n",
    "print(tokens)\n",
    "print([num(tok) for tok in tokens])\n",
    "print(num(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextCollater\n",
       "\n",
       ">      TextCollater (tokenizer, numericalizer, padding_value:int=0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/slegroux/nimrod/blob/main/nimrod/text/tokenizers.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextCollater\n",
       "\n",
       ">      TextCollater (tokenizer, numericalizer, padding_value:int=0)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(TextCollater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'it', '...'], ['this', 'is', 'the', 'second', 'sentence', '.']]\n",
      "[[58, 27, 34, 67], [58, 27, 4, 95, 3714, 6]]\n",
      "(tensor([[  58,   27,   34,   67,    0,    0],\n",
      "        [  58,   27,    4,   95, 3714,    6]]), tensor([4, 6]))\n"
     ]
    }
   ],
   "source": [
    "texts = [\"this is it...\", \"this is the second sentence.\"]\n",
    "t = tok(texts)\n",
    "print(t)\n",
    "tt = num(t)\n",
    "print(tt)\n",
    "ttt= [torch.Tensor(t) for t in tt]\n",
    "[t.shape[0] for t in ttt]\n",
    "collater = TextCollater(tok, num)\n",
    "print(collater.collate_list(texts))\n",
    "dl = DataLoader(dataset=ds, batch_size=2, shuffle=True, collate_fn=collater.collate_agnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  (tensor([[ 3894,  6448,    13,   532,   179,  9855,  1683,    14,   932,    17,\n",
      "          3586,    18,   453,    11,  3894,   371,  2260,     5,     4,   100,\n",
      "            16,    19,   171,   284,     8,   312,  1820,     5,  1406,    42,\n",
      "            22,   454,   401,  2514,    28,     4,    63,    27,  2297,  3327,\n",
      "            21,    41,  2455,  2275,   797,    69],\n",
      "        [ 3592,  6422,  3288,     7,  1359,   928,    15,  2846,    17,    36,\n",
      "            18,     8,   212,   119,  2290,    71,   157,    15,  3202,  1163,\n",
      "          8066,    60,    13,  7023,   971,    20,    15,  1673,    10,  2735,\n",
      "          6044,    39,  2951, 10519,   194,     4,    15,   873,  8398,  1748,\n",
      "             6,     0,     0,     0,     0,     0]]), tensor([46, 41]))\n",
      "[3894, 6448, 13, 532, 179, 9855, 1683, 14, 932, 17, 3586, 18, 453, 11, 3894, 371, 2260, 5, 4, 100, 16, 19, 171, 284, 8, 312, 1820, 5, 1406, 42, 22, 454, 401, 2514, 28, 4, 63, 27, 2297, 3327, 21, 41, 2455, 2275, 797, 69]\n",
      "['Vodafone', 'Drops', 'on', 'Report', 'It', 'Supports', 'Bid', 'for', 'Sprint', '(', 'Update2', ')', 'Shares', 'in', 'Vodafone', 'Group', 'Plc', ',', 'the', 'world', '#', '39;s', 'largest', 'mobile', '-', 'phone', 'operator', ',', 'dropped', 'after', 'The', 'Wall', 'Street', 'Journal', 'said', 'the', 'company', 'is', 'considering', 'bidding', 'with', 'US', 'partner', 'Verizon', 'Communications', 'Inc.']\n",
      "[3592, 6422, 3288, 7, 1359, 928, 15, 2846, 17, 36, 18, 8, 212, 119, 2290, 71, 157, 15, 3202, 1163, 8066, 60, 13, 7023, 971, 20, 15, 1673, 10, 2735, 6044, 39, 2951, 10519, 194, 4, 15, 873, 8398, 1748, 6]\n",
      "['Winter', 'Concerns', 'Push', 'to', 'Record', 'High', ' ', 'SINGAPORE', '(', 'Reuters', ')', '-', 'Oil', 'prices', 'broke', 'into', 'record', ' ', 'territory', 'above', '\\\\$52', 'Thursday', 'on', 'heightened', 'concerns', 'that', ' ', 'supplies', 'of', 'heating', 'fuels', 'will', 'prove', 'inadequate', 'during', 'the', ' ', 'northern', 'hemisphere', 'winter', '.']\n"
     ]
    }
   ],
   "source": [
    "b = next(iter(dl))\n",
    "print('batch: ', b)\n",
    "tokens, lens = b[0], b[1]\n",
    "for token, len in zip(tokens, lens):\n",
    "    print(token[:len].tolist())\n",
    "    print(num.inverse(token[:len].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
