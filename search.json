[
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Nimrod",
    "section": "Description",
    "text": "Description\nThis is a repo with minimal tooling, modules, models and recipes to get easily get started with deep learning training and experimentation with an emphasis on speech, audio and language modeling.",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Nimrod",
    "section": "Install",
    "text": "Install\nyou need python &lt;3.12\n\nInstall using Pip\npip install slg-nimrod",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "Nimrod",
    "section": "Usage",
    "text": "Usage\nCheck recipes in recipes/ folder. E.g. for a simple digit recognizer on MNIST:\ngit clone https://github.com/slegroux/nimrod.git\npython train.py experiment=mnist_mlp data.num_workers=8 trainer.max_epochs=20 \nhead config/train.yaml\nAll the parameters of the experiment are editable and read from a .yaml file which details:\n\ndata and logging directory paths\ndata module with data source path and batching parameters\nmodel architecture\ntrainer with hardware acceleration and number of epochs\ncallbacks for early stopping and automatic logging to Wandb",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#docker",
    "href": "index.html#docker",
    "title": "Nimrod",
    "section": "Docker",
    "text": "Docker\nYou might want to use docker containers for reproductible development environment or run your project in the cloud\nmake container\ndocker pull slegroux/nimrod\ndocker run -it --rm -p 8888:8888 slegroux/nimrod /bin/bash\nYou can also use docker-compose to define services and volumes\ncd .devcontainer\ndocker-compose up\ndocker-compose down",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#develop",
    "href": "index.html#develop",
    "title": "Nimrod",
    "section": "Develop",
    "text": "Develop\npip install -e .",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#hyperparameter-tuning",
    "href": "index.html#hyperparameter-tuning",
    "title": "Nimrod",
    "section": "Hyperparameter tuning",
    "text": "Hyperparameter tuning\nto compare training results on different model parameters:\ncd nimrod/recipes/images/mnist\npython train.py --multirun model.n_h=16,64,256 logger='tensorboard' trainer.max_epochs=5",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#server",
    "href": "index.html#server",
    "title": "Nimrod",
    "section": "Server",
    "text": "Server\n\nst webapp\nRun a simple digit recognizer webapp with GUI\ncd server\n./run_st_app.sh",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "Nimrod",
    "section": "Authors",
    "text": "Authors\n2023 Sylvain Le Groux sylvain.legroux@gmail.com",
    "crumbs": [
      "Nimrod"
    ]
  },
  {
    "objectID": "text.phonemizer.html",
    "href": "text.phonemizer.html",
    "title": "Text phonemizers",
    "section": "",
    "text": "p = Phonemizer()\ntext = \"oh shoot I missed my train\"\nprint(p(text))\ntext = [\"Oh Dear, you'll be fine!\", \"this is it\"]\nprint(p(text))"
  },
  {
    "objectID": "text.phonemizer.html#usage",
    "href": "text.phonemizer.html#usage",
    "title": "Text phonemizers",
    "section": "",
    "text": "p = Phonemizer()\ntext = \"oh shoot I missed my train\"\nprint(p(text))\ntext = [\"Oh Dear, you'll be fine!\", \"this is it\"]\nprint(p(text))"
  },
  {
    "objectID": "models.autoencoders.html",
    "href": "models.autoencoders.html",
    "title": "AutoEncoders",
    "section": "",
    "text": "cfg = OmegaConf.load('../config/data/image/image.yaml')\ndm = instantiate(cfg, name='fashion_mnist', data_dir='../data/image/')\ndm.prepare_data()\ndm.setup()\nprint(dm.num_classes)\n\n[15:41:01] INFO - Init ImageDataModule for fashion_mnist\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406, in hf_raise_for_status(response, endpoint_name)\n    405 try:\n--&gt; 406     response.raise_for_status()\n    407 except HTTPError as e:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/api/datasets/zalando-datasets/fashion_mnist/paths-info/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n\nThe above exception was the direct cause of the following exception:\n\nHfHubHTTPError                            Traceback (most recent call last)\nCell In[5], line 4\n      2 cfg = OmegaConf.load('../config/data/image/image.yaml')\n      3 dm = instantiate(cfg, name='fashion_mnist', data_dir='../data/image/')\n----&gt; 4 dm.prepare_data()\n      5 dm.setup()\n      6 print(dm.num_classes)\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:415, in ImageDataModule.prepare_data(self)\n    412 \"\"\"Download data if needed \n    413 \"\"\"\n    414 # train set\n--&gt; 415 self.train_ds = ImageDataset(\n    416     self.hparams.name,\n    417     *self.args,\n    418     data_dir = self.hparams.data_dir,\n    419     split='train',\n    420     transforms = self.hparams.transforms,\n    421     **self.kwargs\n    422 )\n    423 # get num classes before setup method converst ImageDataset to Subset\n    424 self._num_classes = self.train_ds.num_classes\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:204, in ImageDataset.__init__(self, name, data_dir, split, transforms, streaming, exclude_grey_scale, verification_mode, from_image_folder, from_disk, *args)\n    202 self.exclude_grey_scale = exclude_grey_scale\n    203 if not from_image_folder:\n--&gt; 204     self.info = load_dataset_builder(name, *args)\n    205     if split not in self.info.info.splits:\n    206         raise ValueError(f\"The specified split '{split}' does not exist in the dataset '{name}'. Available splits: {list(self.info.info.splits.keys())}\")\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n   1885 # Instantiate the dataset builder\n-&gt; 1886 builder_instance: DatasetBuilder = builder_cls(\n   1887     cache_dir=cache_dir,\n   1888     dataset_name=dataset_name,\n   1889     config_name=config_name,\n   1890     data_dir=data_dir,\n   1891     data_files=data_files,\n   1892     hash=dataset_module.hash,\n   1893     info=info,\n   1894     features=features,\n   1895     token=token,\n   1896     storage_options=storage_options,\n   1897     **builder_kwargs,\n   1898     **config_kwargs,\n   1899 )\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n   1902 return builder_instance\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:342, in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    340     config_kwargs[\"data_dir\"] = data_dir\n    341 self.config_kwargs = config_kwargs\n--&gt; 342 self.config, self.config_id = self._create_builder_config(\n    343     config_name=config_name,\n    344     custom_features=features,\n    345     **config_kwargs,\n    346 )\n    348 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    349 # Prefill datasetinfo\n    350 if info is None:\n    351     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:597, in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    594     raise ValueError(f\"BuilderConfig must have a name, got {builder_config.name}\")\n    596 # resolve data files if needed\n--&gt; 597 builder_config._resolve_data_files(\n    598     base_path=self.base_path,\n    599     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    600 )\n    602 # compute the config id that is going to be used for caching\n    603 config_id = builder_config.create_config_id(\n    604     config_kwargs,\n    605     custom_features=custom_features,\n    606 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:206, in BuilderConfig._resolve_data_files(self, base_path, download_config)\n    204 if isinstance(self.data_files, DataFilesPatternsDict):\n    205     base_path = xjoin(base_path, self.data_dir) if self.data_dir else base_path\n--&gt; 206     self.data_files = self.data_files.resolve(base_path, download_config)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:818, in DataFilesPatternsDict.resolve(self, base_path, download_config)\n    816 out = DataFilesDict()\n    817 for key, data_files_patterns_list in self.items():\n--&gt; 818     out[key] = data_files_patterns_list.resolve(base_path, download_config)\n    819 return out\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:771, in DataFilesPatternsList.resolve(self, base_path, download_config)\n    768 for pattern, allowed_extensions in zip(self, self.allowed_extensions):\n    769     try:\n    770         data_files.extend(\n--&gt; 771             resolve_pattern(\n    772                 pattern,\n    773                 base_path=base_path,\n    774                 allowed_extensions=allowed_extensions,\n    775                 download_config=download_config,\n    776             )\n    777         )\n    778     except FileNotFoundError:\n    779         if not has_magic(pattern):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:388, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\n    383 if protocol == \"hf\" and config.HF_HUB_VERSION &gt;= version.parse(\"0.20.0\"):\n    384     # 10 times faster glob with detail=True (ignores costly info like lastCommit)\n    385     glob_kwargs[\"expand_info\"] = False\n    386 matched_paths = [\n    387     filepath if filepath.startswith(protocol_prefix) else protocol_prefix + filepath\n--&gt; 388     for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n    389     if info[\"type\"] == \"file\"\n    390     and (xbasename(filepath) not in files_to_ignore)\n    391     and not _is_inside_unrequested_special_dir(filepath, fs_pattern)\n    392     and not _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n    393 ]  # ignore .ipynb and __pycache__, but keep /../\n    394 if allowed_extensions is not None:\n    395     out = [\n    396         filepath\n    397         for filepath in matched_paths\n    398         if any(\".\" + suffix in allowed_extensions for suffix in xbasename(filepath).split(\".\")[1:])\n    399     ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:521, in HfFileSystem.glob(self, path, **kwargs)\n    519 kwargs = {\"expand_info\": kwargs.get(\"detail\", False), **kwargs}\n    520 path = self.resolve_path(path, revision=kwargs.get(\"revision\")).unresolve()\n--&gt; 521 return super().glob(path, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:611, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs)\n    608     else:\n    609         depth = None\n--&gt; 611 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n    613 pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n    614 pattern = re.compile(pattern)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:556, in HfFileSystem.find(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\n    533 \"\"\"\n    534 List all files below path.\n    535 \n   (...)\n    553     `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\n    554 \"\"\"\n    555 if maxdepth:\n--&gt; 556     return super().find(\n    557         path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, refresh=refresh, revision=revision, **kwargs\n    558     )\n    559 resolved_path = self.resolve_path(path, revision=revision)\n    560 path = resolved_path.unresolve()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:502, in AbstractFileSystem.find(self, path, maxdepth, withdirs, detail, **kwargs)\n    499 # Add the root directory if withdirs is requested\n    500 # This is needed for posix glob compliance\n    501 if withdirs and path != \"\" and self.isdir(path):\n--&gt; 502     out[path] = self.info(path)\n    504 for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\n    505     if withdirs:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:719, in HfFileSystem.info(self, path, refresh, revision, **kwargs)\n    717     out = out1[0]\n    718 if refresh or out is None or (expand_info and out and out[\"last_commit\"] is None):\n--&gt; 719     paths_info = self._api.get_paths_info(\n    720         resolved_path.repo_id,\n    721         resolved_path.path_in_repo,\n    722         expand=expand_info,\n    723         revision=resolved_path.revision,\n    724         repo_type=resolved_path.repo_type,\n    725     )\n    726     if not paths_info:\n    727         _raise_file_not_found(path, None)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    111 if check_use_auth_token:\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n--&gt; 114 return fn(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3303, in HfApi.get_paths_info(self, repo_id, paths, expand, revision, repo_type, token)\n   3293 headers = self._build_hf_headers(token=token)\n   3295 response = get_session().post(\n   3296     f\"{self.endpoint}/api/{repo_type}s/{repo_id}/paths-info/{revision}\",\n   3297     data={\n   (...)\n   3301     headers=headers,\n   3302 )\n-&gt; 3303 hf_raise_for_status(response)\n   3304 paths_info = response.json()\n   3305 return [\n   3306     RepoFile(**path_info) if path_info[\"type\"] == \"file\" else RepoFolder(**path_info)\n   3307     for path_info in paths_info\n   3308 ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:477, in hf_raise_for_status(response, endpoint_name)\n    473     raise _format(HfHubHTTPError, message, response) from e\n    475 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\n    476 # as well (request id and/or server error message)\n--&gt; 477 raise _format(HfHubHTTPError, str(e), response) from e\n\nHfHubHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/api/datasets/zalando-datasets/fashion_mnist/paths-info/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n\n\n\n\ndm.show_grid(3,3)\n\n\n\n\n\n\n\n\n\nprint(dm.label_names)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[6], line 2\n      1 #| notest\n----&gt; 2 print(dm.label_names)\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:409, in ImageDataModule.label_names(self)\n    407 if self.train_ds is not None:\n    408     return self._label_names\n--&gt; 409 raise RuntimeError(\"train_ds is not initialized. Call prepare_data() first.\")\n\nRuntimeError: train_ds is not initialized. Call prepare_data() first.\n\n\n\n\n\n\ncfg = OmegaConf.load('../config/model/image/convnetx_adam.yaml')\n# nnet = instantiate(cfg.nnet, num_classes=dm.num_classes)\n# optimizer = instantiate(cfg.optimizer)\n# scheduler = instantiate(cfg.scheduler)\n\n# model = ConvNetX(nnet, dm.num_classes, optimizer, scheduler)\nmodel = instantiate(cfg, num_classes=10)\n\n[15:41:17] INFO - ConvNetX: init\n[15:41:17] INFO - Classifier: init\n\n\n\nMAX_EPOCHS = 5\ndm.batch_size = 256\nprint(dm.batch_size)\n# lr = 0.4\n\ntrainer = Trainer(\n    max_epochs=MAX_EPOCHS,\n    logger=CSVLogger(\"logs\", name=\"fashion_mnist_convnet\"),\n    callbacks = [LearningRateMonitor(logging_interval=\"step\")],\n    check_val_every_n_epoch=1,\n    log_every_n_steps=1\n    )\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n\n256\n\n\n\n\n\n\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(\n    model,\n    datamodule=dm,\n    min_lr=1e-6,\n    max_lr=1.0,\n    num_training=100,  # number of iterations\n    # attr_name=\"optimizer.lr\",\n)\nfig = lr_finder.plot(suggest=True)\nplt.show()\nprint(f\"Suggested learning rate: {lr_finder.suggestion()}\")\n\nYou are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[22:43:36] INFO - loading dataset fashion_mnist with args () from split train\n[22:43:36] INFO - loading dataset fashion_mnist from split train\nOverwrite dataset info from restored data version if exists.\n[22:43:37] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:37] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[22:43:37] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:37] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:41] INFO - loading dataset fashion_mnist with args () from split test\n[22:43:41] INFO - loading dataset fashion_mnist from split test\nOverwrite dataset info from restored data version if exists.\n[22:43:43] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:43] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[22:43:43] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:43] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[22:43:43] INFO - Optimizer: &lt;class 'torch.optim.adam.Adam'&gt;\n[22:43:43] INFO - Scheduler: &lt;class 'torch.optim.lr_scheduler.ReduceLROnPlateau'&gt;\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n\n\n\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 3\n      1 #| notest\n      2 tuner = Tuner(trainer)\n----&gt; 3 lr_finder = tuner.lr_find(\n      4     model,\n      5     datamodule=dm,\n      6     min_lr=1e-6,\n      7     max_lr=1.0,\n      8     num_training=100,  # number of iterations\n      9     # attr_name=\"optimizer.lr\",\n     10 )\n     11 fig = lr_finder.plot(suggest=True)\n     12 plt.show()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:180, in Tuner.lr_find(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\n    177 lr_finder_callback._early_exit = True\n    178 self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\n--&gt; 180 self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\n    182 self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\n    184 return lr_finder_callback.optimal_lr\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    537 self.state.status = TrainerStatus.RUNNING\n    538 self.training = True\n--&gt; 539 call._call_and_handle_interrupt(\n    540     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    541 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     45     if trainer.strategy.launcher is not None:\n     46         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---&gt; 47     return trainer_fn(*args, **kwargs)\n     49 except _TunerExitException:\n     50     _call_teardown_hook(trainer)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    568 assert self.state.fn is not None\n    569 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    570     self.state.fn,\n    571     ckpt_path,\n    572     model_provided=True,\n    573     model_connected=self.lightning_module is not None,\n    574 )\n--&gt; 575 self._run(model, ckpt_path=ckpt_path)\n    577 assert self.state.stopped\n    578 self.training = False\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:962, in Trainer._run(self, model, ckpt_path)\n    960 # hook\n    961 if self.state.fn == TrainerFn.FITTING:\n--&gt; 962     call._call_callback_hooks(self, \"on_fit_start\")\n    963     call._call_lightning_module_hook(self, \"on_fit_start\")\n    965 _log_hyperparams(self)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:222, in _call_callback_hooks(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\n    220     if callable(fn):\n    221         with trainer.profiler.profile(f\"[Callback]{callback.state_key}.{hook_name}\"):\n--&gt; 222             fn(trainer, trainer.lightning_module, *args, **kwargs)\n    224 if pl_module:\n    225     # restore current_fx when nested context\n    226     pl_module._current_fx_name = prev_fx_name\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:130, in LearningRateFinder.on_fit_start(self, trainer, pl_module)\n    128 @override\n    129 def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n--&gt; 130     self.lr_find(trainer, pl_module)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:113, in LearningRateFinder.lr_find(self, trainer, pl_module)\n    111 def lr_find(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n    112     with isolate_rng():\n--&gt; 113         self.optimal_lr = _lr_find(\n    114             trainer,\n    115             pl_module,\n    116             min_lr=self._min_lr,\n    117             max_lr=self._max_lr,\n    118             num_training=self._num_training_steps,\n    119             mode=self._mode,\n    120             early_stop_threshold=self._early_stop_threshold,\n    121             update_attr=self._update_attr,\n    122             attr_name=self._attr_name,\n    123         )\n    125     if self._early_exit:\n    126         raise _TunerExitException()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:278, in _lr_find(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\n    275 lr_finder._exchange_scheduler(trainer)\n    277 # Fit, lr & loss logged in callback\n--&gt; 278 _try_loop_run(trainer, params)\n    280 # Prompt if we stopped early\n    281 if trainer.global_step != num_training + start_steps:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:523, in _try_loop_run(trainer, params)\n    521 loop.load_state_dict(deepcopy(params[\"loop_state_dict\"]))\n    522 loop.restarting = False\n--&gt; 523 loop.run()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216, in _FitLoop.run(self)\n    214 try:\n    215     self.on_advance_start()\n--&gt; 216     self.advance()\n    217     self.on_advance_end()\n    218 except StopIteration:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455, in _FitLoop.advance(self)\n    453 with self.trainer.profiler.profile(\"run_training_epoch\"):\n    454     assert self._data_fetcher is not None\n--&gt; 455     self.epoch_loop.run(self._data_fetcher)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150, in _TrainingEpochLoop.run(self, data_fetcher)\n    148 while not self.done:\n    149     try:\n--&gt; 150         self.advance(data_fetcher)\n    151         self.on_advance_end(data_fetcher)\n    152     except StopIteration:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:322, in _TrainingEpochLoop.advance(self, data_fetcher)\n    320             batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n    321         else:\n--&gt; 322             batch_output = self.manual_optimization.run(kwargs)\n    324 self.batch_progress.increment_processed()\n    326 # update non-plateau LR schedulers\n    327 # update epoch-interval ones only when we are at the end of training epoch\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/manual.py:94, in _ManualOptimization.run(self, kwargs)\n     92 self.on_run_start()\n     93 with suppress(StopIteration):  # no loop to break at this level\n---&gt; 94     self.advance(kwargs)\n     95 self._restarting = False\n     96 return self.on_run_end()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/manual.py:114, in _ManualOptimization.advance(self, kwargs)\n    111 trainer = self.trainer\n    113 # manually capture logged metrics\n--&gt; 114 training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n    115 del kwargs  # release the batch from memory\n    116 self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:323, in _call_strategy_hook(trainer, hook_name, *args, **kwargs)\n    320     return None\n    322 with trainer.profiler.profile(f\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"):\n--&gt; 323     output = fn(*args, **kwargs)\n    325 # restore current_fx when nested context\n    326 pl_module._current_fx_name = prev_fx_name\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391, in Strategy.training_step(self, *args, **kwargs)\n    389 if self.model != self.lightning_module:\n    390     return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n--&gt; 391 return self.lightning_module.training_step(*args, **kwargs)\n\nFile ~/Projects/nimrod/nimrod/models/core.py:186, in Classifier.training_step(self, batch, batch_idx)\n    183     sched.step() #reduce plateau sched is updated at end of epoch only instead TODO: should it be applied to val loop by default?\n    185 self.train_loss(loss)\n--&gt; 186 self.train_acc(preds, y)\n    187 metrics = {\"train/loss\": self.train_loss, \"train/acc\": self.train_acc}\n    188 self.log_dict(metrics, on_epoch=True, on_step=True, prog_bar=True)# Pass the validation loss to the scheduler\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:316, in Metric.forward(self, *args, **kwargs)\n    314     self._forward_cache = self._forward_full_state_update(*args, **kwargs)\n    315 else:\n--&gt; 316     self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)\n    318 return self._forward_cache\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:385, in Metric._forward_reduce_state_update(self, *args, **kwargs)\n    382 self._enable_grad = True  # allow grads for batch computation\n    384 # calculate batch state and compute batch value\n--&gt; 385 self.update(*args, **kwargs)\n    386 batch_val = self.compute()\n    388 # reduce batch and global state\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:560, in Metric._wrap_update.&lt;locals&gt;.wrapped_func(*args, **kwargs)\n    552         if \"Expected all tensors to be on\" in str(err):\n    553             raise RuntimeError(\n    554                 \"Encountered different devices in metric calculation (see stacktrace for details).\"\n    555                 \" This could be due to the metric class not being on the same device as input.\"\n   (...)\n    558                 \" device corresponds to the device of the input.\"\n    559             ) from err\n--&gt; 560         raise err\n    562 if self.compute_on_cpu:\n    563     self._move_list_states_to_cpu()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:550, in Metric._wrap_update.&lt;locals&gt;.wrapped_func(*args, **kwargs)\n    548 with torch.set_grad_enabled(self._enable_grad):\n    549     try:\n--&gt; 550         update(*args, **kwargs)\n    551     except RuntimeError as err:\n    552         if \"Expected all tensors to be on\" in str(err):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:339, in MulticlassStatScores.update(self, preds, target)\n    337 \"\"\"Update state with predictions and targets.\"\"\"\n    338 if self.validate_args:\n--&gt; 339     _multiclass_stat_scores_tensor_validation(\n    340         preds, target, self.num_classes, self.multidim_average, self.ignore_index\n    341     )\n    342 preds, target = _multiclass_stat_scores_format(preds, target, self.top_k)\n    343 tp, fp, tn, fn = _multiclass_stat_scores_update(\n    344     preds, target, self.num_classes, self.top_k, self.average, self.multidim_average, self.ignore_index\n    345 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:318, in _multiclass_stat_scores_tensor_validation(preds, target, num_classes, multidim_average, ignore_index)\n    316 num_unique_values = len(torch.unique(t, dim=None))\n    317 if num_unique_values &gt; check_value:\n--&gt; 318     raise RuntimeError(\n    319         f\"Detected more unique values in `{name}` than expected. Expected only {check_value} but found\"\n    320         f\" {num_unique_values} in `{name}`. Found values: {torch.unique(t, dim=None)}.\"\n    321     )\n\nRuntimeError: Detected more unique values in `preds` than expected. Expected only 10 but found 30 in `preds`. Found values: tensor([ 0,  1,  3,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22,\n        23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39], device='cuda:0').\n\n\n\n\n# print(f\"lr: {model.lr}, bs: {dm.batch_size}\")\n\nlr: 9.120108393559098e-06, bs: 256\n\n\n\n\n\n\ncfg.optimizer.lr = 0.4\nprint(OmegaConf.to_yaml(cfg))\n\nmodel = instantiate(cfg)\ntrainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\n\n[22:10:36] INFO - ConvNetX: init\n[22:10:36] INFO - Classifier: init\n\n\n_target_: nimrod.models.conv.ConvNetX\nnum_classes: 10\nnnet:\n  _target_: nimrod.models.conv.ConvNet\n  n_features:\n  - 1\n  - 8\n  - 16\n  - 32\n  - 64\n  num_classes: ${..num_classes}\n  kernel_size: 3\n  bias: null\n  normalization:\n    _target_: hydra.utils.get_class\n    path: torch.nn.BatchNorm2d\n  activation:\n    _target_: hydra.utils.get_class\n    path: torch.nn.ReLU\noptimizer:\n  _target_: torch.optim.Adam\n  _partial_: true\n  lr: 0.4\nscheduler:\n  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n  _partial_: true\n  mode: min\n  factor: 0.1\n  patience: 5\n\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n[22:10:54] INFO - Optimizer: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.4\n    maximize: False\n    weight_decay: 0\n)\n[22:10:54] INFO - Scheduler: &lt;torch.optim.lr_scheduler.ReduceLROnPlateau object&gt;\n\n  | Name         | Type               | Params | Mode \n------------------------------------------------------------\n0 | loss         | CrossEntropyLoss   | 0      | train\n1 | train_acc    | MulticlassAccuracy | 0      | train\n2 | val_acc      | MulticlassAccuracy | 0      | train\n3 | test_acc     | MulticlassAccuracy | 0      | train\n4 | train_loss   | MeanMetric         | 0      | train\n5 | val_loss     | MeanMetric         | 0      | train\n6 | test_loss    | MeanMetric         | 0      | train\n7 | val_acc_best | MaxMetric          | 0      | train\n8 | nnet         | ConvNet            | 30.3 K | train\n------------------------------------------------------------\n30.3 K    Trainable params\n0         Non-trainable params\n30.3 K    Total params\n0.121     Total estimated model params size (MB)\n34        Modules in train mode\n0         Modules in eval mode\n\n\n\n\n\n\n\n\n\n\n\n[22:11:24] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:11:54] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:12:24] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:12:55] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:13:25] INFO - scheduler is an instance of Reduce plateau\n`Trainer.fit` stopped: `max_epochs=5` reached.\n\n\n\n########################\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head()\n\n#########################\nplt.figure()\nplt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.figure()\nplt.plot(metrics['step'], metrics['lr-Adam'], 'g.-')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrainer.test(model, dm.test_dataloader())\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │     0.803600013256073     │\n│         test/loss         │    0.5951264500617981     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[{'test/loss': 0.5951264500617981, 'test/acc': 0.803600013256073}]",
    "crumbs": [
      "Image",
      "Models",
      "AutoEncoders"
    ]
  },
  {
    "objectID": "models.autoencoders.html#fashion-mnist",
    "href": "models.autoencoders.html#fashion-mnist",
    "title": "AutoEncoders",
    "section": "",
    "text": "cfg = OmegaConf.load('../config/data/image/image.yaml')\ndm = instantiate(cfg, name='fashion_mnist', data_dir='../data/image/')\ndm.prepare_data()\ndm.setup()\nprint(dm.num_classes)\n\n[15:41:01] INFO - Init ImageDataModule for fashion_mnist\n\n\n\n---------------------------------------------------------------------------\nHTTPError                                 Traceback (most recent call last)\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:406, in hf_raise_for_status(response, endpoint_name)\n    405 try:\n--&gt; 406     response.raise_for_status()\n    407 except HTTPError as e:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n   1023 if http_error_msg:\n-&gt; 1024     raise HTTPError(http_error_msg, response=self)\n\nHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/api/datasets/zalando-datasets/fashion_mnist/paths-info/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n\nThe above exception was the direct cause of the following exception:\n\nHfHubHTTPError                            Traceback (most recent call last)\nCell In[5], line 4\n      2 cfg = OmegaConf.load('../config/data/image/image.yaml')\n      3 dm = instantiate(cfg, name='fashion_mnist', data_dir='../data/image/')\n----&gt; 4 dm.prepare_data()\n      5 dm.setup()\n      6 print(dm.num_classes)\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:415, in ImageDataModule.prepare_data(self)\n    412 \"\"\"Download data if needed \n    413 \"\"\"\n    414 # train set\n--&gt; 415 self.train_ds = ImageDataset(\n    416     self.hparams.name,\n    417     *self.args,\n    418     data_dir = self.hparams.data_dir,\n    419     split='train',\n    420     transforms = self.hparams.transforms,\n    421     **self.kwargs\n    422 )\n    423 # get num classes before setup method converst ImageDataset to Subset\n    424 self._num_classes = self.train_ds.num_classes\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:204, in ImageDataset.__init__(self, name, data_dir, split, transforms, streaming, exclude_grey_scale, verification_mode, from_image_folder, from_disk, *args)\n    202 self.exclude_grey_scale = exclude_grey_scale\n    203 if not from_image_folder:\n--&gt; 204     self.info = load_dataset_builder(name, *args)\n    205     if split not in self.info.info.splits:\n    206         raise ValueError(f\"The specified split '{split}' does not exist in the dataset '{name}'. Available splits: {list(self.info.info.splits.keys())}\")\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n   1885 # Instantiate the dataset builder\n-&gt; 1886 builder_instance: DatasetBuilder = builder_cls(\n   1887     cache_dir=cache_dir,\n   1888     dataset_name=dataset_name,\n   1889     config_name=config_name,\n   1890     data_dir=data_dir,\n   1891     data_files=data_files,\n   1892     hash=dataset_module.hash,\n   1893     info=info,\n   1894     features=features,\n   1895     token=token,\n   1896     storage_options=storage_options,\n   1897     **builder_kwargs,\n   1898     **config_kwargs,\n   1899 )\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n   1902 return builder_instance\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:342, in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    340     config_kwargs[\"data_dir\"] = data_dir\n    341 self.config_kwargs = config_kwargs\n--&gt; 342 self.config, self.config_id = self._create_builder_config(\n    343     config_name=config_name,\n    344     custom_features=features,\n    345     **config_kwargs,\n    346 )\n    348 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    349 # Prefill datasetinfo\n    350 if info is None:\n    351     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:597, in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    594     raise ValueError(f\"BuilderConfig must have a name, got {builder_config.name}\")\n    596 # resolve data files if needed\n--&gt; 597 builder_config._resolve_data_files(\n    598     base_path=self.base_path,\n    599     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    600 )\n    602 # compute the config id that is going to be used for caching\n    603 config_id = builder_config.create_config_id(\n    604     config_kwargs,\n    605     custom_features=custom_features,\n    606 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:206, in BuilderConfig._resolve_data_files(self, base_path, download_config)\n    204 if isinstance(self.data_files, DataFilesPatternsDict):\n    205     base_path = xjoin(base_path, self.data_dir) if self.data_dir else base_path\n--&gt; 206     self.data_files = self.data_files.resolve(base_path, download_config)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:818, in DataFilesPatternsDict.resolve(self, base_path, download_config)\n    816 out = DataFilesDict()\n    817 for key, data_files_patterns_list in self.items():\n--&gt; 818     out[key] = data_files_patterns_list.resolve(base_path, download_config)\n    819 return out\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:771, in DataFilesPatternsList.resolve(self, base_path, download_config)\n    768 for pattern, allowed_extensions in zip(self, self.allowed_extensions):\n    769     try:\n    770         data_files.extend(\n--&gt; 771             resolve_pattern(\n    772                 pattern,\n    773                 base_path=base_path,\n    774                 allowed_extensions=allowed_extensions,\n    775                 download_config=download_config,\n    776             )\n    777         )\n    778     except FileNotFoundError:\n    779         if not has_magic(pattern):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:388, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\n    383 if protocol == \"hf\" and config.HF_HUB_VERSION &gt;= version.parse(\"0.20.0\"):\n    384     # 10 times faster glob with detail=True (ignores costly info like lastCommit)\n    385     glob_kwargs[\"expand_info\"] = False\n    386 matched_paths = [\n    387     filepath if filepath.startswith(protocol_prefix) else protocol_prefix + filepath\n--&gt; 388     for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n    389     if info[\"type\"] == \"file\"\n    390     and (xbasename(filepath) not in files_to_ignore)\n    391     and not _is_inside_unrequested_special_dir(filepath, fs_pattern)\n    392     and not _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n    393 ]  # ignore .ipynb and __pycache__, but keep /../\n    394 if allowed_extensions is not None:\n    395     out = [\n    396         filepath\n    397         for filepath in matched_paths\n    398         if any(\".\" + suffix in allowed_extensions for suffix in xbasename(filepath).split(\".\")[1:])\n    399     ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:521, in HfFileSystem.glob(self, path, **kwargs)\n    519 kwargs = {\"expand_info\": kwargs.get(\"detail\", False), **kwargs}\n    520 path = self.resolve_path(path, revision=kwargs.get(\"revision\")).unresolve()\n--&gt; 521 return super().glob(path, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:611, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs)\n    608     else:\n    609         depth = None\n--&gt; 611 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n    613 pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n    614 pattern = re.compile(pattern)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:556, in HfFileSystem.find(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\n    533 \"\"\"\n    534 List all files below path.\n    535 \n   (...)\n    553     `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\n    554 \"\"\"\n    555 if maxdepth:\n--&gt; 556     return super().find(\n    557         path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, refresh=refresh, revision=revision, **kwargs\n    558     )\n    559 resolved_path = self.resolve_path(path, revision=revision)\n    560 path = resolved_path.unresolve()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:502, in AbstractFileSystem.find(self, path, maxdepth, withdirs, detail, **kwargs)\n    499 # Add the root directory if withdirs is requested\n    500 # This is needed for posix glob compliance\n    501 if withdirs and path != \"\" and self.isdir(path):\n--&gt; 502     out[path] = self.info(path)\n    504 for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\n    505     if withdirs:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:719, in HfFileSystem.info(self, path, refresh, revision, **kwargs)\n    717     out = out1[0]\n    718 if refresh or out is None or (expand_info and out and out[\"last_commit\"] is None):\n--&gt; 719     paths_info = self._api.get_paths_info(\n    720         resolved_path.repo_id,\n    721         resolved_path.path_in_repo,\n    722         expand=expand_info,\n    723         revision=resolved_path.revision,\n    724         repo_type=resolved_path.repo_type,\n    725     )\n    726     if not paths_info:\n    727         _raise_file_not_found(path, None)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    111 if check_use_auth_token:\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n--&gt; 114 return fn(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3303, in HfApi.get_paths_info(self, repo_id, paths, expand, revision, repo_type, token)\n   3293 headers = self._build_hf_headers(token=token)\n   3295 response = get_session().post(\n   3296     f\"{self.endpoint}/api/{repo_type}s/{repo_id}/paths-info/{revision}\",\n   3297     data={\n   (...)\n   3301     headers=headers,\n   3302 )\n-&gt; 3303 hf_raise_for_status(response)\n   3304 paths_info = response.json()\n   3305 return [\n   3306     RepoFile(**path_info) if path_info[\"type\"] == \"file\" else RepoFolder(**path_info)\n   3307     for path_info in paths_info\n   3308 ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:477, in hf_raise_for_status(response, endpoint_name)\n    473     raise _format(HfHubHTTPError, message, response) from e\n    475 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\n    476 # as well (request id and/or server error message)\n--&gt; 477 raise _format(HfHubHTTPError, str(e), response) from e\n\nHfHubHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/api/datasets/zalando-datasets/fashion_mnist/paths-info/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n\n\n\n\ndm.show_grid(3,3)\n\n\n\n\n\n\n\n\n\nprint(dm.label_names)\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[6], line 2\n      1 #| notest\n----&gt; 2 print(dm.label_names)\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:409, in ImageDataModule.label_names(self)\n    407 if self.train_ds is not None:\n    408     return self._label_names\n--&gt; 409 raise RuntimeError(\"train_ds is not initialized. Call prepare_data() first.\")\n\nRuntimeError: train_ds is not initialized. Call prepare_data() first.\n\n\n\n\n\n\ncfg = OmegaConf.load('../config/model/image/convnetx_adam.yaml')\n# nnet = instantiate(cfg.nnet, num_classes=dm.num_classes)\n# optimizer = instantiate(cfg.optimizer)\n# scheduler = instantiate(cfg.scheduler)\n\n# model = ConvNetX(nnet, dm.num_classes, optimizer, scheduler)\nmodel = instantiate(cfg, num_classes=10)\n\n[15:41:17] INFO - ConvNetX: init\n[15:41:17] INFO - Classifier: init\n\n\n\nMAX_EPOCHS = 5\ndm.batch_size = 256\nprint(dm.batch_size)\n# lr = 0.4\n\ntrainer = Trainer(\n    max_epochs=MAX_EPOCHS,\n    logger=CSVLogger(\"logs\", name=\"fashion_mnist_convnet\"),\n    callbacks = [LearningRateMonitor(logging_interval=\"step\")],\n    check_val_every_n_epoch=1,\n    log_every_n_steps=1\n    )\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n\n256\n\n\n\n\n\n\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(\n    model,\n    datamodule=dm,\n    min_lr=1e-6,\n    max_lr=1.0,\n    num_training=100,  # number of iterations\n    # attr_name=\"optimizer.lr\",\n)\nfig = lr_finder.plot(suggest=True)\nplt.show()\nprint(f\"Suggested learning rate: {lr_finder.suggestion()}\")\n\nYou are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n[22:43:36] INFO - loading dataset fashion_mnist with args () from split train\n[22:43:36] INFO - loading dataset fashion_mnist from split train\nOverwrite dataset info from restored data version if exists.\n[22:43:37] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:37] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[22:43:37] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:37] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:41] INFO - loading dataset fashion_mnist with args () from split test\n[22:43:41] INFO - loading dataset fashion_mnist from split test\nOverwrite dataset info from restored data version if exists.\n[22:43:43] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:43] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[22:43:43] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[22:43:43] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[22:43:43] INFO - Optimizer: &lt;class 'torch.optim.adam.Adam'&gt;\n[22:43:43] INFO - Scheduler: &lt;class 'torch.optim.lr_scheduler.ReduceLROnPlateau'&gt;\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:317: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n\n\n\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[12], line 3\n      1 #| notest\n      2 tuner = Tuner(trainer)\n----&gt; 3 lr_finder = tuner.lr_find(\n      4     model,\n      5     datamodule=dm,\n      6     min_lr=1e-6,\n      7     max_lr=1.0,\n      8     num_training=100,  # number of iterations\n      9     # attr_name=\"optimizer.lr\",\n     10 )\n     11 fig = lr_finder.plot(suggest=True)\n     12 plt.show()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:180, in Tuner.lr_find(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\n    177 lr_finder_callback._early_exit = True\n    178 self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\n--&gt; 180 self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\n    182 self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\n    184 return lr_finder_callback.optimal_lr\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:539, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    537 self.state.status = TrainerStatus.RUNNING\n    538 self.training = True\n--&gt; 539 call._call_and_handle_interrupt(\n    540     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n    541 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:47, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)\n     45     if trainer.strategy.launcher is not None:\n     46         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n---&gt; 47     return trainer_fn(*args, **kwargs)\n     49 except _TunerExitException:\n     50     _call_teardown_hook(trainer)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:575, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    568 assert self.state.fn is not None\n    569 ckpt_path = self._checkpoint_connector._select_ckpt_path(\n    570     self.state.fn,\n    571     ckpt_path,\n    572     model_provided=True,\n    573     model_connected=self.lightning_module is not None,\n    574 )\n--&gt; 575 self._run(model, ckpt_path=ckpt_path)\n    577 assert self.state.stopped\n    578 self.training = False\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:962, in Trainer._run(self, model, ckpt_path)\n    960 # hook\n    961 if self.state.fn == TrainerFn.FITTING:\n--&gt; 962     call._call_callback_hooks(self, \"on_fit_start\")\n    963     call._call_lightning_module_hook(self, \"on_fit_start\")\n    965 _log_hyperparams(self)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:222, in _call_callback_hooks(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\n    220     if callable(fn):\n    221         with trainer.profiler.profile(f\"[Callback]{callback.state_key}.{hook_name}\"):\n--&gt; 222             fn(trainer, trainer.lightning_module, *args, **kwargs)\n    224 if pl_module:\n    225     # restore current_fx when nested context\n    226     pl_module._current_fx_name = prev_fx_name\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:130, in LearningRateFinder.on_fit_start(self, trainer, pl_module)\n    128 @override\n    129 def on_fit_start(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n--&gt; 130     self.lr_find(trainer, pl_module)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:113, in LearningRateFinder.lr_find(self, trainer, pl_module)\n    111 def lr_find(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\") -&gt; None:\n    112     with isolate_rng():\n--&gt; 113         self.optimal_lr = _lr_find(\n    114             trainer,\n    115             pl_module,\n    116             min_lr=self._min_lr,\n    117             max_lr=self._max_lr,\n    118             num_training=self._num_training_steps,\n    119             mode=self._mode,\n    120             early_stop_threshold=self._early_stop_threshold,\n    121             update_attr=self._update_attr,\n    122             attr_name=self._attr_name,\n    123         )\n    125     if self._early_exit:\n    126         raise _TunerExitException()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:278, in _lr_find(trainer, model, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\n    275 lr_finder._exchange_scheduler(trainer)\n    277 # Fit, lr & loss logged in callback\n--&gt; 278 _try_loop_run(trainer, params)\n    280 # Prompt if we stopped early\n    281 if trainer.global_step != num_training + start_steps:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/lr_finder.py:523, in _try_loop_run(trainer, params)\n    521 loop.load_state_dict(deepcopy(params[\"loop_state_dict\"]))\n    522 loop.restarting = False\n--&gt; 523 loop.run()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216, in _FitLoop.run(self)\n    214 try:\n    215     self.on_advance_start()\n--&gt; 216     self.advance()\n    217     self.on_advance_end()\n    218 except StopIteration:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:455, in _FitLoop.advance(self)\n    453 with self.trainer.profiler.profile(\"run_training_epoch\"):\n    454     assert self._data_fetcher is not None\n--&gt; 455     self.epoch_loop.run(self._data_fetcher)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150, in _TrainingEpochLoop.run(self, data_fetcher)\n    148 while not self.done:\n    149     try:\n--&gt; 150         self.advance(data_fetcher)\n    151         self.on_advance_end(data_fetcher)\n    152     except StopIteration:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:322, in _TrainingEpochLoop.advance(self, data_fetcher)\n    320             batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n    321         else:\n--&gt; 322             batch_output = self.manual_optimization.run(kwargs)\n    324 self.batch_progress.increment_processed()\n    326 # update non-plateau LR schedulers\n    327 # update epoch-interval ones only when we are at the end of training epoch\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/manual.py:94, in _ManualOptimization.run(self, kwargs)\n     92 self.on_run_start()\n     93 with suppress(StopIteration):  # no loop to break at this level\n---&gt; 94     self.advance(kwargs)\n     95 self._restarting = False\n     96 return self.on_run_end()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/manual.py:114, in _ManualOptimization.advance(self, kwargs)\n    111 trainer = self.trainer\n    113 # manually capture logged metrics\n--&gt; 114 training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n    115 del kwargs  # release the batch from memory\n    116 self.trainer.strategy.post_training_step()  # unused hook - call anyway for backward compatibility\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:323, in _call_strategy_hook(trainer, hook_name, *args, **kwargs)\n    320     return None\n    322 with trainer.profiler.profile(f\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"):\n--&gt; 323     output = fn(*args, **kwargs)\n    325 # restore current_fx when nested context\n    326 pl_module._current_fx_name = prev_fx_name\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391, in Strategy.training_step(self, *args, **kwargs)\n    389 if self.model != self.lightning_module:\n    390     return self._forward_redirection(self.model, self.lightning_module, \"training_step\", *args, **kwargs)\n--&gt; 391 return self.lightning_module.training_step(*args, **kwargs)\n\nFile ~/Projects/nimrod/nimrod/models/core.py:186, in Classifier.training_step(self, batch, batch_idx)\n    183     sched.step() #reduce plateau sched is updated at end of epoch only instead TODO: should it be applied to val loop by default?\n    185 self.train_loss(loss)\n--&gt; 186 self.train_acc(preds, y)\n    187 metrics = {\"train/loss\": self.train_loss, \"train/acc\": self.train_acc}\n    188 self.log_dict(metrics, on_epoch=True, on_step=True, prog_bar=True)# Pass the validation loss to the scheduler\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1532, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1530     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1531 else:\n-&gt; 1532     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1541, in Module._call_impl(self, *args, **kwargs)\n   1536 # If we don't have any hooks, we want to skip the rest of the logic in\n   1537 # this function, and just call forward.\n   1538 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1539         or _global_backward_pre_hooks or _global_backward_hooks\n   1540         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1541     return forward_call(*args, **kwargs)\n   1543 try:\n   1544     result = None\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:316, in Metric.forward(self, *args, **kwargs)\n    314     self._forward_cache = self._forward_full_state_update(*args, **kwargs)\n    315 else:\n--&gt; 316     self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)\n    318 return self._forward_cache\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:385, in Metric._forward_reduce_state_update(self, *args, **kwargs)\n    382 self._enable_grad = True  # allow grads for batch computation\n    384 # calculate batch state and compute batch value\n--&gt; 385 self.update(*args, **kwargs)\n    386 batch_val = self.compute()\n    388 # reduce batch and global state\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:560, in Metric._wrap_update.&lt;locals&gt;.wrapped_func(*args, **kwargs)\n    552         if \"Expected all tensors to be on\" in str(err):\n    553             raise RuntimeError(\n    554                 \"Encountered different devices in metric calculation (see stacktrace for details).\"\n    555                 \" This could be due to the metric class not being on the same device as input.\"\n   (...)\n    558                 \" device corresponds to the device of the input.\"\n    559             ) from err\n--&gt; 560         raise err\n    562 if self.compute_on_cpu:\n    563     self._move_list_states_to_cpu()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/metric.py:550, in Metric._wrap_update.&lt;locals&gt;.wrapped_func(*args, **kwargs)\n    548 with torch.set_grad_enabled(self._enable_grad):\n    549     try:\n--&gt; 550         update(*args, **kwargs)\n    551     except RuntimeError as err:\n    552         if \"Expected all tensors to be on\" in str(err):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:339, in MulticlassStatScores.update(self, preds, target)\n    337 \"\"\"Update state with predictions and targets.\"\"\"\n    338 if self.validate_args:\n--&gt; 339     _multiclass_stat_scores_tensor_validation(\n    340         preds, target, self.num_classes, self.multidim_average, self.ignore_index\n    341     )\n    342 preds, target = _multiclass_stat_scores_format(preds, target, self.top_k)\n    343 tp, fp, tn, fn = _multiclass_stat_scores_update(\n    344     preds, target, self.num_classes, self.top_k, self.average, self.multidim_average, self.ignore_index\n    345 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:318, in _multiclass_stat_scores_tensor_validation(preds, target, num_classes, multidim_average, ignore_index)\n    316 num_unique_values = len(torch.unique(t, dim=None))\n    317 if num_unique_values &gt; check_value:\n--&gt; 318     raise RuntimeError(\n    319         f\"Detected more unique values in `{name}` than expected. Expected only {check_value} but found\"\n    320         f\" {num_unique_values} in `{name}`. Found values: {torch.unique(t, dim=None)}.\"\n    321     )\n\nRuntimeError: Detected more unique values in `preds` than expected. Expected only 10 but found 30 in `preds`. Found values: tensor([ 0,  1,  3,  6,  7,  8,  9, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 22,\n        23, 25, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39], device='cuda:0').\n\n\n\n\n# print(f\"lr: {model.lr}, bs: {dm.batch_size}\")\n\nlr: 9.120108393559098e-06, bs: 256\n\n\n\n\n\n\ncfg.optimizer.lr = 0.4\nprint(OmegaConf.to_yaml(cfg))\n\nmodel = instantiate(cfg)\ntrainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\n\n[22:10:36] INFO - ConvNetX: init\n[22:10:36] INFO - Classifier: init\n\n\n_target_: nimrod.models.conv.ConvNetX\nnum_classes: 10\nnnet:\n  _target_: nimrod.models.conv.ConvNet\n  n_features:\n  - 1\n  - 8\n  - 16\n  - 32\n  - 64\n  num_classes: ${..num_classes}\n  kernel_size: 3\n  bias: null\n  normalization:\n    _target_: hydra.utils.get_class\n    path: torch.nn.BatchNorm2d\n  activation:\n    _target_: hydra.utils.get_class\n    path: torch.nn.ReLU\noptimizer:\n  _target_: torch.optim.Adam\n  _partial_: true\n  lr: 0.4\nscheduler:\n  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n  _partial_: true\n  mode: min\n  factor: 0.1\n  patience: 5\n\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n[22:10:54] INFO - Optimizer: Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.4\n    maximize: False\n    weight_decay: 0\n)\n[22:10:54] INFO - Scheduler: &lt;torch.optim.lr_scheduler.ReduceLROnPlateau object&gt;\n\n  | Name         | Type               | Params | Mode \n------------------------------------------------------------\n0 | loss         | CrossEntropyLoss   | 0      | train\n1 | train_acc    | MulticlassAccuracy | 0      | train\n2 | val_acc      | MulticlassAccuracy | 0      | train\n3 | test_acc     | MulticlassAccuracy | 0      | train\n4 | train_loss   | MeanMetric         | 0      | train\n5 | val_loss     | MeanMetric         | 0      | train\n6 | test_loss    | MeanMetric         | 0      | train\n7 | val_acc_best | MaxMetric          | 0      | train\n8 | nnet         | ConvNet            | 30.3 K | train\n------------------------------------------------------------\n30.3 K    Trainable params\n0         Non-trainable params\n30.3 K    Total params\n0.121     Total estimated model params size (MB)\n34        Modules in train mode\n0         Modules in eval mode\n\n\n\n\n\n\n\n\n\n\n\n[22:11:24] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:11:54] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:12:24] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:12:55] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[22:13:25] INFO - scheduler is an instance of Reduce plateau\n`Trainer.fit` stopped: `max_epochs=5` reached.\n\n\n\n########################\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head()\n\n#########################\nplt.figure()\nplt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.figure()\nplt.plot(metrics['step'], metrics['lr-Adam'], 'g.-')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrainer.test(model, dm.test_dataloader())\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │     0.803600013256073     │\n│         test/loss         │    0.5951264500617981     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[{'test/loss': 0.5951264500617981, 'test/acc': 0.803600013256073}]",
    "crumbs": [
      "Image",
      "Models",
      "AutoEncoders"
    ]
  },
  {
    "objectID": "models.autoencoders.html#fc-autoencoder",
    "href": "models.autoencoders.html#fc-autoencoder",
    "title": "AutoEncoders",
    "section": "FC Autoencoder",
    "text": "FC Autoencoder\n\nclass LinearEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n\n    def forward(self, x):\n        return self.l1(x)\n\n\nclass LinearDecoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n    def forward(self, x):\n        return self.l1(x)\n\n\nenc = LinearEncoder()\ndec = LinearDecoder()\na = AutoEncoder(enc, dec)\nbatch = torch.rand((5, 3, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n# y = a(batch)\n# print(y.shape)\n\ntorch.Size([5, 3, 3])\n\n\n\nds = ImageDataset(name='fashion_mnist', data_dir='../data/image/')\ndl = DataLoader(ds, batch_size=3)\nb = next(iter(dl))\nprint(f\" X: {b[0].shape}, Y: {b[1].shape}\")\n\n X: torch.Size([3, 1, 28, 28]), Y: torch.Size([3])\n\n\n\nacfg = OmegaConf.load('../config/data/image/image.yaml')\ndm = instantiate(cfg, name='fashion_mnist', data_dir='../data/image/')\ndm.prepare_data()\ndm.setup()\n# print(f\"num classes: {dm.num_classes}, bs: {dm.batch_size}, labels: {dm.label_names}\" if dm.label_names else f\"num classes: {dm.num_classes}\")\n\n[14:58:07] INFO - Init ImageDataModule for fashion_mnist\n[14:58:22] INFO - split train into train/val [0.8, 0.2]\n[14:58:22] INFO - train: 48000 val: 12000, test: 10000\n\n\n\ndevice = get_device()\nprint(f\"Device: {device}\")\nenc = LinearEncoder()\ndec =LinearDecoder()\nmodel = AutoEncoder(enc, dec).to(device)\n\n[14:55:26] INFO - Using device: mps\n\n\nDevice: mps\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nN_EPOCHS = 5\n\nfor epoch in range(N_EPOCHS):\n    i = 0\n    model.train()\n    for images, labels in dm.train_dataloader():\n        optimizer.zero_grad()\n        images, labels = images.to(device), labels.to(device)\n        # B x C x H x W -&gt; B x C x L\n        images = images.view(-1, images.size(2) * images.size(3))\n        outputs = model(images)\n        # output should be as close to input as possible\n        loss = criterion(outputs, images)        \n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        total_loss, epoch_step = 0, 0\n        for images, labels in dm.val_dataloader():\n            images, labels = images.to(device), labels.to(device)\n            images = images.view(-1, images.size(2) * images.size(3))\n            outputs = model(images)\n            eval_loss = criterion(outputs, images)\n            epoch_len = len(images)\n            epoch_step += epoch_len\n            total_loss += eval_loss.item() * epoch_len\n    logger.info(f\"Epoch: {epoch}, len: {epoch_len}, Loss: {total_loss / epoch_step:.3f}\")\n\n[14:56:24] INFO - Epoch: 0, len: 32, Loss: 0.025\n[14:56:27] INFO - Epoch: 1, len: 32, Loss: 0.025\n[14:56:31] INFO - Epoch: 2, len: 32, Loss: 0.025\n[14:56:35] INFO - Epoch: 3, len: 32, Loss: 0.025\n[14:56:39] INFO - Epoch: 4, len: 32, Loss: 0.025\n\n\n\nx, y = next(iter(dm.train_dataloader()))\nprint(f\" X: {x.shape}, Y: {y.shape}\")\nx = x.to(device)\nB, C, H, W = x.shape\nx_hat = model(x.view(-1, H * W)).view(-1, C, H, W)\nprint(f\" X_hat: {x_hat.shape}\")\nidx = 0\nn_rows, n_cols = 1, 2\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\naxs[0].imshow(x[idx].permute(1, 2, 0).cpu().numpy(), cmap='gray')\naxs[1].imshow(x_hat[idx].permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')\nplt.show()\n\n X: torch.Size([64, 1, 28, 28]), Y: torch.Size([64])\n X_hat: torch.Size([64, 1, 28, 28])",
    "crumbs": [
      "Image",
      "Models",
      "AutoEncoders"
    ]
  },
  {
    "objectID": "models.autoencoders.html#convnet-autoencoder",
    "href": "models.autoencoders.html#convnet-autoencoder",
    "title": "AutoEncoders",
    "section": "ConvNet Autoencoder",
    "text": "ConvNet Autoencoder\n\nclass ConvEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = nn.ModuleList()\n        # X -&gt; B,C,28,28\n        layers.append(nn.ZeroPad2d(2)) # X -&gt; B,C,32,32\n        layers.append(ConvLayer(1,2, normalization=None)) # 16 x 16\n        layers.append(ConvLayer(2,4, normalization=None)) # 8 x 8\n        self._nnet = nn.Sequential(*layers)\n\n    def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n        return self._nnet(x)\n\n\nclass ConvDecoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers = nn.ModuleList()\n        layers.append(DeconvLayer(4,2, normalization=None)) # 16 x 16\n        layers.append(DeconvLayer(2,1, normalization=None, activation=None)) # 32 x 32\n        layers.append(nn.ZeroPad2d(-2)) # 28 x 28\n        layers.append(nn.Sigmoid())\n        self._nnet = nn.Sequential(*layers)\n\n    def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n        return self._nnet(x)\n\n\ndevice = get_device()\nprint(f\"Device: {device}\")\nenc = ConvEncoder()\ndec = ConvDecoder()\nmodel = AutoEncoder(enc, dec).to(device)\n\n[22:44:23] INFO - Using device: cuda\n\n\nDevice: cuda\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\nN_EPOCHS = 5\n\nfor epoch in range(N_EPOCHS):\n    i = 0\n    model.train()\n    for images, labels in dm.train_dataloader():\n        optimizer.zero_grad()\n        images, labels = images.to(device), labels.to(device)\n\n        # images = images.view(-1, images.size(2) * images.size(3))\n        outputs = model(images)\n        # output should be as close to input as possible\n        loss = criterion(outputs, images)        \n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    with torch.no_grad():\n        total_loss, epoch_step = 0, 0\n        for images, labels in dm.val_dataloader():\n            images, labels = images.to(device), labels.to(device)\n            # images = images.view(-1, images.size(2) * images.size(3))\n            outputs = model(images)\n            eval_loss = criterion(outputs, images)\n            epoch_len = len(images)\n            epoch_step += epoch_len\n            total_loss += eval_loss.item() * epoch_len\n    logger.info(f\"Epoch: {epoch}, len: {epoch_len}, Loss: {total_loss / epoch_step:.3f}\")\n\n[15:34:38] INFO - Epoch: 0, len: 32, Loss: 0.012\n[15:34:43] INFO - Epoch: 1, len: 32, Loss: 0.011\n[15:34:48] INFO - Epoch: 2, len: 32, Loss: 0.011\n[15:34:53] INFO - Epoch: 3, len: 32, Loss: 0.011\n[15:34:59] INFO - Epoch: 4, len: 32, Loss: 0.011\n\n\n\nx, y = next(iter(dm.train_dataloader()))\nprint(f\" X: {x.shape}, Y: {y.shape}\")\nx = x.to(device)\nB, C, H, W = x.shape\nx_hat = model(x)\nprint(f\" X_hat: {x_hat.shape}\")\nidx = 0\nn_rows, n_cols = 1, 2\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 10))\naxs[0].imshow(x[idx].permute(1, 2, 0).cpu().numpy(), cmap='gray')\naxs[1].imshow(x_hat[idx].permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')\nplt.show()\n\n X: torch.Size([64, 1, 28, 28]), Y: torch.Size([64])\n X_hat: torch.Size([64, 1, 28, 28])",
    "crumbs": [
      "Image",
      "Models",
      "AutoEncoders"
    ]
  },
  {
    "objectID": "models.autoencoders.html#autoencoder_x",
    "href": "models.autoencoders.html#autoencoder_x",
    "title": "AutoEncoders",
    "section": "AutoEncoder_X",
    "text": "AutoEncoder_X\n\n#show_doc(AutoEncoderPL.forward)\n\n\n\nAutoEncoderPL.forward\n\n AutoEncoderPL.forward (x:torch.Tensor)\n\nForward pass of the AutoEncoder model.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nTensor B x L\n\n\nReturns\nTensor\nReconstructed input tensor of shape B x L\n\n\n\n\n\n\n\nautoencoder_pl = AutoEncoderPL(a)\nb = torch.rand((5,28*28))\ny = autoencoder_pl(b)\nprint(y.shape)\n\ntorch.Size([5, 784])",
    "crumbs": [
      "Image",
      "Models",
      "AutoEncoders"
    ]
  },
  {
    "objectID": "image.med.html",
    "href": "image.med.html",
    "title": "Image helper modules",
    "section": "",
    "text": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on huggingface code base\n * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n'''\n\n\nsource\n\n\n\n BertEmbeddings (config)\n\nConstruct the embeddings from word and position embeddings.\n\nsource\n\n\n\n\n BertSelfAttention (config, is_cross_attention)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertSelfOutput (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertAttention (config, is_cross_attention=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertIntermediate (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertOutput (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLayer (config, layer_num)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLMHeadModel (config)\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nsource\n\n\n\n\n BertModel (config, add_pooling_layer=True)\n\nThe model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in Attention is all you need &lt;https://arxiv.org/abs/1706.03762&gt;__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. argument and :obj:add_cross_attention set to :obj:True; an :obj:encoder_hidden_states is then expected as an input to the forward pass.\n\nsource\n\n\n\n\n BertPreTrainedModel\n                      (config:transformers.configuration_utils.PretrainedC\n                      onfig, *inputs, **kwargs)\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nsource\n\n\n\n\n BertOnlyMLMHead (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLMPredictionHead (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertPredictionHeadTransform (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertPooler (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertEncoder (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "image.med.html#modules",
    "href": "image.med.html#modules",
    "title": "Image helper modules",
    "section": "",
    "text": "'''\n * Copyright (c) 2022, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on huggingface code base\n * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n'''\n\n\nsource\n\n\n\n BertEmbeddings (config)\n\nConstruct the embeddings from word and position embeddings.\n\nsource\n\n\n\n\n BertSelfAttention (config, is_cross_attention)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertSelfOutput (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertAttention (config, is_cross_attention=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertIntermediate (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertOutput (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLayer (config, layer_num)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLMHeadModel (config)\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nsource\n\n\n\n\n BertModel (config, add_pooling_layer=True)\n\nThe model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in Attention is all you need &lt;https://arxiv.org/abs/1706.03762&gt;__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin. argument and :obj:add_cross_attention set to :obj:True; an :obj:encoder_hidden_states is then expected as an input to the forward pass.\n\nsource\n\n\n\n\n BertPreTrainedModel\n                      (config:transformers.configuration_utils.PretrainedC\n                      onfig, *inputs, **kwargs)\n\nAn abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n\nsource\n\n\n\n\n BertOnlyMLMHead (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertLMPredictionHead (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertPredictionHeadTransform (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertPooler (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n BertEncoder (config)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*"
  },
  {
    "objectID": "audio.datasets.tts.html",
    "href": "audio.datasets.tts.html",
    "title": "Audio TTS Datasets",
    "section": "",
    "text": "https://github.com/Lightning-AI/lightning/issues/10358 https://colab.research.google.com/drive/1HKSYPsWx_HoCdrnLpaPdYj5zwlPsM3NH\n\n# tok = TokenCollater()\n# ds = LhotseTTSDataset(tok)\n\n\n\n\n\n\n\n\n#(Waveform, Sample_rate, Original_text, Normalized_text, Speaker_ID, Chapter_ID, Utterance_ID)\nds = LIBRITTS(\"../data/en\", 'test-clean')\nprint(ds[0])\n\n\nplot_waveform(ds[0][0], ds[0][1])\n\n\n\n\n\n# num_jobs=0 turns parallel computing off within jupyter notebook. Else it fails.\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\n# libri = prepare_libritts(\"../data/en/LibriTTS\", dataset_parts=\"test-clean\")\n\n\ndm.setup(stage='test')\n\n\ntest_dl = dm.test_dataloader()\nbatch = next(iter(test_dl))\nprint(batch.keys())\n\n\nprint(batch['feats_pad'].shape)\nplt.imshow(batch['feats_pad'][3].transpose(0,1))\nprint(batch['feats_lens'])\nprint(batch['tokens_pad'][3], batch['tokens_lens'][3])\n\n\noriginal_sentences = dm.tokenizer.inverse(batch['tokens_pad'], batch['tokens_lens'])\nprint(original_sentences)",
    "crumbs": [
      "Audio",
      "Data",
      "Audio TTS Datasets"
    ]
  },
  {
    "objectID": "audio.datasets.tts.html#libritts",
    "href": "audio.datasets.tts.html#libritts",
    "title": "Audio TTS Datasets",
    "section": "",
    "text": "https://github.com/Lightning-AI/lightning/issues/10358 https://colab.research.google.com/drive/1HKSYPsWx_HoCdrnLpaPdYj5zwlPsM3NH\n\n# tok = TokenCollater()\n# ds = LhotseTTSDataset(tok)\n\n\n\n\n\n\n\n\n#(Waveform, Sample_rate, Original_text, Normalized_text, Speaker_ID, Chapter_ID, Utterance_ID)\nds = LIBRITTS(\"../data/en\", 'test-clean')\nprint(ds[0])\n\n\nplot_waveform(ds[0][0], ds[0][1])\n\n\n\n\n\n# num_jobs=0 turns parallel computing off within jupyter notebook. Else it fails.\ndm = LibriTTSDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"test-clean\",\n    output_dir=\"../data/en/LibriTTS/test-clean\",\n    num_jobs=1\n)\n\n\n# skip download and use local data folder\n# dm.prepare_data()\n\n\n# libri = prepare_libritts(\"../data/en/LibriTTS\", dataset_parts=\"test-clean\")\n\n\ndm.setup(stage='test')\n\n\ntest_dl = dm.test_dataloader()\nbatch = next(iter(test_dl))\nprint(batch.keys())\n\n\nprint(batch['feats_pad'].shape)\nplt.imshow(batch['feats_pad'][3].transpose(0,1))\nprint(batch['feats_lens'])\nprint(batch['tokens_pad'][3], batch['tokens_lens'][3])\n\n\noriginal_sentences = dm.tokenizer.inverse(batch['tokens_pad'], batch['tokens_lens'])\nprint(original_sentences)",
    "crumbs": [
      "Audio",
      "Data",
      "Audio TTS Datasets"
    ]
  },
  {
    "objectID": "modules.html",
    "href": "modules.html",
    "title": "Modules",
    "section": "",
    "text": "source\n\n\n\n Decoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Encoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)"
  },
  {
    "objectID": "modules.html#modules",
    "href": "modules.html#modules",
    "title": "Modules",
    "section": "",
    "text": "source\n\n\n\n Decoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Encoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)"
  },
  {
    "objectID": "models.resnet.html",
    "href": "models.resnet.html",
    "title": "ResNet",
    "section": "",
    "text": "source\n\n\n\n ResBlock (in_channels:int, out_channels:int, stride:int=2,\n           kernel_size:int=3, activation:Optional[Type[torch.nn.modules.mo\n           dule.Module]]=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, norma\n           lization:Optional[Type[torch.nn.modules.module.Module]]=&lt;class\n           'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n           pre_activation:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nstride\nint\n2\nStride\n\n\nkernel_size\nint\n3\nKernel size\n\n\nactivation\nOptional\nReLU\nActivation class if no activatoin set to nn.Identity\n\n\nnormalization\nOptional\nBatchNorm2d\nNormalization class\n\n\npre_activation\nbool\nFalse\nreplace conv block by pre-act block. used in unets e.g.\n\n\n\n\n\n\n\nmodel = ResBlock(3, 8, stride=2, activation=partial(nn.LeakyReLU, negative_slope=0.1), normalization=nn.BatchNorm2d)\nx = torch.randn(1, 3, 32, 32)\ny = model(x)\nprint(y.shape)\nsummary(model=model, input_size=(1, 3, 32, 32), depth=2)\n\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([1, 8, 16, 16])\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResBlock                                 [1, 8, 16, 16]            --\n├─Sequential: 1-1                        [1, 8, 16, 16]            --\n│    └─ConvBlock: 2-1                    [1, 8, 32, 32]            232\n│    └─ConvBlock: 2-2                    [1, 8, 16, 16]            592\n├─AvgPool2d: 1-2                         [1, 3, 16, 16]            --\n├─ConvBlock: 1-3                         [1, 8, 16, 16]            --\n│    └─Sequential: 2-3                   [1, 8, 16, 16]            40\n├─LeakyReLU: 1-4                         [1, 8, 16, 16]            --\n==========================================================================================\nTotal params: 864\nTrainable params: 864\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.37\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.20\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.21\n=========================================================================================="
  },
  {
    "objectID": "models.resnet.html#res-block",
    "href": "models.resnet.html#res-block",
    "title": "ResNet",
    "section": "",
    "text": "source\n\n\n\n ResBlock (in_channels:int, out_channels:int, stride:int=2,\n           kernel_size:int=3, activation:Optional[Type[torch.nn.modules.mo\n           dule.Module]]=&lt;class 'torch.nn.modules.activation.ReLU'&gt;, norma\n           lization:Optional[Type[torch.nn.modules.module.Module]]=&lt;class\n           'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n           pre_activation:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nstride\nint\n2\nStride\n\n\nkernel_size\nint\n3\nKernel size\n\n\nactivation\nOptional\nReLU\nActivation class if no activatoin set to nn.Identity\n\n\nnormalization\nOptional\nBatchNorm2d\nNormalization class\n\n\npre_activation\nbool\nFalse\nreplace conv block by pre-act block. used in unets e.g.\n\n\n\n\n\n\n\nmodel = ResBlock(3, 8, stride=2, activation=partial(nn.LeakyReLU, negative_slope=0.1), normalization=nn.BatchNorm2d)\nx = torch.randn(1, 3, 32, 32)\ny = model(x)\nprint(y.shape)\nsummary(model=model, input_size=(1, 3, 32, 32), depth=2)\n\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n[11:14:43] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([1, 8, 16, 16])\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResBlock                                 [1, 8, 16, 16]            --\n├─Sequential: 1-1                        [1, 8, 16, 16]            --\n│    └─ConvBlock: 2-1                    [1, 8, 32, 32]            232\n│    └─ConvBlock: 2-2                    [1, 8, 16, 16]            592\n├─AvgPool2d: 1-2                         [1, 3, 16, 16]            --\n├─ConvBlock: 1-3                         [1, 8, 16, 16]            --\n│    └─Sequential: 2-3                   [1, 8, 16, 16]            40\n├─LeakyReLU: 1-4                         [1, 8, 16, 16]            --\n==========================================================================================\nTotal params: 864\nTrainable params: 864\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 0.37\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.20\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.21\n=========================================================================================="
  },
  {
    "objectID": "models.resnet.html#resnet",
    "href": "models.resnet.html#resnet",
    "title": "ResNet",
    "section": "ResNet",
    "text": "ResNet\n\nsource\n\nResNet\n\n ResNet (n_features:List[int]=[1, 8, 16, 32, 64, 32], num_classes:int=10,\n         activation:Optional[Type[torch.nn.modules.module.Module]]=&lt;class\n         'torch.nn.modules.activation.ReLU'&gt;, normalization:Optional[Type[\n         torch.nn.modules.module.Module]]=&lt;class\n         'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n         weight_initialization:bool=False, pre_activation:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[1, 8, 16, 32, 64, 32]\nNumber of input & output channels\n\n\nnum_classes\nint\n10\nNumber of classes\n\n\nactivation\nOptional\nReLU\nActivation function if None set to nn.Identity\n\n\nnormalization\nOptional\nBatchNorm2d\nNormalization function if None set to nn.Identity\n\n\nweight_initialization\nbool\nFalse\nweight init with kaiming\n\n\npre_activation\nbool\nFalse\npre-activation block for deep nets\n\n\n\n\n\nUsage\n\nx = torch.randn(64, 3, 28, 28)\nmodel = ResNet(\n    n_features=[3, 8, 16, 32, 64, 32],\n    num_classes=10,\n    activation=partial(nn.LeakyReLU, negative_slope=0.1),\n    # activation=nn.ReLU,\n    normalization=torch.nn.BatchNorm2d,\n    weight_initialization=True,\n    pre_activation=True\n    )\ny = model(x)\nprint(y.shape)\n# summary(model=model, input_size=(64, 3, 28, 28), depth=10)\n\n[13:12:27] INFO - ResNet: init\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] WARNING - setting conv bias back to False as Batchnorm is used\n[13:12:27] INFO - Init conv & linear with kaiming\n[13:12:27] INFO - LeakyRelu layers weight init\n\n\ntorch.Size([64, 10])\n\n\n\n\nconfig\n\ncfg = OmegaConf.load('../config/model/image/resnetx.yaml')\nB, C, H, W = 64, 1, 28, 28\nx = torch.randn(B, C, H, W)\nnnet = instantiate(cfg.nnet)\ny = nnet(x)\nprint(y.shape)\nsummary(nnet, input_size=(B, C, H, W), depth=10)\n\n[14:43:51] INFO - ResNet: init\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n[14:43:51] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([64, 40])\n\n\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nResNet                                             [64, 40]                  --\n├─Sequential: 1-1                                  [64, 40]                  --\n│    └─ResBlock: 2-1                               [64, 8, 28, 28]           --\n│    │    └─Sequential: 3-1                        [64, 8, 28, 28]           --\n│    │    │    └─ConvBlock: 4-1                    [64, 8, 28, 28]           --\n│    │    │    │    └─Sequential: 5-1              [64, 8, 28, 28]           --\n│    │    │    │    │    └─Conv2d: 6-1             [64, 8, 28, 28]           72\n│    │    │    │    │    └─BatchNorm2d: 6-2        [64, 8, 28, 28]           16\n│    │    │    │    │    └─ReLU: 6-3               [64, 8, 28, 28]           --\n│    │    │    └─ConvBlock: 4-2                    [64, 8, 28, 28]           --\n│    │    │    │    └─Sequential: 5-2              [64, 8, 28, 28]           --\n│    │    │    │    │    └─Conv2d: 6-4             [64, 8, 28, 28]           576\n│    │    │    │    │    └─BatchNorm2d: 6-5        [64, 8, 28, 28]           16\n│    │    └─Identity: 3-2                          [64, 1, 28, 28]           --\n│    │    └─ConvBlock: 3-3                         [64, 8, 28, 28]           --\n│    │    │    └─Sequential: 4-3                   [64, 8, 28, 28]           --\n│    │    │    │    └─Conv2d: 5-3                  [64, 8, 28, 28]           8\n│    │    │    │    └─BatchNorm2d: 5-4             [64, 8, 28, 28]           16\n│    │    └─ReLU: 3-4                              [64, 8, 28, 28]           --\n│    └─ResBlock: 2-2                               [64, 16, 14, 14]          --\n│    │    └─Sequential: 3-5                        [64, 16, 14, 14]          --\n│    │    │    └─ConvBlock: 4-4                    [64, 16, 28, 28]          --\n│    │    │    │    └─Sequential: 5-5              [64, 16, 28, 28]          --\n│    │    │    │    │    └─Conv2d: 6-6             [64, 16, 28, 28]          1,152\n│    │    │    │    │    └─BatchNorm2d: 6-7        [64, 16, 28, 28]          32\n│    │    │    │    │    └─ReLU: 6-8               [64, 16, 28, 28]          --\n│    │    │    └─ConvBlock: 4-5                    [64, 16, 14, 14]          --\n│    │    │    │    └─Sequential: 5-6              [64, 16, 14, 14]          --\n│    │    │    │    │    └─Conv2d: 6-9             [64, 16, 14, 14]          2,304\n│    │    │    │    │    └─BatchNorm2d: 6-10       [64, 16, 14, 14]          32\n│    │    └─AvgPool2d: 3-6                         [64, 8, 14, 14]           --\n│    │    └─ConvBlock: 3-7                         [64, 16, 14, 14]          --\n│    │    │    └─Sequential: 4-6                   [64, 16, 14, 14]          --\n│    │    │    │    └─Conv2d: 5-7                  [64, 16, 14, 14]          128\n│    │    │    │    └─BatchNorm2d: 5-8             [64, 16, 14, 14]          32\n│    │    └─ReLU: 3-8                              [64, 16, 14, 14]          --\n│    └─ResBlock: 2-3                               [64, 32, 7, 7]            --\n│    │    └─Sequential: 3-9                        [64, 32, 7, 7]            --\n│    │    │    └─ConvBlock: 4-7                    [64, 32, 14, 14]          --\n│    │    │    │    └─Sequential: 5-9              [64, 32, 14, 14]          --\n│    │    │    │    │    └─Conv2d: 6-11            [64, 32, 14, 14]          4,608\n│    │    │    │    │    └─BatchNorm2d: 6-12       [64, 32, 14, 14]          64\n│    │    │    │    │    └─ReLU: 6-13              [64, 32, 14, 14]          --\n│    │    │    └─ConvBlock: 4-8                    [64, 32, 7, 7]            --\n│    │    │    │    └─Sequential: 5-10             [64, 32, 7, 7]            --\n│    │    │    │    │    └─Conv2d: 6-14            [64, 32, 7, 7]            9,216\n│    │    │    │    │    └─BatchNorm2d: 6-15       [64, 32, 7, 7]            64\n│    │    └─AvgPool2d: 3-10                        [64, 16, 7, 7]            --\n│    │    └─ConvBlock: 3-11                        [64, 32, 7, 7]            --\n│    │    │    └─Sequential: 4-9                   [64, 32, 7, 7]            --\n│    │    │    │    └─Conv2d: 5-11                 [64, 32, 7, 7]            512\n│    │    │    │    └─BatchNorm2d: 5-12            [64, 32, 7, 7]            64\n│    │    └─ReLU: 3-12                             [64, 32, 7, 7]            --\n│    └─ResBlock: 2-4                               [64, 16, 4, 4]            --\n│    │    └─Sequential: 3-13                       [64, 16, 4, 4]            --\n│    │    │    └─ConvBlock: 4-10                   [64, 16, 7, 7]            --\n│    │    │    │    └─Sequential: 5-13             [64, 16, 7, 7]            --\n│    │    │    │    │    └─Conv2d: 6-16            [64, 16, 7, 7]            4,608\n│    │    │    │    │    └─BatchNorm2d: 6-17       [64, 16, 7, 7]            32\n│    │    │    │    │    └─ReLU: 6-18              [64, 16, 7, 7]            --\n│    │    │    └─ConvBlock: 4-11                   [64, 16, 4, 4]            --\n│    │    │    │    └─Sequential: 5-14             [64, 16, 4, 4]            --\n│    │    │    │    │    └─Conv2d: 6-19            [64, 16, 4, 4]            2,304\n│    │    │    │    │    └─BatchNorm2d: 6-20       [64, 16, 4, 4]            32\n│    │    └─AvgPool2d: 3-14                        [64, 32, 4, 4]            --\n│    │    └─ConvBlock: 3-15                        [64, 16, 4, 4]            --\n│    │    │    └─Sequential: 4-12                  [64, 16, 4, 4]            --\n│    │    │    │    └─Conv2d: 5-15                 [64, 16, 4, 4]            512\n│    │    │    │    └─BatchNorm2d: 5-16            [64, 16, 4, 4]            32\n│    │    └─ReLU: 3-16                             [64, 16, 4, 4]            --\n│    └─ResBlock: 2-5                               [64, 10, 2, 2]            --\n│    │    └─Sequential: 3-17                       [64, 10, 2, 2]            --\n│    │    │    └─ConvBlock: 4-13                   [64, 10, 4, 4]            --\n│    │    │    │    └─Sequential: 5-17             [64, 10, 4, 4]            --\n│    │    │    │    │    └─Conv2d: 6-21            [64, 10, 4, 4]            1,440\n│    │    │    │    │    └─BatchNorm2d: 6-22       [64, 10, 4, 4]            20\n│    │    │    │    │    └─ReLU: 6-23              [64, 10, 4, 4]            --\n│    │    │    └─ConvBlock: 4-14                   [64, 10, 2, 2]            --\n│    │    │    │    └─Sequential: 5-18             [64, 10, 2, 2]            --\n│    │    │    │    │    └─Conv2d: 6-24            [64, 10, 2, 2]            900\n│    │    │    │    │    └─BatchNorm2d: 6-25       [64, 10, 2, 2]            20\n│    │    └─AvgPool2d: 3-18                        [64, 16, 2, 2]            --\n│    │    └─ConvBlock: 3-19                        [64, 10, 2, 2]            --\n│    │    │    └─Sequential: 4-15                  [64, 10, 2, 2]            --\n│    │    │    │    └─Conv2d: 5-19                 [64, 10, 2, 2]            160\n│    │    │    │    └─BatchNorm2d: 5-20            [64, 10, 2, 2]            20\n│    │    └─ReLU: 3-20                             [64, 10, 2, 2]            --\n│    └─Flatten: 2-6                                [64, 40]                  --\n====================================================================================================\nTotal params: 28,992\nTrainable params: 28,992\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 228.65\n====================================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 49.74\nParams size (MB): 0.12\nEstimated Total Size (MB): 50.06\n===================================================================================================="
  },
  {
    "objectID": "models.resnet.html#resnetx",
    "href": "models.resnet.html#resnetx",
    "title": "ResNet",
    "section": "ResNetX",
    "text": "ResNetX\n\nsource\n\nResNetX\n\n ResNetX (nnet:__main__.ResNet, num_classes:int,\n          optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n          scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nResNet\n\n\n\n\nnum_classes\nint\n\n\n\n\noptimizer\nCallable\n\noptimizer,\n\n\nscheduler\nOptional\nNone\nscheduler\n\n\n\n\n\nUsage\n\nneed to instantiate optimizer to get X models\n\n\ncfg = OmegaConf.load('../config/optimizer/adam_w.yaml')\noptimizer = instantiate(cfg)\n\ncfg = OmegaConf.load('../config/scheduler/step_lr.yaml')\nscheduler = instantiate(cfg)\n\ncfg = OmegaConf.load('../config/model/image/resnetx.yaml')\n\nB, C, H, W = 64, 1, 28, 28\nx = torch.randn(B, C, H, W)\n\nnnet = instantiate(cfg)(optimizer=optimizer, scheduler=scheduler)\ny = nnet(x)\nprint(y.shape)\n\n[14:57:43] INFO - ResNet: init\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:57:43] INFO - ResNetX: init\n[14:57:43] INFO - Classifier: init\n\n\ntorch.Size([64, 40])\n\n\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n\n\n\nsummary(nnet, input_size=(B, C, H, W), depth=5)\n\n=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nResNetX                                                 [64, 40]                  --\n├─ResNet: 1-1                                           [64, 40]                  --\n│    └─Sequential: 2-1                                  [64, 40]                  --\n│    │    └─ResBlock: 3-1                               [64, 8, 28, 28]           --\n│    │    │    └─Sequential: 4-1                        [64, 8, 28, 28]           --\n│    │    │    │    └─ConvBlock: 5-1                    [64, 8, 28, 28]           88\n│    │    │    │    └─ConvBlock: 5-2                    [64, 8, 28, 28]           592\n│    │    │    └─Identity: 4-2                          [64, 1, 28, 28]           --\n│    │    │    └─ConvBlock: 4-3                         [64, 8, 28, 28]           --\n│    │    │    │    └─Sequential: 5-3                   [64, 8, 28, 28]           24\n│    │    │    └─LeakyReLU: 4-4                         [64, 8, 28, 28]           --\n│    │    └─ResBlock: 3-2                               [64, 16, 14, 14]          --\n│    │    │    └─Sequential: 4-5                        [64, 16, 14, 14]          --\n│    │    │    │    └─ConvBlock: 5-4                    [64, 16, 28, 28]          1,184\n│    │    │    │    └─ConvBlock: 5-5                    [64, 16, 14, 14]          2,336\n│    │    │    └─AvgPool2d: 4-6                         [64, 8, 14, 14]           --\n│    │    │    └─ConvBlock: 4-7                         [64, 16, 14, 14]          --\n│    │    │    │    └─Sequential: 5-6                   [64, 16, 14, 14]          160\n│    │    │    └─LeakyReLU: 4-8                         [64, 16, 14, 14]          --\n│    │    └─ResBlock: 3-3                               [64, 32, 7, 7]            --\n│    │    │    └─Sequential: 4-9                        [64, 32, 7, 7]            --\n│    │    │    │    └─ConvBlock: 5-7                    [64, 32, 14, 14]          4,672\n│    │    │    │    └─ConvBlock: 5-8                    [64, 32, 7, 7]            9,280\n│    │    │    └─AvgPool2d: 4-10                        [64, 16, 7, 7]            --\n│    │    │    └─ConvBlock: 4-11                        [64, 32, 7, 7]            --\n│    │    │    │    └─Sequential: 5-9                   [64, 32, 7, 7]            576\n│    │    │    └─LeakyReLU: 4-12                        [64, 32, 7, 7]            --\n│    │    └─ResBlock: 3-4                               [64, 16, 4, 4]            --\n│    │    │    └─Sequential: 4-13                       [64, 16, 4, 4]            --\n│    │    │    │    └─ConvBlock: 5-10                   [64, 16, 7, 7]            4,640\n│    │    │    │    └─ConvBlock: 5-11                   [64, 16, 4, 4]            2,336\n│    │    │    └─AvgPool2d: 4-14                        [64, 32, 4, 4]            --\n│    │    │    └─ConvBlock: 4-15                        [64, 16, 4, 4]            --\n│    │    │    │    └─Sequential: 5-12                  [64, 16, 4, 4]            544\n│    │    │    └─LeakyReLU: 4-16                        [64, 16, 4, 4]            --\n│    │    └─ResBlock: 3-5                               [64, 10, 2, 2]            --\n│    │    │    └─Sequential: 4-17                       [64, 10, 2, 2]            --\n│    │    │    │    └─ConvBlock: 5-13                   [64, 10, 4, 4]            1,460\n│    │    │    │    └─ConvBlock: 5-14                   [64, 10, 2, 2]            920\n│    │    │    └─AvgPool2d: 4-18                        [64, 16, 2, 2]            --\n│    │    │    └─ConvBlock: 4-19                        [64, 10, 2, 2]            --\n│    │    │    │    └─Sequential: 5-15                  [64, 10, 2, 2]            180\n│    │    │    └─LeakyReLU: 4-20                        [64, 10, 2, 2]            --\n│    │    └─Flatten: 3-6                                [64, 40]                  --\n=========================================================================================================\nTotal params: 28,992\nTrainable params: 28,992\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 228.65\n=========================================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 49.74\nParams size (MB): 0.12\nEstimated Total Size (MB): 50.06\n========================================================================================================="
  },
  {
    "objectID": "data.lhotse.html",
    "href": "data.lhotse.html",
    "title": "Lhotse support for datasets",
    "section": "",
    "text": "# download_ljspeech('~/Data/en/')\n# skip this step already done\nljspeech = prepare_ljspeech('../data/en/LJSpeech-1.1', '../recipes/tts/ljspeech/data')\n\n\ncut_set = CutSet.from_manifests(**ljspeech)\nsubset = cut_set.subset(first=3)\nsubset.to_file('../recipes/tts/ljspeech/data/first_3.jsonl.gz')\nreload_subset = CutSet.from_file('../recipes/tts/ljspeech/data/first_3.jsonl.gz')\n\n\nprint(subset[1])\nprint(reload_subset[1])\nprint(len(subset))\n\n\n\n\n\nencodec_extractor = EncoDecExtractor()\n\n\n# torch.set_num_threads(1)\n# torch.set_num_interop_threads(1)\n\n\n# TODO: fix bug for n_jobs &gt;1\ncuts = subset.compute_and_store_features(\n    extractor=encodec_extractor,\n    storage_path=\"../recipes/tts/ljspeech/data/encodec\",\n    num_jobs=1,\n    # storage_type=NumpyHdf5Writer\n)\n\n\nprint(cuts[0])\n\n\ncuts.to_file(\"../recipes/tts/ljspeech/data/first_3.encodec.jsonl.gz\")\ncuts[0]\nreload_cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/first_3.encodec.jsonl.gz\")\nreload_cuts[0]\n\n\n# cuts[0].recording\n!soxi '../data/en/LJSpeech-1.1/wavs/LJ001-0001.wav'\n\n\nstrategy = PrecomputedFeatures()\nfeats, feats_len = strategy(cuts)\n\n# print([(f\"feat: {feat.shape}\", f\"len: {feat_len}\") for feat in feats for feat_len in feats_len])\nprint([feat.shape for feat in feats])\nprint([int(feat_len) for feat_len in feats_len])\nprint(feats.shape, feats_len.shape)\n# TODO: debug OnTheFlyFeature case\n# strategy = OnTheFlyFeatures(extractor=encodec_extractor)\n# feats, feats_len = strategy(cuts)\n# print(feats, feats_len)\n\n\n\n\n\ncleaner = TTSTextNormalizer()\ntokenizer = Phonemizer()\n\n\ncleaner(\"tutu. this is ture!\")\n\n\nn_jobs = 1\nunique_phonemes = set()\nwith CutSet.open_writer('../recipes/tts/ljspeech/data/first_3.final.jsonl.gz', overwrite=True) as writer:\n    for cut in cuts:\n        text = cut.supervisions[0].text\n        print(text)\n        normalized = cleaner(text)\n        print(normalized)\n        phonemes = tokenizer(text)\n        print(phonemes)\n        cut.custom = {'normalized': normalized, 'phonemes': phonemes}\n        writer.write(cut, flush=True)\n        unique_phonemes.update(list(phonemes))\n\n\n\n\n\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.final.jsonl.gz\")\nprint(cuts[0])\nmap = {}\nunique_syms = set()\nfor cut in cuts:\n    unique_syms.update(list(cut.custom['phonemes']))\nfor (i, v) in enumerate(sorted(list(unique_syms))):\n    map[i] = v\nmap[len(map)] = \"&lt;eps&gt;\"\nprint(map, len(map))\n\njson_map = json.dumps(map)\nwith open(\"../data/en/LJSpeech-1.1/map.json\",\"w\") as f:\n    f.write(json_map)\n\n\nwith open('../data/en/LJSpeech-1.1/map.json', 'r') as f:\n    data = json.load(f)\n\nprint(data)\n\n\n\n\n\ncuts[0]\n\n\npc = PhonemeCollater(cuts)\ntokens, tokens_len = pc(cuts)\nprint(tokens, tokens_len)\nprint(pc.inverse(tokens, tokens_len))\n\n\nclass ValleDataset(Dataset):\n    def __init__(\n            self,\n            cuts:CutSet,\n            strategy:BatchIO=PrecomputedFeatures()\n        ):\n        self.extractor = strategy\n        self.tokenizer = PhonemeCollater(cuts)\n\n    def __getitem__(self, cuts: CutSet) -&gt; Dict[str, torch.Tensor]:\n        # getitem is on full cutset not just one cut like usual for pytorch datasets\n        cuts = cuts.sort_by_duration()\n        feats, feat_lens = self.extractor(cuts)\n        tokens, token_lens = self.tokenizer(cuts)\n        return {\"feats_pad\": feats, \"feats_lens\": feat_lens, \"tokens_pad\": tokens, \"tokens_lens\": token_lens}\n\n\nds = ValleDataset(cuts)\n# Dataset performs batching by itself, so we have to indicate that to the DataLoader with batch_size=None\n# train_sampler = BucketingSampler(cuts, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\ntrain_sampler = DynamicBucketingSampler(cuts, max_duration=300, shuffle=True, num_buckets=2)\ndl = DataLoader(ds, sampler=train_sampler, batch_size=None, num_workers=0)\nprint(next(iter(dl)))"
  },
  {
    "objectID": "data.lhotse.html#tts-lhotse",
    "href": "data.lhotse.html#tts-lhotse",
    "title": "Lhotse support for datasets",
    "section": "",
    "text": "# download_ljspeech('~/Data/en/')\n# skip this step already done\nljspeech = prepare_ljspeech('../data/en/LJSpeech-1.1', '../recipes/tts/ljspeech/data')\n\n\ncut_set = CutSet.from_manifests(**ljspeech)\nsubset = cut_set.subset(first=3)\nsubset.to_file('../recipes/tts/ljspeech/data/first_3.jsonl.gz')\nreload_subset = CutSet.from_file('../recipes/tts/ljspeech/data/first_3.jsonl.gz')\n\n\nprint(subset[1])\nprint(reload_subset[1])\nprint(len(subset))\n\n\n\n\n\nencodec_extractor = EncoDecExtractor()\n\n\n# torch.set_num_threads(1)\n# torch.set_num_interop_threads(1)\n\n\n# TODO: fix bug for n_jobs &gt;1\ncuts = subset.compute_and_store_features(\n    extractor=encodec_extractor,\n    storage_path=\"../recipes/tts/ljspeech/data/encodec\",\n    num_jobs=1,\n    # storage_type=NumpyHdf5Writer\n)\n\n\nprint(cuts[0])\n\n\ncuts.to_file(\"../recipes/tts/ljspeech/data/first_3.encodec.jsonl.gz\")\ncuts[0]\nreload_cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/first_3.encodec.jsonl.gz\")\nreload_cuts[0]\n\n\n# cuts[0].recording\n!soxi '../data/en/LJSpeech-1.1/wavs/LJ001-0001.wav'\n\n\nstrategy = PrecomputedFeatures()\nfeats, feats_len = strategy(cuts)\n\n# print([(f\"feat: {feat.shape}\", f\"len: {feat_len}\") for feat in feats for feat_len in feats_len])\nprint([feat.shape for feat in feats])\nprint([int(feat_len) for feat_len in feats_len])\nprint(feats.shape, feats_len.shape)\n# TODO: debug OnTheFlyFeature case\n# strategy = OnTheFlyFeatures(extractor=encodec_extractor)\n# feats, feats_len = strategy(cuts)\n# print(feats, feats_len)\n\n\n\n\n\ncleaner = TTSTextNormalizer()\ntokenizer = Phonemizer()\n\n\ncleaner(\"tutu. this is ture!\")\n\n\nn_jobs = 1\nunique_phonemes = set()\nwith CutSet.open_writer('../recipes/tts/ljspeech/data/first_3.final.jsonl.gz', overwrite=True) as writer:\n    for cut in cuts:\n        text = cut.supervisions[0].text\n        print(text)\n        normalized = cleaner(text)\n        print(normalized)\n        phonemes = tokenizer(text)\n        print(phonemes)\n        cut.custom = {'normalized': normalized, 'phonemes': phonemes}\n        writer.write(cut, flush=True)\n        unique_phonemes.update(list(phonemes))\n\n\n\n\n\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.final.jsonl.gz\")\nprint(cuts[0])\nmap = {}\nunique_syms = set()\nfor cut in cuts:\n    unique_syms.update(list(cut.custom['phonemes']))\nfor (i, v) in enumerate(sorted(list(unique_syms))):\n    map[i] = v\nmap[len(map)] = \"&lt;eps&gt;\"\nprint(map, len(map))\n\njson_map = json.dumps(map)\nwith open(\"../data/en/LJSpeech-1.1/map.json\",\"w\") as f:\n    f.write(json_map)\n\n\nwith open('../data/en/LJSpeech-1.1/map.json', 'r') as f:\n    data = json.load(f)\n\nprint(data)\n\n\n\n\n\ncuts[0]\n\n\npc = PhonemeCollater(cuts)\ntokens, tokens_len = pc(cuts)\nprint(tokens, tokens_len)\nprint(pc.inverse(tokens, tokens_len))\n\n\nclass ValleDataset(Dataset):\n    def __init__(\n            self,\n            cuts:CutSet,\n            strategy:BatchIO=PrecomputedFeatures()\n        ):\n        self.extractor = strategy\n        self.tokenizer = PhonemeCollater(cuts)\n\n    def __getitem__(self, cuts: CutSet) -&gt; Dict[str, torch.Tensor]:\n        # getitem is on full cutset not just one cut like usual for pytorch datasets\n        cuts = cuts.sort_by_duration()\n        feats, feat_lens = self.extractor(cuts)\n        tokens, token_lens = self.tokenizer(cuts)\n        return {\"feats_pad\": feats, \"feats_lens\": feat_lens, \"tokens_pad\": tokens, \"tokens_lens\": token_lens}\n\n\nds = ValleDataset(cuts)\n# Dataset performs batching by itself, so we have to indicate that to the DataLoader with batch_size=None\n# train_sampler = BucketingSampler(cuts, max_duration=300, shuffle=True, bucket_method=\"equal_duration\")\ntrain_sampler = DynamicBucketingSampler(cuts, max_duration=300, shuffle=True, num_buckets=2)\ndl = DataLoader(ds, sampler=train_sampler, batch_size=None, num_workers=0)\nprint(next(iter(dl)))"
  },
  {
    "objectID": "data.core.html",
    "href": "data.core.html",
    "title": "Data Core Utils",
    "section": "",
    "text": "source\n\nDataModule\n\n DataModule (train_val_split:Tuple[float,float]=[0.8, 0.2],\n             batch_size:int=64, num_workers:int=0, pin_memory:bool=False,\n             persistent_workers:bool=False)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_val_split\nTuple\n[0.8, 0.2]\ntrain val test %\n\n\nbatch_size\nint\n64\nsize of compute batch\n\n\nnum_workers\nint\n0\nnum_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow\n\n\npin_memory\nbool\nFalse\nIf you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer\n\n\npersistent_workers\nbool\nFalse\n\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nsplit_train_valid_test\n\n split_train_valid_test (dataset:torch.utils.data.dataset.Dataset,\n                         splits:List[float])",
    "crumbs": [
      "Misc",
      "Data Core Utils"
    ]
  },
  {
    "objectID": "text.tokenizers.html",
    "href": "text.tokenizers.html",
    "title": "Text tokenizers",
    "section": "",
    "text": "# ---\n# skip_exec: true\n# skip_showdoc: true\n# ---\n# # torchtext\n# import torchtext\n# from torchtext.vocab import vocab\n# from torchtext.data.utils import get_tokenizer\n# from torchtext.datasets import AG_NEWS",
    "crumbs": [
      "Text",
      "Data",
      "Text tokenizers"
    ]
  },
  {
    "objectID": "text.tokenizers.html#char-based-tokenizer",
    "href": "text.tokenizers.html#char-based-tokenizer",
    "title": "Text tokenizers",
    "section": "Char-based Tokenizer",
    "text": "Char-based Tokenizer\n\nsource\n\nCharTokenizer\n\n CharTokenizer (vocabulary:List[str])\n\nInitialize self. See help(type(self)) for accurate signature.\n\ntext = Path('../data/text/tiny_shakespeare.txt').read_text()\nprint(text[:25])\n\ntokenizer = CharTokenizer.from_text(text)\nprint(len(tokenizer))\nprint(tokenizer.ctoi['a'])\nprint(tokenizer.itoc[39])\nenc = tokenizer.encode(\"this is swell\")\nprint(enc)\nprint(tokenizer.decode(enc))\n\nFirst Citizen:\nBefore we \n65\n39\na\ntensor([58, 46, 47, 57,  1, 47, 57,  1, 57, 61, 43, 50, 50])\nthis is swell",
    "crumbs": [
      "Text",
      "Data",
      "Text tokenizers"
    ]
  },
  {
    "objectID": "text.tokenizers.html#torchtext",
    "href": "text.tokenizers.html#torchtext",
    "title": "Text tokenizers",
    "section": "Torchtext",
    "text": "Torchtext\nhttps://pytorch.org/text/0.16.0/data_utils.html\n\nclass Tokenizer:\n    def __init__(self,\n                backend:str='spacy', # backend tokenizer default to spacy\n                language:str='en', # language on which tokenization is applied\n                bos:bool=False, # add beginning of sentence tag &lt;bos&gt;\n                eos:bool=False, # add end of sentence tag &lt;eos&gt;\n                ):\n        if backend == 'spacy' and language == 'en':\n            language = 'en_core_web_sm'\n        if backend== 'character_based':\n            self.tokenizer = self.character_tokenizer\n        else:\n            self.tokenizer = get_tokenizer(backend, language=language)\n        self.bos = bos\n        self.eos = eos\n        self.backend = backend\n        print(f\"# Tokenizer uses {self.backend} backend\")\n    \n    @staticmethod\n    def character_tokenizer(text:str)-&gt;List[str]:\n        return [c for c in text]\n    \n    @dispatch\n    def __call__(self, text:str)-&gt;List[str]:\n        res = self.tokenizer(text)\n        if self.bos:\n            res = ['&lt;bos&gt;'] + res\n        if self.eos:\n            res = res + ['&lt;eos&gt;']\n        return(res)\n    \n    @dispatch\n    def __call__(self, texts:List[str])-&gt;List[List[str]]:\n        return [self(text) for text in texts]\n    \n    @dispatch # to replace Iterable\n    # works with agnews type of dataset [(index, text)]\n    def __call__(self, data_iter:Iterable)-&gt;Iterable:\n        for _, text in data_iter:\n            yield self(text)\n\n    @dispatch    \n    def inverse(self, tokens:List[str])-&gt;str:\n        if self.backend == 'character_based':\n            return ''.join(tokens)\n        # TODO: take care of white spaces\n        else:\n            return ' '.join(tokens)\n\n    @dispatch\n    def inverse(self, list_of_tokens:List[List[str]])-&gt;List[str]:\n        s = []\n        for tokens in list_of_tokens:\n            s.append(self.inverse(tokens))\n        return s\n\n\nUsage\n\nString\n\ntok = Tokenizer(backend='character_based', bos=True, eos=True)\n# str -&gt; List[str]\ns = \"Oh, yeah I'm not sure...\"\ntokenized = tok(s)\nprint(\"original sentence: \", s)\nprint(\"tokenized: \", tokenized)\nprint(\"un-tokenized: \", tok.inverse(tokenized))\n\ntok = Tokenizer(backend='spacy', bos=True, eos=True)\n# str -&gt; List[str]\ns = \"Oh, yeah I'm not sure...\"\ntokenized = tok(s)\nprint(\"original sentence: \", s)\nprint(\"tokenized: \", tokenized)\nprint(\"un-tokenized: \", tok.inverse(tokenized))\n\ntok = Tokenizer(backend='basic_english', bos=True, eos=True)\n# str -&gt; List[str]\ns = \"Oh, yeah I'm not sure...\"\ntokenized = tok(s)\nprint(\"original sentence: \", s)\nprint(\"tokenized: \", tokenized)\nprint(\"un-tokenized: \", tok.inverse(tokenized))\n\n# Tokenizer uses character_based backend\noriginal sentence:  Oh, yeah I'm not sure...\ntokenized:  ['&lt;bos&gt;', 'O', 'h', ',', ' ', 'y', 'e', 'a', 'h', ' ', 'I', \"'\", 'm', ' ', 'n', 'o', 't', ' ', 's', 'u', 'r', 'e', '.', '.', '.', '&lt;eos&gt;']\nun-tokenized:  &lt;bos&gt;Oh, yeah I'm not sure...&lt;eos&gt;\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 11\n      8 print(\"tokenized: \", tokenized)\n      9 print(\"un-tokenized: \", tok.inverse(tokenized))\n---&gt; 11 tok = Tokenizer(backend='spacy', bos=True, eos=True)\n     12 # str -&gt; List[str]\n     13 s = \"Oh, yeah I'm not sure...\"\n\nCell In[9], line 15, in Tokenizer.__init__(self, backend, language, bos, eos)\n     13     self.tokenizer = self.character_tokenizer\n     14 else:\n---&gt; 15     self.tokenizer = get_tokenizer(backend, language=language)\n     16 self.bos = bos\n     17 self.eos = eos\n\nNameError: name 'get_tokenizer' is not defined\n\n\n\n\n\nList of strings\n\n# List[str]-&gt;List[List[str]]\ns = [\"Oh, yeah I don't know dude...\", \"this is a test\"]\ntokenized = tok(s)\nprint(\"original sentence: \", s)\nprint(\"tokenized: \", tokenized)\nprint(\"un-tokenized: \", tok.inverse(tokenized))\n\n\n\nIterable\n\n# Iterable -&gt; Iterable\ntok = Tokenizer()\nds = AG_NEWS(split='test') # data pipe\nsample = next(iter(ds)) # (label, text)\nprint(sample)\nit = tok(ds)\ntokens = [token for token in it]\nprint(len(tokens))\nprint(tokens[:2])",
    "crumbs": [
      "Text",
      "Data",
      "Text tokenizers"
    ]
  },
  {
    "objectID": "text.tokenizers.html#hugging-face-tokenizers",
    "href": "text.tokenizers.html#hugging-face-tokenizers",
    "title": "Text tokenizers",
    "section": "Hugging Face tokenizers",
    "text": "Hugging Face tokenizers\nhttps://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n\nsequence = \"Using a Transformer network is simple\"\ntokens = tokenizer.tokenize(sequence)\nprint(tokens)\n\n\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)",
    "crumbs": [
      "Text",
      "Data",
      "Text tokenizers"
    ]
  },
  {
    "objectID": "text.datasets.html",
    "href": "text.datasets.html",
    "title": "Text datasets",
    "section": "",
    "text": "SEED = 42\nset_seed(SEED)\n\nSeed set to 42",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "text.datasets.html#vocab",
    "href": "text.datasets.html#vocab",
    "title": "Text datasets",
    "section": "Vocab",
    "text": "Vocab\nEach row is a list of words (sentence). For each row, extract unique character and add to vocabulary. deals with special characters too.\n\nsource\n\nVocab\n\n Vocab (data_path:str|os.PathLike='../data/text/tiny_shakespeare.txt',\n        specials=['&lt;pad&gt;', '&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_path\nstr | os.PathLike\n../data/text/tiny_shakespeare.txt\npath to text data file\n\n\nspecials\nlist\n[‘’, ‘’, ‘’, ‘’]\nencode special characters\n\n\n\n\n\nUsage\nread text file into a pandas data framew with each row as a new line\n\nv = Vocab('../data/text/tiny_shakespeare.txt', specials=['&lt;pad&gt;', '&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;'])\nprint(v.vocabulary)\n\n[11:50:02] INFO - Vocab: read text file\n\n\n['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;pad&gt;', '&lt;unk&gt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\n\n\n# egs where token * is not in vocab\nprint(v.stoi('*'))\nprint(v.itos(61))\n\n12\nK\n\n\n\nprint(v.vocabulary)\ns = v.stoi([\"&lt;bos&gt;\",\"h\", \"e\", \"l\", \"l\", \"o\", \"*\", \"&lt;eos&gt;\"])\nprint(s)\nprint(v.itos(s))\n\n['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '&lt;bos&gt;', '&lt;eos&gt;', '&lt;pad&gt;', '&lt;unk&gt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n[58, 42, 54, 49, 49, 46, 12, 57]\n['&lt;bos&gt;', 'h', 'e', 'l', 'l', 'o', '&lt;unk&gt;', '&lt;eos&gt;']",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "text.datasets.html#tiny-shakespeare",
    "href": "text.datasets.html#tiny-shakespeare",
    "title": "Text datasets",
    "section": "Tiny shakespeare",
    "text": "Tiny shakespeare\n\nsource\n\nSimpleCharDataset\n\n SimpleCharDataset (data, context_length)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\ndata = Path('../data/text/tiny_shakespeare.txt').read_text()\nprint(data[:64])\n\nds = SimpleCharDataset(data, 8)\nprint(ds[0])\nprint(len(ds))\n\ntokenizer = CharTokenizer.from_text(data)\ntokenized = tokenizer.encode(data)\nds = SimpleCharDataset(tokenized, 8)\nprint(ds[0])\n\ndl = DataLoader(ds, batch_size=8, shuffle=True)\nx,y = next(iter(dl))\nprint(tokenizer.decode(x[0]))\nprint(tokenizer.decode(y[0]))\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAl\n('First Ci', 'irst Cit')\n1115386\n(tensor([18, 47, 56, 57, 58,  1, 15, 47]), tensor([47, 56, 57, 58,  1, 15, 47, 58]))\nom my fa\nm my fat\n\n\n\n\nChar Dataset\nC.f. https://karpathy.github.io/char-rnn/ text is a long continuous string\n\nsource\n\n\nCharDataset\n\n CharDataset\n              (data_path:str|os.PathLike='../data/text/tiny_shakespeare.tx\n              t', context_length:int=3, specials=['&lt;pad&gt;', '&lt;unk&gt;',\n              '&lt;bos&gt;', '&lt;eos&gt;'], add_sentence_tokens:bool=True)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_path\nstr | os.PathLike\n../data/text/tiny_shakespeare.txt\npath to the data file\n\n\ncontext_length\nint\n3\ncontext length\n\n\nspecials\nlist\n[‘’, ‘’, ‘’, ‘’]\nencode special characters\n\n\nadd_sentence_tokens\nbool\nTrue\nadd special tokens to the data\n\n\n\n\nUsage\n\nblock_size = 3 #context_length\nds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['&lt;pad&gt;', '&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;'], add_sentence_tokens=True)\n# just encode &lt;unk&gt; in case unknown characters are encountered in test set\nds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size, specials=['&lt;unk&gt;', '&lt;pad&gt;'], add_sentence_tokens=False)\nprint(\"vocab size: \", ds.vocab_size)\nprint(len(ds))\nfor i in range(2):\n    x, y = ds[i]\n    print(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y)[-1])\n\n[17:33:29] INFO - CharDataset: init\n[17:33:29] INFO - Vocab: read text file\n[17:33:29] INFO - CharDataset: init\n[17:33:29] INFO - Vocab: read text file\n\n\nvocab size:  67\n1115394\nx: tensor([29, 18, 49]) itos:  Fir \ny: tensor([18, 49, 11]) itos:  s\nx: tensor([18, 49, 11]) itos:  irs \ny: tensor([49, 11, 44]) itos:  t\nCPU times: user 160 ms, sys: 12 ms, total: 172 ms\nWall time: 176 ms\n\n\n\nx,y = ds[0]\nprint(\"x:\", x,  \"itos: \", ds.from_tokens(x), \"\\ny:\", y, \"itos: \", ds.from_tokens(y))\nprint(\"vocab size: \", ds.vocab_size)\nprint(\"vocabulary: \", ds.vocabulary)\n\nx: tensor([29, 18, 49]) itos:  Fir \ny: tensor([18, 49, 11]) itos:  irs\nvocab size:  67\nvocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '&lt;pad&gt;', '&lt;unk&gt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n\n\n\nprint(len(ds))\nt = len(ds)*torch.tensor((0.8, 0.1, 0.1))\nlengths = [int(p * len(ds)) for p in (0.8, 0.1, 0.1)]\nlengths[-1] = len(ds) - sum(lengths[:-1])\nprint(lengths)\n\nrandom_split(ds, lengths)\n\n1115394\n[892315, 111539, 111540]\n\n\n[&lt;torch.utils.data.dataset.Subset&gt;,\n &lt;torch.utils.data.dataset.Subset&gt;,\n &lt;torch.utils.data.dataset.Subset&gt;]\n\n\n\n\n\nChar Data Module\n\nsource\n\n\nCharDataModule\n\n CharDataModule\n                 (data_path:str|os.PathLike='../data/text/tiny_shakespeare\n                 .txt', specials=['&lt;pad&gt;', '&lt;unk&gt;', '&lt;bos&gt;', '&lt;eos&gt;'],\n                 add_sentence_tokens:bool=False,\n                 train_val_test_split:Tuple[int,int,int]=(0.8, 0.1, 0.1),\n                 context_size:int=3, batch_size:int=32, num_workers:int=1,\n                 pin_memory:bool=False, persistent_workers:bool=False,\n                 random_split:bool=True)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_path\nstr | os.PathLike\n../data/text/tiny_shakespeare.txt\ndataset\n\n\nspecials\nlist\n[‘’, ‘’, ‘’, ‘’]\n\n\n\nadd_sentence_tokens\nbool\nFalse\n\n\n\ntrain_val_test_split\nTuple\n(0.8, 0.1, 0.1)\ndata module\n\n\ncontext_size\nint\n3\n\n\n\nbatch_size\nint\n32\n\n\n\nnum_workers\nint\n1\n\n\n\npin_memory\nbool\nFalse\n\n\n\npersistent_workers\nbool\nFalse\n\n\n\nrandom_split\nbool\nTrue\n\n\n\n\n\nUsage\n\ndm = CharDataModule(\n    data_path=\"../data/text/tiny_shakespeare.txt\",\n    add_sentence_tokens=False,\n    specials=['&lt;unk&gt;', '&lt;pad&gt;'],\n    context_size=3,\n    train_val_test_split = (0.8, 0.1, 0.1),\n    random_split=False,\n    batch_size=64,\n    num_workers=0,\n    pin_memory=False,\n    persistent_workers=False,\n    )\ndm.prepare_data()\ndm.setup()\n\n[17:46:46] INFO - CharDataModule: init\n[17:46:46] INFO - CharDataModule: setup, split datasets\n[17:46:46] INFO - CharDataset: init\n[17:46:46] INFO - Vocab: read text file\n[17:46:47] INFO - Split dataset into train/val/test. Keep sequence order.\n\n\n\nprint(len(dm.ds))\n\n1115394\n\n\n\nX, Y = dm.train_ds[0]\nprint(dm.ds.from_tokens(X), dm.ds.from_tokens(Y), dm.vocab_size)\n\nFir irs 67\n\n\n\nlen(dm.train_ds), len(dm.val_ds), len(dm.test_ds)\nprint(dm.test_ds[0])\nprint(len(dm.test_dataloader()))\n\n(tensor([51, 59, 59]), tensor([59, 59, 16]))\n1743\n\n\n\ntest_dl = dm.test_dataloader()\nX,Y = next(iter(test_dl))\nprint(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\nprint( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))\n\nX (B,T):  torch.Size([64, 3]) X:  tensor([51, 59, 59]) chars:  ?\n\n\nY (B):  torch.Size([64, 3]) Y:  tensor([59, 59, 16]) chars:  \n\nG\n\n\n\n\nInit from config file\n\ncfg = OmegaConf.load('../config/text/data/tinyshakespeare.yaml')\nprint(cfg)\ndm = instantiate(cfg)\ndm.setup()\n\n\ntest_dl = dm.test_dataloader()\nX,Y = next(iter(test_dl))\nprint(\"X (B,T): \", X.shape, \"X: \", X[0], \"chars: \", dm.ds.from_tokens(X[0]))\nprint( \"Y (B): \", Y.shape, \"Y: \", Y[0], \"chars: \", dm.ds.from_tokens(Y[0]))\n\n\n\n\nHugging Face\nhttps://huggingface.co/learn/nlp-course/chapter7/6?fw=pt\n\nLoad text file\n\ndataset = load_dataset(\"text\", data_files=\"../data/text/tiny_shakespeare.txt\") #, split=['train','dev','test'])\nprint(dataset)\nfull = dataset['train']\n\n\ntrain_test = full.train_test_split(train_size=0.8)\ntest_valid = train_test['test'].train_test_split(train_size=0.5)\nshake = DatasetDict({\n    'train': train_test['train'],\n    'test': test_valid['test'],\n    'valid': test_valid['train']})\nprint(shake)\n\n\nshake['test'][0]\n\n\n\nTokenization / Numericalization\n\nTokenize single element\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\nprint(\"vocab size: \", len(tokenizer))\nprint(\"text row 0: \", shake['test'][0]['text'])\ntokens = tokenizer.tokenize(shake['test'][0]['text'])\nprint(\"tokens of row 0: \", tokens)\n\ncontext_length = 10\npadded = tokenizer(shake['test'][0]['text'], max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\nprint(\"context block & padding for lm: \", padded)\n# print(padded.keys())\nprint('decode single input_id: ', tokenizer.decode(849))\nprint([tokenizer.decode(x) for x in padded['input_ids']])\n\n\n\nTokenize whole dataset using map\n\nfrom omegaconf import OmegaConf\n\n\ncfg = {\n    \"context_length\": 10,\n    \"truncation\": True,\n    \"return_length\": True,\n    \"return_overflowing_tokens\": True,\n}\n\ncfg = OmegaConf.create(cfg)\n\n# tokenizer function called via dataset map\ndef tokenize_function(examples:List[dict[str,str]], cfg:OmegaConf=cfg) -&gt; dict[str, List[List[int]]]:\n    result = tokenizer(\n        examples[\"text\"],\n        max_length=cfg.context_length,\n        truncation=cfg.truncation,\n        return_length=cfg.return_length,\n        return_overflowing_tokens=cfg.return_overflowing_tokens\n        )\n    if tokenizer.is_fast:\n        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n    return result\n\n\ntokenize_function(shake['test'][0])\n\n\nshake_toked = shake.map(\n    tokenize_function, batched=True,\n    remove_columns=[\"text\"],\n    num_proc = 1\n)\n\n\nprint(shake_toked['test'][0])\nprint([tokenizer.decode(x) for x in shake_toked['test'][0]['input_ids']])\nprint(tokenizer.decode(shake_toked['test'][0]['input_ids']))\n\n\nfor split, dset in shake_toked.items():\n    print(split, dset)\n    arr_len = np.sum(dset['length'], dtype=np.uint64)\n\n\n\n\nData Collator\n\nprint(tokenizer.pad_token, tokenizer.eos_token)\n\n\ntokenizer.pad_token = tokenizer.eos_token\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\nout = data_collator([shake_toked[\"test\"]['input_ids'][i] for i in range(5)])\n# out = default_data_collator(shake_toked['test']['input_ids'][0])\nprint(out)\n# for key in out:\n#     print(f\"{key} shape: {out[key].shape}\")\n# print('inputs: ', out['input_ids'])\n# print('labels: ', out['labels'])\n\n# data_collator = DefaultDataCollator(tokenizer)\n# out = data_collator([shake_toked[\"test\"][i] for i in range(5)])\n# print(out)\n\n\ndef my_collate(examples, block_size: int, **kwargs):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\n\nexample = shake['test'][0]\n# concatenated_examples = {k: sum(example[k], []) for k in example.keys()}\nprint([example[k] for k in example.keys()])\n\n\nout = data_collator([shake_toked['test']['input_ids'][i] for i in range(4)])\nprint(out)\n\n\nfor i in range(4):\n    print([tokenizer.decode(x) for x in out['input_ids'][i]])\n\n\n\nDataloader\n\ntest_dl = DataLoader(\n    shake_toked['test']['input_ids'],\n    batch_size=128,\n    collate_fn=data_collator,\n    num_workers=0,\n)\n\n\n#!head ../data/text/tiny_shakespeare.txt\n\n\nb = next(iter(test_dl))\nprint(b['input_ids'].shape)\nprint(b['input_ids'][1])\nprint(b['labels'][1])\n# for i in range(128):\n#     print([tokenizer.decode(x) for x in b['input_ids'][i]])",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "text.datasets.html#wikitext-2",
    "href": "text.datasets.html#wikitext-2",
    "title": "Text datasets",
    "section": "Wikitext-2",
    "text": "Wikitext-2\n\nData source from Hugging Face\n\ndataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n\n\nprint(len(dataset), type(dataset), dataset)\nprint(dataset[100])\n\n\ndataset[100]\n\n\n\nData source from torchtext\nhttps://pytorch.org/tutorials/beginner/transformer_tutorial.html\n\n# train_iter = WikiText2(root='../data/text', split='test')\n# tokenizer = get_tokenizer('basic_english')\n\n\n# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['&lt;unk&gt;'])\n# vocab.set_default_index(vocab['&lt;unk&gt;'])\n# len(vocab)\n\n\n# vocab['the']\n\n\n# vocab(tokenizer('this is a test'))\n\n\n# # concatenate all sentences together\ndef data_process(raw_text_iter) -&gt; torch.Tensor:\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n    return torch.cat(tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\n\n# for idx, i in enumerate(train_iter):\n#     print(idx, i)\n#     print(vocab(tokenizer(i)))\n\n\n# train_iter, val_iter, test_iter = WikiText2()\n# train_data = data_process(train_iter)\n# val_data = data_process(val_iter)\n# test_data = data_process(test_iter)\n\n\ndef batchify(data: torch.Tensor, bsz: int) -&gt; torch.Tensor:\n    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Arguments:\n        data: Tensor, shape ``[N]``\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape ``[N // bsz, bsz]``\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data\n\n\n# test_data = batchify(test_data, 10)\n# print(test_data)\n\n\nbptt = 35\ndef get_batch(source: torch.Tensor, i: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        source: Tensor, shape ``[full_seq_len, batch_size]``\n        i: int\n\n    Returns:\n        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n        target has shape ``[seq_len * batch_size]``\n    \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len]\n    return data, target\n\n\n# x, y = get_batch(test_data, 0)\n# print(\"x: \", x[:2])\n# print(\"y: \", y[:2])\n\n\n\nWord-based tokenization\n\ntorchtext tokenizer\n\n# tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n# vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['&lt;unk&gt;'])\n# # vocab.set_default_index(vocab['&lt;unk&gt;'])\n# tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n# tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], \n# fn_kwargs={'tokenizer': tokenizer})\n# print(tokenized_dataset['train'][88]['tokens'])\n\n\n\nhugging face tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ntokens = tokenizer.tokenize(dataset[100]['text'])\nprint(tokens)\n\n\n\n\nNumericalization\n\ntorchtext\n\n# vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], \n# min_freq=3) \n# vocab.insert_token('&lt;unk&gt;', 0)           \n# vocab.insert_token('&lt;eos&gt;', 1)            \n# vocab.set_default_index(vocab['&lt;unk&gt;'])   \n# print(len(vocab))                         \n# print(vocab.get_itos()[:10])\n\n\n\nhugging face\n\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)\nprint(tokenizer.decode(ids))",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "text.datasets.html#hugging-face-for-lm-without-intermediary-steps",
    "href": "text.datasets.html#hugging-face-for-lm-without-intermediary-steps",
    "title": "Text datasets",
    "section": "Hugging Face for LM without intermediary steps",
    "text": "Hugging Face for LM without intermediary steps\nhttps://huggingface.co/course/chapter7/6?fw=pt\n\n# directly without intermediary steps\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\ntext = [\"this is a text.\", \"or so it seems\"]\npadded = tokenizer(text, max_length=4, truncation=True, return_length=True, return_overflowing_tokens=True)\n\nprint(padded)\nprint(padded.keys())\nprint([tokenizer.decode(x) for x in padded['input_ids']])\n\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\nout = data_collator(padded['input_ids'])\nfor key in out:\n    print(f\"{key} shape: {out[key].shape}\")\n\nprint(out['input_ids'], out['labels'])\n# Shifting the inputs and labels to align them happens inside the model, so the data collator just copies the inputs to create the labels.\n\n\nData loader\nConcatenate all data into one large string of text and then chunk it into context length chunks - https://towardsdatascience.com/language-modeling-with-lstms-in-pytorch-381a26badcbf - https://www.youtube.com/watch?v=ma1TrR7gE7I&t=273s\n\ndef get_data(dataset, vocab, batch_size):\n    data = []                                                   \n    for example in dataset:\n        if example['tokens']:                                      \n            tokens = example['tokens'].append('&lt;eos&gt;')             \n            tokens = [vocab[token] for token in example['tokens']] \n            data.extend(tokens)                                    \n    data = torch.LongTensor(data)                                 \n    num_batches = data.shape[0] // batch_size \n    data = data[:num_batches * batch_size]                       \n    data = data.view(batch_size, num_batches)          \n    return data\n\n\n# batch_size = 1024\n# train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n# valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n# test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n\n\n# print(tokenized_dataset.shape)\n# print(tokenized_dataset['train']['tokens'][88])",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "text.datasets.html#language-modeling-dataset",
    "href": "text.datasets.html#language-modeling-dataset",
    "title": "Text datasets",
    "section": "Language modeling dataset",
    "text": "Language modeling dataset\nBasically concatenate all data into one big array of ids and then create block_lengths inputs. shift for corresponding labels.\n\nprint(shake)\n\n\nTokenization\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\n\n\ndef tokenize_function(examples:List[dict[str,str]]) -&gt; dict[str, List[List[int]]]:\n    result = tokenizer(examples[\"text\"]) #, max_length=context_length, truncation=True, return_length=True, return_overflowing_tokens=True)\n    if tokenizer.is_fast:\n        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n    return result\n\n\ntokenized = shake.map(\n    tokenize_function, batched=True,\n    remove_columns=[\"text\"],\n    num_proc = 1\n)\n\n\nprint(tokenized['train'], type(tokenized['train']))\n\n\n\nSentences concatenation\n\nall = []\nfor k,v in tokenized.items():\n    print(k, v)\n    for x in v['input_ids']:\n        all += x\n    print(len(all))\nprint(all[:15])\n\n\n\nBatchify\n\ndef get_batch(data, batch_size, block_size):\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    return x, y\n\n\nx, y = get_batch(np.array(all), 16, 10)\nprint(x.shape, y.shape)\nprint(x[0], y[0])\nprint(tokenizer.decode(x[0]), tokenizer.decode(y[0]))",
    "crumbs": [
      "Text",
      "Data",
      "Text datasets"
    ]
  },
  {
    "objectID": "models.aligners.html",
    "href": "models.aligners.html",
    "title": "Aligners",
    "section": "",
    "text": "source\n\n\n\n AlignerWAV2VEC2 (text_normalizer, device='cuda')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Point (token_index:int, time_index:int, score:float)\n\n\nsource\n\n\n\n\n Segment (label:str, start:int, end:int, score:float)",
    "crumbs": [
      "Text",
      "Models",
      "Aligners"
    ]
  },
  {
    "objectID": "models.aligners.html#wav2vec2.0-aligner",
    "href": "models.aligners.html#wav2vec2.0-aligner",
    "title": "Aligners",
    "section": "",
    "text": "source\n\n\n\n AlignerWAV2VEC2 (text_normalizer, device='cuda')\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\n\n\n Point (token_index:int, time_index:int, score:float)\n\n\nsource\n\n\n\n\n Segment (label:str, start:int, end:int, score:float)",
    "crumbs": [
      "Text",
      "Models",
      "Aligners"
    ]
  },
  {
    "objectID": "models.aligners.html#usage",
    "href": "models.aligners.html#usage",
    "title": "Aligners",
    "section": "Usage",
    "text": "Usage\n\ntext_normalizer = TTSTextNormalizer().english_cleaners\naligner = AlignerWAV2VEC2(text_normalizer, device='cpu') # for CI on cpu\nwav_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.wav\"\ntxt_path = \"../data/en/LibriTTS/test-clean/1089/134686/1089_134686_000015_000001.original.txt\"\nwav, sr = torchaudio.load(wav_path)\nwith open(txt_path, 'r') as f: txt = f.read()\nalignments = aligner.get_alignments(wav, txt)",
    "crumbs": [
      "Text",
      "Models",
      "Aligners"
    ]
  },
  {
    "objectID": "text.normalizers.html",
    "href": "text.normalizers.html",
    "title": "Text Normalizers",
    "section": "",
    "text": "source\n\n\n\n TTSTextNormalizer (language='en')\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncleaner = TTSTextNormalizer()\nprint(cleaner.en_normalize_numbers(\"$350\"))\nprint(cleaner.expand_time_english(\"12:05pm\"))\nprint(cleaner(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))\nprint(cleaner([\"Oh my dear! this is $5 too soon...\", \"It's 1:04 am!\"]))\n\n\nsource\n\n\n\n\n Punctuation (puncs:str=';:,.!?¡¿—…\"«»“”')\n\n*Handle punctuations in text.\nJust strip punctuations from text or strip and restore them later.\nArgs: puncs (str): The punctuations to be processed. Defaults to _DEF_PUNCS.\nExample: &gt;&gt;&gt; punc = Punctuation() &gt;&gt;&gt; punc.strip(“This is. example !”) ‘This is example’\n&gt;&gt;&gt; text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n&gt;&gt;&gt; ' '.join(text_striped)\n'This is example'\n\n&gt;&gt;&gt; text_restored = punc.restore(text_striped, punc_map)\n&gt;&gt;&gt; text_restored[0]\n'This is. example !'*\n\nsource\n\n\n\n\n PuncPosition (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nEnum for the punctuations positions\n\npunc = Punctuation()\ntext = \"This is. This is, example!\"\nprint(punc.strip(text))\nsplit_text, puncs = punc.strip_to_restore(text)\nprint(split_text, \" ---- \", puncs)\nrestored_text = punc.restore(split_text, puncs)\nprint(restored_text)",
    "crumbs": [
      "Text",
      "Data",
      "Text Normalizers"
    ]
  },
  {
    "objectID": "text.normalizers.html#tts-cleaning-normalization",
    "href": "text.normalizers.html#tts-cleaning-normalization",
    "title": "Text Normalizers",
    "section": "",
    "text": "source\n\n\n\n TTSTextNormalizer (language='en')\n\nInitialize self. See help(type(self)) for accurate signature.\n\ncleaner = TTSTextNormalizer()\nprint(cleaner.en_normalize_numbers(\"$350\"))\nprint(cleaner.expand_time_english(\"12:05pm\"))\nprint(cleaner(\"Oh my dear! this is $5 too soon... It's 1:04 am!\"))\nprint(cleaner([\"Oh my dear! this is $5 too soon...\", \"It's 1:04 am!\"]))\n\n\nsource\n\n\n\n\n Punctuation (puncs:str=';:,.!?¡¿—…\"«»“”')\n\n*Handle punctuations in text.\nJust strip punctuations from text or strip and restore them later.\nArgs: puncs (str): The punctuations to be processed. Defaults to _DEF_PUNCS.\nExample: &gt;&gt;&gt; punc = Punctuation() &gt;&gt;&gt; punc.strip(“This is. example !”) ‘This is example’\n&gt;&gt;&gt; text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")\n&gt;&gt;&gt; ' '.join(text_striped)\n'This is example'\n\n&gt;&gt;&gt; text_restored = punc.restore(text_striped, punc_map)\n&gt;&gt;&gt; text_restored[0]\n'This is. example !'*\n\nsource\n\n\n\n\n PuncPosition (value, names=None, module=None, qualname=None, type=None,\n               start=1)\n\nEnum for the punctuations positions\n\npunc = Punctuation()\ntext = \"This is. This is, example!\"\nprint(punc.strip(text))\nsplit_text, puncs = punc.strip_to_restore(text)\nprint(split_text, \" ---- \", puncs)\nrestored_text = punc.restore(split_text, puncs)\nprint(restored_text)",
    "crumbs": [
      "Text",
      "Data",
      "Text Normalizers"
    ]
  },
  {
    "objectID": "models.ngram.html",
    "href": "models.ngram.html",
    "title": "N-gram Language Models",
    "section": "",
    "text": "https://github.com/karpathy/makemore",
    "crumbs": [
      "Text",
      "Models",
      "N-gram Language Models"
    ]
  },
  {
    "objectID": "models.ngram.html#unigram",
    "href": "models.ngram.html#unigram",
    "title": "N-gram Language Models",
    "section": "Unigram",
    "text": "Unigram\n\nUsage\n\n# without pandas\nwith open('../data/text/names.txt', 'r') as f:\n    list_of_words = f.read().splitlines()\n# with pandas\ndf = pd.read_csv('../data/text/names.txt', names=['name'], header=None)\nlist_of_words = list(df.head().name)\n\nunigram = CharUnigram(list_of_words)\nprint(\"sorted counts: \", unigram.counts)\nprint(\"sorted probs: \", unigram.probs)\nprint(len(unigram))\nprint(unigram.chars)\nprint(unigram._stoi)\nprint(unigram.stoi('a'))\nprint(unigram.itos(0))\n\n\ndf = pd.DataFrame.from_dict(unigram.counts, orient='index')\ndf.plot(kind='bar')\n\n\nsamples = []\nfor i in range(10000):\n    s = unigram.sample()\n    samples.append(s)\n\n# sampled\ncount = Counter([c for w in samples for c in w])\ndf = pd.DataFrame.from_dict(count, orient='index')\ndf[0].sort_values(ascending=False).plot(kind='bar')",
    "crumbs": [
      "Text",
      "Models",
      "N-gram Language Models"
    ]
  },
  {
    "objectID": "models.ngram.html#bigram",
    "href": "models.ngram.html#bigram",
    "title": "N-gram Language Models",
    "section": "Bigram",
    "text": "Bigram\n\nclass CharBigram():\n    def __init__(self):\n        pass\n\n\nUsage\n\n# data\nwith open('../data/text/names.txt', 'r') as f:\n    data = f.read().splitlines()\nprint(\"first lines of text: \", data[:10])\n\n# data = [\"this is a text\"]\n\n\n# bigram counts\nbigrams = {}\nunique_tokens = set()\nfor name in data:\n    line = list(name)\n    unique_tokens.update(line)\n    line.append('&lt;stop&gt;')\n    line.insert(0, '&lt;stop&gt;')\n    for i,v in enumerate(range(len(line)-1)):\n        bigram = (line[i], line[i+1])\n        if bigram in bigrams:\n            bigrams[bigram] += 1\n        else:\n            bigrams[bigram] = 1\n\n# print(\"unsorted: \", list(bigrams)[:10])\n# print(\"sorted: \", sort_dict_by_value(bigrams))\n\n\n\nNumericalization\n\ntokens = sorted(unique_tokens)\n# use same for start & stop in this case (separate lines of names)\n# tokens.append('&lt;start&gt;')\ntokens.append('&lt;stop&gt;')\nprint(tokens)\nstoi = {v:i for i,v in enumerate(tokens)}\nitos = {i:v for i, v in enumerate(tokens)}\nprint(stoi, itos)\n\n\n\nMatrix representation\n\nn_toks = len(tokens)\nprint(n_toks)\nN = torch.zeros((n_toks, n_toks)).long()\nprint(N.shape)\n\n\nfor bigram, value in bigrams.items():\n    idx1, idx2 = stoi[bigram[0]], stoi[bigram[1]]\n    N[idx1, idx2] = value\n\nplt.xlabel('char_t+1')\nplt.ylabel('char_t')\ni = [i for i, v in itos.items()]\nv = [v for i,v in itos.items()]\nplt.xticks(i, v)\nplt.yticks(i, v)\nplt.imshow(N, origin='lower')\n\n\n\nFrom counts to probabilities\n\nprint(N)\n\n\n# smoothing avoids having log(0) = inf when computing NLL loss\nsmoothing = 1\nP = (N.float()+smoothing) / N.sum(1,keepdim=True)\nplt.imshow(P, origin='lower')\n\n\nrow_6 = (N[6,:]/N[6,:].sum())\nprint(row_6)\nprint(row_6.sum())\n\n\np = P[6, :]\nprint(p.sum(), p.max(), torch.argmax(p))\n\n\n\nSampling\n\nfor i in range(10):\n    res = []\n    prev = stoi['&lt;stop&gt;']\n    while True:\n        # max prob sampling\n        next = int(torch.argmax(P[prev, :]))\n        # multinomial sampling\n        next = int(torch.multinomial(P[prev,:],num_samples=1,replacement=True))\n        if next == stoi['&lt;stop&gt;']:\n            print(''.join(res))\n            break\n        else:\n            res.append(itos[next])\n            prev = next\n\n\n\nLog likelihood loss function\n\nbigram_p = {}\nfor bigram, value in bigrams.items():\n    idx1, idx2 = stoi[bigram[0]], stoi[bigram[1]]\n    bigram_p[bigram] = P[idx1,idx2]\n\nprint(bigram_p)\n\n\nbigram_p_sorted = {k: v.float() for k, v in sorted(bigram_p.items(), reverse=True, key=lambda x: x[1])}\nprint(bigram_p_sorted)\n\n\n# likelihood of full corpus = product of all bigram prods\nl = 0\nfor bigram, prob in bigram_p_sorted.items():\n    l += torch.log(prob)\n\n# negative log likelihood loss nll\nnll = -l /len(bigram_p_sorted)\nprint(nll)\n\n\n\nGenerate training data\n\nword = \"this\"\nsample = [(word[i], word[i+1]) for i,c in enumerate(word) if i &lt; len(word)-1]\nprint(list(zip(*sample)))\n\n\nxs, ys = [], []\nfor word in data:\n    sample = [(stoi[word[i]], stoi[word[i+1]]) for i,c in enumerate(word) if i &lt; len(word)-1]\n    x, y = list(zip(*sample)) # inverse of zip\n    xs.append(torch.tensor(x))\n    ys.append(torch.tensor(y))\n\n\nprint('x:', xs[:3])\nprint('y', ys[:3])\n\n\n\n1-hot encoded input\n\nenc = [F.one_hot(x, num_classes=len(tokens)).float() for x in xs]\nprint(enc[:3])\n\n\nplt.imshow(enc[0])\n\n\nX = enc[0]\nprint(X.shape)\n\n\n\n‘Neural net’ modeling\nwe model the transition probability matrix by neural net activations\n\nW = torch.randn(27, 27)\n\n\nlogits = X @ W\ncounts = logits.exp()\nprobs = counts / counts.sum(1, keepdims=True)\nprint(probs)",
    "crumbs": [
      "Text",
      "Models",
      "N-gram Language Models"
    ]
  },
  {
    "objectID": "models.ngram.html#kenlm",
    "href": "models.ngram.html#kenlm",
    "title": "N-gram Language Models",
    "section": "KenLM",
    "text": "KenLM\nWe refer to efficient kenlm implementation for larger n-gram models usable for production\n\nPreprocess data into kenlm format\ntokens separated by space with new sentence at each line\n\ndf = pd.read_csv('../data/text/names.txt', header=None, names=['name']) \ndf = df.name.apply(lambda x: list(x)) # str into list of char\n# df.apply(lambda x: x.append('&lt;eos&gt;')) # if eos needed\nprint(df.head())\ndf_toks = df.str.join(' ') # for kenlm input format tokens are separated by space\nprint(df_toks.head())\n\n\nUnique tokens\n\ndf.head()\n# for row in df.iterrows():\n#     print(row)\ntokens = set()\nfor k,v in df.items():\n    tokens.update(list(v))\n\nprint(tokens)\nlen(tokens)\n\n\n\nSave data to kenlm format for training\n\ndata_file = df.to_csv('../data/text/names.kenlm.txt', header=None, index=None)\n! bzip2 -kz ../data/text/names.kenlm.txt\n! bzcat ../data/text/names.kenlm.txt.bz2 | head\n\n\n\n\nTrain KenLM n-gram model\nhttps://lukesalamone.github.io/posts/running-simple-language-model/\nKenLM requires data to be one sentence per line lowercase\n\n# ! if [ ! -f \"../data/text/names.2gram.arpa\" ]; then lmplz --discount_fallback -o 2 &lt; ../data/text/names.kenlm.txt.bz2&gt;../data/text/names.2gram.arpa; fi\n\n\n# ! if [ ! -f \"../data/text/names.2gram.kenlm\" ]; then build_binary ../data/text/names.2gram.arpa ../data/text/names.2gram.kenlm; fi\n\n\nTest original Kenlm python api probs\n\nmodel = kenlm.LanguageModel('../data/text/names.2gram.kenlm')\nsentence = \"emma\"\ntokenized = \"e m m a\"\n# model.score(\"emma\", bos = False, eos = False)\nwords = ['&lt;s&gt;'] + list(sentence) + ['&lt;/s&gt;']\nprint(words)\nfinal = 0\nfor i, (prob, length, oov) in enumerate(model.full_scores(tokenized)):\n    print(f'words: {words[i:i+length]} index:{i}, prob:{prob}, length:{length}, oov:{oov}')\n    final += prob\n\nprint(final)\nprint(model.score(\"e m m a\"))\nprint(f'prob &lt;s&gt; e: {model.score(\"e\", bos=True, eos=False)}')\nprint(f'prob e: {model.score(\"e\", bos=False, eos=False)}')\nprint(f'prob &lt;s&gt; e m: {model.score(\"e m\", bos=True, eos=False)}')\nprint(f'prob e m: {model.score(\"e m\", bos=False, eos=False)}')\nstate = kenlm.State()\nstate2 = kenlm.State()\nmodel.BeginSentenceWrite(state)\naccum = 0\naccum += model.BaseScore(state, \"e\", state2)\nprint(f'prob &lt;s&gt; e: {accum}')\nstate, state2 = state2, state\naccum += model.BaseScore(state, \"m\", state2)\nprint(f'prob &lt;s&gt; e m: {accum}')\n\n\n\nDefine LM vocabulary\n\n# add special tokens to vocabulary\ntokens.add('&lt;s&gt;')\ntokens.add('&lt;/s&gt;')\ntokens.add('&lt;unk&gt;')\nprint(tokens, len(tokens))\nvocab = list(tokens)\n\n\n\n\nInference / Sampling from prob distributions\n\nlm = KenLM('../data/text/names.2gram.kenlm', vocab)\ninit_char = '&lt;s&gt; e m m'\n# probs = lm.nbest(len(vocab), log_prob=False)\n# print(np.sum([p for char, p in probs]))\n# res = [init_char]\n# next = int(torch.multinomial(P[prev,:],num_samples=1,replacement=True))\nfor i in range(50):\n    lm.new_sentence_init()\n    lm.append(init_char)\n    while True:\n        # nbest probs at current state\n        probs = lm.nbest(len(vocab), log_prob=False)\n        # print(probs)\n        # print(np.sum(probs))\n        # sample from prob distribution\n        try:\n            index_next = int(torch.multinomial(torch.tensor([prob for char, prob in probs]),num_samples=1,replacement=True))\n        except:\n            print(\"probs too small\")\n            break\n        char_next = probs[index_next][0]\n        lm.append(char_next)\n        # print(init_char + '&lt;s&gt;')\n        if char_next == '&lt;/s&gt;' or char_next == '&lt;s&gt;' and lm.text != init_char and (lm.text != init_char+' &lt;s&gt;'):\n            print(lm.text.replace(' ', ''))\n            break",
    "crumbs": [
      "Text",
      "Models",
      "N-gram Language Models"
    ]
  },
  {
    "objectID": "models.unet.html",
    "href": "models.unet.html",
    "title": "U-Net",
    "section": "",
    "text": "source\n\n\n\n TinyUnet (n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024],\n           activation=functools.partial(&lt;class\n           'torch.nn.modules.activation.LeakyReLU'&gt;, negative_slope=0.1),\n           normalization=&lt;class 'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n           pre_activation:bool=False, weight_initialization=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[3, 32, 64, 128, 256, 512, 1024]\nNumber of features in each layer\n\n\nactivation\npartial\nfunctools.partial(&lt;class ‘torch.nn.modules.activation.LeakyReLU’&gt;, negative_slope=0.1)\nActivation function\n\n\nnormalization\ntype\nBatchNorm2d\nNormalization function\n\n\npre_activation\nbool\nFalse\nuse Resblock with pre-activation\n\n\nweight_initialization\nbool\nFalse\n\n\n\n\n\nsource\n\n\n\n\n zero_weights (layer)\n\n\nsource\n\n\n\n\n init_weights (m, leaky=0.0)\n\n\nmodel = TinyUnet(n_features=[3, 16, 32], pre_activation=True)\nmodel_init = TinyUnet(n_features=[3, 16, 32], pre_activation=True, weight_initialization=True)\nx = torch.randn(1, 3, 64, 64)\nmodel(x).shape\nfig, ax = plt.subplots(1, 3, figsize=(10, 10), tight_layout=True)\nax[0].imshow(x.squeeze().permute(1,2,0), cmap='gray')\nax[1].imshow(model(x).squeeze().permute(1,2,0).detach(), cmap='gray')\nax[2].imshow(model_init(x).squeeze().permute(1,2,0).detach(), cmap='gray')\nplt.show()\n\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] INFO - Init conv & linear with kaiming\n[14:31:43] INFO - LeakyRelu layers weight init\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n TinyUnetX (nnet:__main__.TinyUnet,\n            optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n            scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nTinyUnet\n\nsuper res autoencoder neural net\n\n\noptimizer\nCallable\n\noptimizer partial\n\n\nscheduler\nOptional\nNone\nscheduler partial\n\n\n\n\ncfg = OmegaConf.load('../config/model/image/tinyunetx.yaml')\nmodel = instantiate(cfg.nnet)\nx = torch.randn(1, 3, 64, 64)\nmodel(x).shape\n\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([1, 3, 64, 64])"
  },
  {
    "objectID": "models.unet.html#tiny-unet",
    "href": "models.unet.html#tiny-unet",
    "title": "U-Net",
    "section": "",
    "text": "source\n\n\n\n TinyUnet (n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024],\n           activation=functools.partial(&lt;class\n           'torch.nn.modules.activation.LeakyReLU'&gt;, negative_slope=0.1),\n           normalization=&lt;class 'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n           pre_activation:bool=False, weight_initialization=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[3, 32, 64, 128, 256, 512, 1024]\nNumber of features in each layer\n\n\nactivation\npartial\nfunctools.partial(&lt;class ‘torch.nn.modules.activation.LeakyReLU’&gt;, negative_slope=0.1)\nActivation function\n\n\nnormalization\ntype\nBatchNorm2d\nNormalization function\n\n\npre_activation\nbool\nFalse\nuse Resblock with pre-activation\n\n\nweight_initialization\nbool\nFalse\n\n\n\n\n\nsource\n\n\n\n\n zero_weights (layer)\n\n\nsource\n\n\n\n\n init_weights (m, leaky=0.0)\n\n\nmodel = TinyUnet(n_features=[3, 16, 32], pre_activation=True)\nmodel_init = TinyUnet(n_features=[3, 16, 32], pre_activation=True, weight_initialization=True)\nx = torch.randn(1, 3, 64, 64)\nmodel(x).shape\nfig, ax = plt.subplots(1, 3, figsize=(10, 10), tight_layout=True)\nax[0].imshow(x.squeeze().permute(1,2,0), cmap='gray')\nax[1].imshow(model(x).squeeze().permute(1,2,0).detach(), cmap='gray')\nax[2].imshow(model_init(x).squeeze().permute(1,2,0).detach(), cmap='gray')\nplt.show()\n\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] WARNING - setting conv bias back to False as Batchnorm is used\n[14:31:43] INFO - Init conv & linear with kaiming\n[14:31:43] INFO - LeakyRelu layers weight init\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n TinyUnetX (nnet:__main__.TinyUnet,\n            optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n            scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nTinyUnet\n\nsuper res autoencoder neural net\n\n\noptimizer\nCallable\n\noptimizer partial\n\n\nscheduler\nOptional\nNone\nscheduler partial\n\n\n\n\ncfg = OmegaConf.load('../config/model/image/tinyunetx.yaml')\nmodel = instantiate(cfg.nnet)\nx = torch.randn(1, 3, 64, 64)\nmodel(x).shape\n\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:42] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n[17:43:43] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([1, 3, 64, 64])"
  },
  {
    "objectID": "docker.html",
    "href": "docker.html",
    "title": "Docker containerization",
    "section": "",
    "text": "Check .devcontainer/Dockerfile\nInstalls basic dependencies for nimrod to work seamlessly\n\n\nbuilds nimrod docker image with automatic tag ‘:latest’\nmake nimrod\n\n\n\nUpload your image to https://hub.docker.com for pulling from cloud instances\nmake push\n\n\n\nimage_name=\"nimrod\"\n./run_docker_locally.sh ${image_name}"
  },
  {
    "objectID": "docker.html#base-image",
    "href": "docker.html#base-image",
    "title": "Docker containerization",
    "section": "",
    "text": "Check .devcontainer/Dockerfile\nInstalls basic dependencies for nimrod to work seamlessly\n\n\nbuilds nimrod docker image with automatic tag ‘:latest’\nmake nimrod\n\n\n\nUpload your image to https://hub.docker.com for pulling from cloud instances\nmake push\n\n\n\nimage_name=\"nimrod\"\n./run_docker_locally.sh ${image_name}"
  },
  {
    "objectID": "docker.html#docker-compose",
    "href": "docker.html#docker-compose",
    "title": "Docker containerization",
    "section": "Docker compose",
    "text": "Docker compose\ncheck .devcontainer/docker-compose.yml\nThis defines port sharing, mounts volumes and read api secrets from file\n\nUsage\n# start services\ndocker-compose up\n# stop services\ndocker-compose down"
  },
  {
    "objectID": "audio.features.html",
    "href": "audio.features.html",
    "title": "Audio features",
    "section": "",
    "text": "source\n\n\n\n MelSpecgram (sampling_rate:int=22050, win_length:int=1024,\n              hop_length:int=256, n_fft:int=1024, f_min:float=0.0,\n              f_max:float=8000.0, n_mels:int=80, power:float=1.0,\n              mel_scale:str='slaney', normalized:bool=True, _target_=None)\n\n*MelSpecgram\nuse torchaudio mel computation and add clamping & scaling to mimic taco2*\n\n\n\n\ncfg = OmegaConf.load(\"../config/audio/features/mel_spectrogram.yaml\")\ncfg.sampling_rate = 16000\nprint(type(cfg), cfg)\nwav, sr = torchaudio.load(\"../data/audio/obama.wav\")\nprint(wav.shape,sr)\n\n\nmel_spectrogramer = MelSpecgram(**cfg)\n# mel_spectrogramer = instantiate(cfg)\nprint(mel_spectrogramer)\nmels = mel_spectrogramer(wav)\nprint(mels.shape)\n\n\nplt.imshow(mels.squeeze(0))"
  },
  {
    "objectID": "audio.features.html#mel-spectrogram",
    "href": "audio.features.html#mel-spectrogram",
    "title": "Audio features",
    "section": "",
    "text": "source\n\n\n\n MelSpecgram (sampling_rate:int=22050, win_length:int=1024,\n              hop_length:int=256, n_fft:int=1024, f_min:float=0.0,\n              f_max:float=8000.0, n_mels:int=80, power:float=1.0,\n              mel_scale:str='slaney', normalized:bool=True, _target_=None)\n\n*MelSpecgram\nuse torchaudio mel computation and add clamping & scaling to mimic taco2*\n\n\n\n\ncfg = OmegaConf.load(\"../config/audio/features/mel_spectrogram.yaml\")\ncfg.sampling_rate = 16000\nprint(type(cfg), cfg)\nwav, sr = torchaudio.load(\"../data/audio/obama.wav\")\nprint(wav.shape,sr)\n\n\nmel_spectrogramer = MelSpecgram(**cfg)\n# mel_spectrogramer = instantiate(cfg)\nprint(mel_spectrogramer)\nmels = mel_spectrogramer(wav)\nprint(mels.shape)\n\n\nplt.imshow(mels.squeeze(0))"
  },
  {
    "objectID": "image.blip.html",
    "href": "image.blip.html",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "",
    "text": "https://github.com/salesforce/BLIP\n\nCopyright (c) 2022, salesforce.com, inc.\nAll rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\nFor full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\nBy Junnan Li"
  },
  {
    "objectID": "image.blip.html#references",
    "href": "image.blip.html#references",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "",
    "text": "https://github.com/salesforce/BLIP\n\nCopyright (c) 2022, salesforce.com, inc.\nAll rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\nFor full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\nBy Junnan Li"
  },
  {
    "objectID": "image.blip.html#imports",
    "href": "image.blip.html#imports",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "Imports",
    "text": "Imports"
  },
  {
    "objectID": "image.blip.html#base",
    "href": "image.blip.html#base",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "Base",
    "text": "Base"
  },
  {
    "objectID": "image.blip.html#decoder",
    "href": "image.blip.html#decoder",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "Decoder",
    "text": "Decoder"
  },
  {
    "objectID": "image.blip.html#helpers",
    "href": "image.blip.html#helpers",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "Helpers",
    "text": "Helpers"
  },
  {
    "objectID": "image.blip.html#usage",
    "href": "image.blip.html#usage",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "Usage",
    "text": "Usage\n\ndevice = get_device()\nprint(\"DEVICE: \", device)\n\ndef load_demo_image(image_size,device):\n    img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')   \n\n    w,h = raw_image.size\n    display(raw_image.resize((w//5,h//5)))\n    \n    transform = transforms.Compose([\n        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n        transforms.ToTensor(),\n        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ]) \n    image = transform(raw_image).unsqueeze(0).to(device)   \n    return image\n\nDEVICE:  mps\n\n\n\n# # from nimrod.image.blip import blip_decoder\n# image_size = 384\n# image = load_demo_image(image_size=image_size, device=device)\n\n# model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth'\n    \n# model = blip_decoder(pretrained=model_url, image_size=image_size, vit='base')\n# model.eval()\n# model = model.to(device)\n\n# with torch.no_grad():\n#     # beam search\n#     caption = model.generate(image, sample=False, num_beams=3, max_length=20, min_length=5) \n#     # nucleus sampling\n#     # caption = model.generate(image, sample=True, top_p=0.9, max_length=20, min_length=5) \n#     print('caption: '+caption[0])"
  },
  {
    "objectID": "image.blip.html#hf-transformer-version",
    "href": "image.blip.html#hf-transformer-version",
    "title": "BLIP: Bootstrapping Language-Image Pre-training",
    "section": "HF Transformer version",
    "text": "HF Transformer version\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nmodel = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n\n# Load an image\n# image = Image.open(\"path_to_your_image.jpg\")\nimg_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \nimage = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# Conditional image captioning\ntext = \"a photography of\"\ninputs = processor(image, text=text, return_tensors=\"pt\")\n\n\noutputs = model.generate(**inputs)\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Caption:\", caption)\ndisplay(image)\n\n\n# unConditional image captioning\ninputs = processor(image, return_tensors=\"pt\")\n\noutputs = model.generate(**inputs)\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Caption:\", caption)\n\nGenerated Caption: a woman sitting on the beach with her dog\n\n\n\n# Load an image\n# image = Image.open(\"path_to_your_image.jpg\")\n\n# img_url = \"https://images.artnet.com/aoa_lot_images/141282/shepard-fairey-kurt-cobain-endless-nameless-prints-and-multiples-zoom_374_500.jpg\"\nimg_url = \"https://images.artnet.com/aoa_lot_images/141287/hiroshi-sugimoto-gulf-of-bothnia-photographs.jpeg\"\nimage = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n\n# Conditional image captioning\ntext = \"a painting of\"\ninputs = processor(image, text=text, return_tensors=\"pt\")\noutputs = model.generate(**inputs)\ncaption = processor.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Caption:\", caption)\ndisplay(image)\n\nGenerated Caption: a painting of a body of water in the middle of the ocean"
  },
  {
    "objectID": "audio.utils.html",
    "href": "audio.utils.html",
    "title": "Audio Utilities",
    "section": "",
    "text": "source\n\nplot_waveform\n\n plot_waveform (waveform, sample_rate, title='Waveform', xlim=None,\n                ylim=None)\n\n\nwav, sr = torchaudio.load(\"../data/audio/obama.wav\")\nplot_waveform(wav,sr)",
    "crumbs": [
      "Audio",
      "Utils",
      "Audio Utilities"
    ]
  },
  {
    "objectID": "models.diffusion.html",
    "href": "models.diffusion.html",
    "title": "Diffusion Model",
    "section": "",
    "text": "NOISE_TIME_STEPS = 10\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=NOISE_TIME_STEPS,\n    beta_schedule=\"squaredcos_cap_v2\" # beter for small img sizes\n    )\n\n\n\n\nB, C, H, W = 100, 3, 32, 32\nxb = torch.zeros((B,C,H,W))\nprint(f\"xb: {xb.shape}\")\nnoise = torch.randn(xb.shape)\nprint(f\"noise: {noise.shape}\")\ntime_steps = torch.linspace(0, NOISE_TIME_STEPS-1, B).long()\n# time_steps = torch.randint(0, NOISE_TIME_STEPS, (B,))\nprint(f\"time_steps: {time_steps.shape}\")\nnoisy_images = noise_scheduler.add_noise(xb, noise, time_steps)\nprint(f\"noisy_images: {noisy_images.shape}\")\nshow_images(noisy_images, ncols=10)\n\nxb: torch.Size([100, 3, 32, 32])\nnoise: torch.Size([100, 3, 32, 32])\ntime_steps: torch.Size([100])\nnoisy_images: torch.Size([100, 3, 32, 32])"
  },
  {
    "objectID": "models.diffusion.html#noise-scheduler",
    "href": "models.diffusion.html#noise-scheduler",
    "title": "Diffusion Model",
    "section": "",
    "text": "NOISE_TIME_STEPS = 10\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=NOISE_TIME_STEPS,\n    beta_schedule=\"squaredcos_cap_v2\" # beter for small img sizes\n    )\n\n\n\n\nB, C, H, W = 100, 3, 32, 32\nxb = torch.zeros((B,C,H,W))\nprint(f\"xb: {xb.shape}\")\nnoise = torch.randn(xb.shape)\nprint(f\"noise: {noise.shape}\")\ntime_steps = torch.linspace(0, NOISE_TIME_STEPS-1, B).long()\n# time_steps = torch.randint(0, NOISE_TIME_STEPS, (B,))\nprint(f\"time_steps: {time_steps.shape}\")\nnoisy_images = noise_scheduler.add_noise(xb, noise, time_steps)\nprint(f\"noisy_images: {noisy_images.shape}\")\nshow_images(noisy_images, ncols=10)\n\nxb: torch.Size([100, 3, 32, 32])\nnoise: torch.Size([100, 3, 32, 32])\ntime_steps: torch.Size([100])\nnoisy_images: torch.Size([100, 3, 32, 32])"
  },
  {
    "objectID": "models.diffusion.html#u-net-noise-prediction",
    "href": "models.diffusion.html#u-net-noise-prediction",
    "title": "Diffusion Model",
    "section": "U-Net: Noise prediction",
    "text": "U-Net: Noise prediction\n\nB,C,H,W = xb.shape\n\nmodel = UNet2DModel(\n    sample_size=H,\n    in_channels=C, \n    out_channels=C, \n    block_out_channels=(32, 64, 128, 256)\n    )\n\n\nUsage\n\nprint(noisy_images.shape, time_steps.shape)\nwith torch.no_grad():\n    noise_pred = model(noisy_images, time_steps).sample\n\nshow_images(noise_pred, ncols=10)\n\ntorch.Size([100, 3, 32, 32]) torch.Size([100])"
  },
  {
    "objectID": "models.diffusion.html#training",
    "href": "models.diffusion.html#training",
    "title": "Diffusion Model",
    "section": "Training",
    "text": "Training\n\nData\n\nBATCH_SIZE = 512\n\ncfg = OmegaConf.load('../config/data/image/mnist.yaml')\ntfs = transforms.Compose(\n    [transforms.ToImage(),\n    transforms.Resize(32),\n    # transforms.Normalize((0.1307,), (0.3081,))\n    ]\n    )\n\ndm = instantiate(cfg, batch_size=BATCH_SIZE, transforms=tfs, num_workers=20)\ndm.prepare_data()\ndm.setup()\n\n# check properties\nprint(f\"num_classes: {dm.num_classes}, batch_size: {dm.batch_size}\")\nprint(f\"labels: {dm.label_names}\")\n\n# sample\nx, y = dm.test_ds[0]\nprint(f\"X: {x.shape}, Y: {y}\")\n\n# batch\nxb,yb = next(iter(dm.train_dataloader()))\nprint(f\"X: {xb.shape}, Y: {yb.shape}\")\nB,C,H,W = xb.shape\ndm.show(0)\n\n[22:28:16] INFO - Init ImageDataModule for mnist\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transforms' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transforms'])`.\n[22:28:18] INFO - loading dataset mnist with args () from split train\n[22:28:18] INFO - loading dataset mnist from split train\nOverwrite dataset info from restored data version if exists.\n[22:28:19] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:19] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\nFound cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n[22:28:19] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:19] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:23] INFO - loading dataset mnist with args () from split test\n[22:28:23] INFO - loading dataset mnist from split test\nOverwrite dataset info from restored data version if exists.\n[22:28:24] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:24] INFO - Loading Dataset info from ../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\nFound cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\n[22:28:24] INFO - Found cached dataset mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:24] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/mnist/mnist/0.0.0/77f3279092a1c1579b2250db8eafed0ad422088c\n[22:28:25] INFO - split train into train/val [0.8, 0.2]\n[22:28:25] INFO - train: 48000 val: 12000, test: 10000\n\n\nnum_classes: 10, batch_size: 512\nlabels: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nX: torch.Size([1, 32, 32]), Y: 7\nX: torch.Size([512, 1, 32, 32]), Y: torch.Size([512])\n\n\n\n\n\n\n\n\n\n\n\nNoise Scheduler / “noisifier”\n\nNOISE_TIME_STEPS = 100\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=NOISE_TIME_STEPS,\n    beta_schedule=\"squaredcos_cap_v2\" # beter for small img sizes\n    )\n\nB,C,H,W = xb.shape\n\nmodel = UNet2DModel(\n    sample_size=H,\n    in_channels=C, \n    out_channels=C, \n    block_out_channels=(32, 64, 128, 256)\n    )\n# generate gaussian noise images of shape (B, C, H, W)\nnoise = torch.randn(xb.shape)\n# generate different time steps for each image in batch\ntime_steps = torch.linspace(0, NOISE_TIME_STEPS-1, B).long()\n# generate more or less noise from scheduler depending on time step\nnoisy_images = noise_scheduler.add_noise(xb, noise, time_steps)\nshow_images(noisy_images, ncols=64)\n\n\n\n\n\n\n\n\n\n# predict noise from noisy image at time_step\nwith torch.no_grad():\n    noise_pred = model(noisy_images, time_steps).sample\n\nshow_images(noise_pred, ncols=32)\n\n\n\n\n\n\n\n\n\n\nLR scheduler & optimizer\n\ndevice = get_device()\nprint(f\"device: {device}\")\n\nNOISE_TIME_STEPS = 1000\nNUM_EPOCHS = 1\nlr = 1e-4\nlr_warmup_steps = 500\n# total steps =number of batches * num_epochs\nnum_training_steps = len(dm.train_dataloader()) * NUM_EPOCHS\nprint(f\"num_training_steps: {num_training_steps}\")\n\nmodel = UNet2DModel(\n    sample_size=H,\n    in_channels=C,\n    out_channels=C,\n    block_out_channels=(32, 64, 128, 256)\n    )\n\nmodel.to(device)\n\nnoise_scheduler = DDPMScheduler(num_train_timesteps=NOISE_TIME_STEPS)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n\ncriterion = nn.MSELoss()\n\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=lr_warmup_steps,\n    num_training_steps=num_training_steps\n)\n\n[22:30:10] INFO - Using device: cuda\n\n\ndevice: cuda\nnum_training_steps: 94\n\n\n\n\nTraining Loop\n\ntrain_step_losses = []\nval_step_losses = []\nlrs = []\n\nfor epoch in range(NUM_EPOCHS):\n    i = 0\n    model.train()\n    n_steps, total_loss = 0, 0\n\n    for step, (images, labels) in tqdm(enumerate(dm.train_dataloader()), total=len(dm.train_dataloader()), leave=False):\n        optimizer.zero_grad()\n        images, labels = images.to(device), labels.to(device)\n        B, C, H, W = images.shape\n        # sample noise\n        noise = torch.randn(images.shape).to(device)\n        # sample random timesteps\n        # timesteps = torch.randint(0, NOISE_TIME_STEPS, (images.shape[0],)).to(device)\n        timesteps = torch.randint(0, NOISE_TIME_STEPS, (B,)).to(device)\n        # create noisy image at timestep with noise scheduler\n        noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n\n        # train model to predict noise\n        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n        # output should be as close to input as possible\n        loss = criterion(noise_pred, noise)\n        n_steps += len(images)\n        total_loss += (loss.item() * len(images))\n        train_step_losses.append(loss.item())\n        current_lr = optimizer.param_groups[0]['lr']\n        lrs.append(current_lr)\n        # logger.info(f\"loss.item(): {loss.item()}, len(images): {len(images)}\")\n        # logger.info(f\"Epoch: {epoch}, step: {step}, n_steps: {n_steps}, loss: {loss.item()}\")\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n\n    logger.info(f\"Epoch: {epoch} train_loss: {total_loss / n_steps}\")\n\n    model.eval()\n    total_loss, n_steps = 0, 0\n    for step, (images, labels) in tqdm(enumerate(dm.val_dataloader()), total=len(dm.val_dataloader()), leave=False):\n        optimizer.zero_grad()\n        images, labels = images.to(device), labels.to(device)\n        # sample noise\n        noise = torch.randn(images.shape).to(device)\n        # sample random timesteps\n        timesteps = torch.randint(0, NOISE_TIME_STEPS, (images.shape[0],)).to(device)\n        # create noisy image at timestep with noise scheduler\n        noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n        # train model to predict noise\n        outputs = model(noisy_images, timesteps, return_dict=False)[0]\n        # output should be as close to input as possible\n\n        loss = criterion(outputs, noise)\n        val_step_losses.append(loss.item())\n        n_steps += len(images) # bs\n        total_loss += loss.item() * len(images)\n        # logger.info(f\"Epoch: {epoch}, step: {step}, n_steps: {n_steps}, loss: {loss.item()}\")\n    logger.info(f\"Epoch: {epoch} val_loss: {total_loss / n_steps}\")\n\n\n\n\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n  return F.conv2d(input, weight, bias, self.stride,\n[22:15:30] INFO - Epoch: 0 train_loss: 0.7241547261873881\n\n\n\n\n\n[22:15:32] INFO - Epoch: 0 val_loss: 0.3778018388748169\n\n\n\nfig, axs = plt.subplots(1,2)\naxs[0].plot(train_step_losses)\naxs[0].set_title(\"train loss\")\naxs[1].plot(val_step_losses)\naxs[1].set_title(\"val loss\")\n# axs[2].plot(lrs)\n# axs[2].set_title(\"learning rate\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTest Image Generation\n\n# Using pre-defined HF pipeline\npipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler).to(device)\nimg_gen = pipeline(batch_size=8)\nmake_grid(img_gen.images)\n\n\n\n\n\n\n\n\n\n\n\n\nimg_gen.images[0]\n\n\n\n\n\n\n\n\n\n# use our own sampler based on noise_scheduler\n\n@time_it\ndef generate_image(model, images_shape, device, noise_scheduler):\n    # start with noise images (B,C,H,W)\n    sample = torch.randn(images_shape).to(device)\n    for i, t in enumerate(noise_scheduler.timesteps):\n        with torch.no_grad():\n            # predict noise \n            residual = model(sample, t).sample\n            # update sample with step\n            sample = noise_scheduler.step(residual, t, sample).prev_sample\n\n\n    # sample = (sample / 2 + 0.5).clamp(0, 1)\n    return sample\n\n        # plt.imshow(image[0].permute(1,2,0).cpu().numpy())\n\n\nimages = generate_image(model, (B,C,H,W), device, noise_scheduler)\n\ngenerate_image: 43.519 seconds\n\n\n\nshow_images(images, ncols=32)"
  },
  {
    "objectID": "models.diffusion.html#diffusorx",
    "href": "models.diffusion.html#diffusorx",
    "title": "Diffusion Model",
    "section": "DiffusorX",
    "text": "DiffusorX\nDiffussor is basically a noise regressor\n\nsource\n\nDiffusorX\n\n DiffusorX (nnet:diffusers.models.unets.unet_2d.UNet2DModel,\n            noise_scheduler:diffusers.schedulers.scheduling_ddpm.DDPMSched\n            uler, optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n            scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nUNet2DModel\n\n\n\n\nnoise_scheduler\nDDPMScheduler\n\n\n\n\noptimizer\nCallable\n\noptimizer,\n\n\nscheduler\nOptional\nNone\nscheduler\n\n\n\n\nB, C, H, W = 16, 1, 32, 32\n\noptimizer = partial(torch.optim.AdamW, lr=3e-4)\n\nnnet = UNet2DModel(\n    sample_size=H,\n    in_channels=C,\n    out_channels=C,\n    block_out_channels=(32, 64, 128, 256)\n    )\n\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n)\n\nprint(f\"steps: {noise_scheduler.config.num_train_timesteps}\")\nmodel = DiffusorX(nnet, noise_scheduler, optimizer)\nx = torch.randn((B, C, H, W))\n\nt = torch.randint(0, noise_scheduler.config.num_train_timesteps, (B,))\nprint(f\"x:{x.shape}, t: {t.shape}, {type(t)}\")\nnoise = nnet(x, t).sample\nprint(noise.shape)\nprint(model(x, t).shape)\n\n[22:30:19] INFO - Regressor: init\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n[22:30:19] INFO - DiffusionX: init\n\n\nsteps: 1000\nx:torch.Size([16, 1, 32, 32]), t: torch.Size([16]), &lt;class 'torch.Tensor'&gt;\ntorch.Size([16, 1, 32, 32])\ntorch.Size([16, 1, 32, 32])\n\n\n\nmodel = model.to(device)\nimgs = model.generate_images(x.shape)\n\n[22:31:00] INFO - diffuse a batch\n\n\n\nshow_images(imgs, ncols=8)\n\n\n\n\n\n\n\n\n\ntorch.set_float32_matmul_precision(\"medium\")\n\ncfg = OmegaConf.load('../config/model/image/diffusorx.yaml')\noptim = partial(torch.optim.AdamW, lr=3e-4)\nmodel = instantiate(cfg)(optimizer= optim)\n\ntotal_time_steps = model.noise_scheduler.config.num_train_timesteps\nx = torch.randn((B, C, H, W)) #.to('mps')\nt = torch.randint(0, total_time_steps, (B,)) #.to('mps')\n\nprint(model(x, t).shape)\n\n[22:32:21] INFO - Regressor: init\n[22:32:21] INFO - DiffusionX: init\n\n\ntorch.Size([16, 1, 32, 32])"
  },
  {
    "objectID": "models.transformer.html",
    "href": "models.transformer.html",
    "title": "Transformer Language Models",
    "section": "",
    "text": "Somewhat basic implemention of transformer model\ndevice = get_device()\nprint(device)\n\n[12:05:37] INFO - Using device: mps\n\n\nmps",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.transformer.html#data-formatting",
    "href": "models.transformer.html#data-formatting",
    "title": "Transformer Language Models",
    "section": "Data formatting",
    "text": "Data formatting\n\nhttps://buomsoo-kim.github.io/attention/2020/04/21/Attention-mechanism-19.md/\n\n\n# dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n\n\ndata = Path('../data/text/tiny_shakespeare.txt').read_text()\ntokenizer = CharTokenizer.from_text(data)\ntokenized = tokenizer.encode(data)\nds = SimpleCharDataset(tokenized, context_length=8)\nx,y = ds[0]\nprint(tokenizer.decode(x), tokenizer.decode(y))\n\nFirst Ci irst Cit",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.transformer.html#attention",
    "href": "models.transformer.html#attention",
    "title": "Transformer Language Models",
    "section": "Attention",
    "text": "Attention\nImagine you’re at position “it” in:\n“The cat sat on the mat because it was tired.”\n\nQ vector for “it” says “I need an antecedent”.\nIt matches strongest with K vector from “cat”.\nSo V vector from “cat” is heavily weighted in the output for “it”.\n\n\nsource\n\nAttentionHead\n\n AttentionHead (embed_dim, head_size, block_size, dropout)\n\nself attention head\n\n\n\n\nDetails\n\n\n\n\nembed_dim\ndimension of embedding\n\n\nhead_size\nsize of attention head\n\n\nblock_size\ncontext size\n\n\ndropout\ndropout rate\n\n\n\n\nvocab_size = 10\nbatch_size = 5\nembed_dim = 20\ncontext_size = 8\ndropout = 0.2\nhead_size = 16\n# embedded input (float)\nx = torch.randn(batch_size, context_size, embed_dim) #(B,T,C)\nprint(x.shape)\natt = AttentionHead(embed_dim, head_size, context_size, dropout)\nxx = att(x)\nprint(xx.shape) # (B, T, Head_size)\n\ntorch.Size([5, 8, 20])\ntorch.Size([5, 8, 16])\n\n\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (num_heads, head_size, embed_dim, block_size, dropout)\n\nmultiple heads of self-attention in parallel\n\nnum_heads = 5\nmulti_att = MultiHeadAttention(num_heads, head_size, embed_dim, context_size, dropout)\nxxx = multi_att(x)\nprint(xxx.shape) #B,T,C/embed_dim\n\ntorch.Size([5, 8, 20])\n\n\n\nmha = nn.MultiheadAttention(\n    embed_dim=embed_dim,\n    num_heads=num_heads,\n    dropout=dropout,\n    batch_first=True\n    )\nattn_out, attn_weight = mha(x, x, x)\nprint(attn_out.shape)\n\ntorch.Size([5, 8, 20])",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.transformer.html#feed-forward",
    "href": "models.transformer.html#feed-forward",
    "title": "Transformer Language Models",
    "section": "Feed forward",
    "text": "Feed forward\n\nsource\n\nFeedFoward\n\n FeedFoward (embed_dim, dropout)\n\na simple linear layer followed by a non-linearity\n\nff = FeedFoward(embed_dim, dropout)\nff_x = ff(x)\nprint(ff_x.shape)\n\ntorch.Size([5, 8, 20])",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.transformer.html#block",
    "href": "models.transformer.html#block",
    "title": "Transformer Language Models",
    "section": "Block",
    "text": "Block\n\nsource\n\nTransformerBlock\n\n TransformerBlock (embed_dim, n_head, block_size, dropout)\n\nTransformer block: communication followed by computation\n\nb = TransformerBlock(embed_dim, num_heads, context_size, dropout)\nbb = b(x)\nprint(bb.shape)\n\ntorch.Size([5, 8, 20])",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.transformer.html#gpt-model",
    "href": "models.transformer.html#gpt-model",
    "title": "Transformer Language Models",
    "section": "GPT model",
    "text": "GPT model\n\nsource\n\nGPTLanguageModel\n\n GPTLanguageModel (vocab_size, embed_dim, block_size, n_head, n_layer,\n                   dropout)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\neval_interval = 500\nlearning_rate = 3e-4\ndevice = get_device()\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\nvocab_size = 46 #len(v)\nm = GPTLanguageModel(vocab_size, n_embd, block_size, n_head, n_layer, dropout)\n\n[11:25:24] INFO - Using device: mps\n\n\n\nprint(device)\n\nmps\n\n\n\ndevice = 'cpu'\nm = m.to(device)\nx = torch.randint(vocab_size, (batch_size, block_size)).to(device)\nlogits, loss = m(x)\nprint(logits.shape)\n\ntorch.Size([64, 256, 46])\n\n\n\n# @torch.no_grad()\n# def estimate_loss():\n#     out = {}\n#     model.eval()\n#     for split in ['train', 'val']:\n#         losses = torch.zeros(eval_iters)\n#         for k in range(eval_iters):\n#             X, Y = get_batch(split)\n#             logits, loss = model(X, Y)\n#             losses[k] = loss.item()\n#         out[split] = losses.mean()\n#     model.train()\n#     return out\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\nacc_loss = []\nmax_iters = 1\nfor iter in range(max_iters):\n    # sample a batch of data\n    xb, yb = get_random_batch(torch.LongTensor(ids), block_size, batch_size, device=device)\n    # evaluate the loss\n    logits, loss = m(xb.to(device), yb.to(device))\n    print(loss.item())\n    acc_loss.append(loss.item())\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\nplt.plot(acc_loss)\n\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(''.join(v.itos(m.generate(context, max_new_tokens=50)[0].tolist())))",
    "crumbs": [
      "Text",
      "Models",
      "Transformer Language Models"
    ]
  },
  {
    "objectID": "models.core.html",
    "href": "models.core.html",
    "title": "Model Core Utils",
    "section": "",
    "text": "Apply init to layers with relu activations\n\nsource\n\n\n\n weight_init (m:torch.nn.modules.module.Module, leaky:int=0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nm\nModule\n\nthe module to initialize\n\n\nleaky\nint\n0\nif leaky relu used\n\n\n\n\nx = torch.randn(1, 1, 32, 32)\nin_channels = x.shape[1]\nout_channels = 3\nkernel_size = 3\nstride = 1\nc1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size//2)\nx1 = c1(x)\nprint(f\"after conv: {x1.shape}\")\nprint(\"x flat dim:\", x1.flatten(2).shape)\nl1 = nn.Linear(32*32, 64)\nx2 = l1(x1.flatten(2))\nprint(\"after linear:\", x2.shape)\n\nleaky = 0.01\nnnet = nn.Sequential(c1, nn.LeakyReLU(negative_slope=leaky), nn.Flatten(2), l1)\nnnet.eval().cpu()\nfig, ax = plt.subplots(1, 3)\nax[0].imshow(x.permute(0,2,3,1).squeeze())\nax[0].set_title(\"x\")\ny = nnet(x)\nax[1].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\nax[1].set_title(\"no init\")\nwi = partial(weight_init, leaky=leaky)\nnnet.apply(wi)\ny = nnet(x)\nax[2].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\nax[2].set_title(\"kaiming init\")\n\nafter conv: torch.Size([1, 3, 32, 32])\n\n\n\nx flat dim:\ntorch.Size([1, 3, 1024])\n\n\n\nafter linear:\ntorch.Size([1, 3, 64])\n\n\n\nText(0.5, 1.0, 'kaiming init')",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#init",
    "href": "models.core.html#init",
    "title": "Model Core Utils",
    "section": "",
    "text": "Apply init to layers with relu activations\n\nsource\n\n\n\n weight_init (m:torch.nn.modules.module.Module, leaky:int=0)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nm\nModule\n\nthe module to initialize\n\n\nleaky\nint\n0\nif leaky relu used\n\n\n\n\nx = torch.randn(1, 1, 32, 32)\nin_channels = x.shape[1]\nout_channels = 3\nkernel_size = 3\nstride = 1\nc1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, kernel_size//2)\nx1 = c1(x)\nprint(f\"after conv: {x1.shape}\")\nprint(\"x flat dim:\", x1.flatten(2).shape)\nl1 = nn.Linear(32*32, 64)\nx2 = l1(x1.flatten(2))\nprint(\"after linear:\", x2.shape)\n\nleaky = 0.01\nnnet = nn.Sequential(c1, nn.LeakyReLU(negative_slope=leaky), nn.Flatten(2), l1)\nnnet.eval().cpu()\nfig, ax = plt.subplots(1, 3)\nax[0].imshow(x.permute(0,2,3,1).squeeze())\nax[0].set_title(\"x\")\ny = nnet(x)\nax[1].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\nax[1].set_title(\"no init\")\nwi = partial(weight_init, leaky=leaky)\nnnet.apply(wi)\ny = nnet(x)\nax[2].imshow(y.detach().squeeze(0).permute(1,0).reshape(8, 8, 3))\nax[2].set_title(\"kaiming init\")\n\nafter conv: torch.Size([1, 3, 32, 32])\n\n\n\nx flat dim:\ntorch.Size([1, 3, 1024])\n\n\n\nafter linear:\ntorch.Size([1, 3, 64])\n\n\n\nText(0.5, 1.0, 'kaiming init')",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#classifier-abstract-base-class",
    "href": "models.core.html#classifier-abstract-base-class",
    "title": "Model Core Utils",
    "section": "Classifier Abstract Base Class",
    "text": "Classifier Abstract Base Class\n\nsource\n\nClassifier\n\n Classifier (nnet:torch.nn.modules.module.Module, num_classes:int,\n             optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n             scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nModule\n\n\n\n\nnum_classes\nint\n\n\n\n\noptimizer\nCallable\n\npartial of optimizer\n\n\nscheduler\nOptional\nNone\npartial of scheduler\n\n\n\n\nsource\n\n\nplot_classifier_metrics_from_csv\n\n plot_classifier_metrics_from_csv (metrics_csv_path:str|os.PathLike)",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#regressor-abstract-class",
    "href": "models.core.html#regressor-abstract-class",
    "title": "Model Core Utils",
    "section": "Regressor Abstract Class",
    "text": "Regressor Abstract Class\n\nsource\n\nRegressor\n\n Regressor (nnet:lightning.pytorch.core.module.LightningModule,\n            optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n            scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nLightningModule\n\n\n\n\noptimizer\nCallable\n\npartial of optimizer\n\n\nscheduler\nOptional\nNone\npartial of scheduler",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#diffuser-abstract-class",
    "href": "models.core.html#diffuser-abstract-class",
    "title": "Model Core Utils",
    "section": "Diffuser Abstract Class",
    "text": "Diffuser Abstract Class\n\nsource\n\nDiffuser\n\n Diffuser (nnet:lightning.pytorch.core.module.LightningModule,\n           optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n           scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nLightningModule\n\n\n\n\noptimizer\nCallable\n\npartial of optimizer\n\n\nscheduler\nOptional\nNone\npartial of scheduler",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#sequential-model",
    "href": "models.core.html#sequential-model",
    "title": "Model Core Utils",
    "section": "Sequential Model",
    "text": "Sequential Model\n\nsource\n\nSequentialModelX\n\n SequentialModelX (modules:List[torch.nn.modules.module.Module], *args,\n                   **kwargs)\n\nHelper class that provides a standard way to create an ABC using inheritance.",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#lr-finder-helper",
    "href": "models.core.html#lr-finder-helper",
    "title": "Model Core Utils",
    "section": "LR Finder Helper",
    "text": "LR Finder Helper\n\n# use LRFinder pythonm module (other version with lightning)\n\ndef find_optimal_lr(model, train_loader, criterion=None, optimizer=None, device='cuda'):\n    # If no criterion provided, use default CrossEntropyLoss\n    if criterion is None:\n        criterion = nn.CrossEntropyLoss()\n    \n    # If no optimizer provided, use Adam\n    if optimizer is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, weight_decay=1e-2)\n    \n    # Initialize LR Finder\n    lr_finder = LRFinder(model, optimizer, criterion, device=device)\n    \n    # Run LR range test\n    lr_finder.range_test(\n        train_loader, \n        start_lr=1e-7,  # Very small starting learning rate\n        end_lr=10,      # Large ending learning rate\n        num_iter=100,   # Number of iterations to test\n        smooth_f=0.05   # Smoothing factor for the loss\n    )\n    \n    # Plot the learning rate vs loss\n    lr_finder.plot(log_lr=True)\n    \n    # Suggest optimal learning rate\n    suggested_lr = lr_finder.reset()\n    \n    print(f\"Suggested Learning Rate: {suggested_lr}\")\n    \n    return suggested_lr\n\n\nsource\n\nlr_finder\n\n lr_finder (model:Callable[...,torch.nn.modules.module.Module],\n            datamodule:nimrod.image.datasets.ImageDataModule,\n            num_training:int=100, plot:bool=True)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nCallable\n\npartial model (missing optim & sched)\n\n\ndatamodule\nImageDataModule\n\ndata module\n\n\nnum_training\nint\n100\nnumber of iterations\n\n\nplot\nbool\nTrue\nplot the learning rate vs loss",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.core.html#cycle-train-helper",
    "href": "models.core.html#cycle-train-helper",
    "title": "Model Core Utils",
    "section": "1-cycle train helper",
    "text": "1-cycle train helper\n\nsource\n\ntrain_one_cycle\n\n train_one_cycle (model:Callable[...,torch.nn.modules.module.Module],\n                  datamodule:nimrod.image.datasets.ImageDataModule,\n                  max_lr:float=0.1, weight_decay=1e-05, n_epochs:int=5,\n                  project_name:str='MNIST-Classifier', tags=['arch',\n                  'dev'], test:bool=True, run_name:str=None,\n                  model_summary:bool=True, logger_cb:str='wandb',\n                  precision='32-true')\n\ntrain one cycle, adamW optim with wandb logging & learning rate monitor by default\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nCallable\n\npartial model (missing optim & sched)\n\n\ndatamodule\nImageDataModule\n\n\n\n\nmax_lr\nfloat\n0.1\n\n\n\nweight_decay\nfloat\n1e-05\n\n\n\nn_epochs\nint\n5\n\n\n\nproject_name\nstr\nMNIST-Classifier\n\n\n\ntags\nlist\n[‘arch’, ‘dev’]\n\n\n\ntest\nbool\nTrue\n\n\n\nrun_name\nstr\nNone\n\n\n\nmodel_summary\nbool\nTrue\n\n\n\nlogger_cb\nstr\nwandb\n\n\n\nprecision\nstr\n32-true\n16-mixed, 32-true\n\n\n\n\n# data\ncfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\ncfg.data_dir = \"../data/image\"\ncfg.batch_size = 128\ncfg.num_workers = 0\ndm = instantiate(cfg)\ndm.prepare_data()\ndm.setup()\n\n[21:54:44] INFO - Init ImageDataModule for fashion_mnist\n[21:54:46] INFO - loading dataset fashion_mnist with args () from split train\n[21:54:46] INFO - loading dataset fashion_mnist from split train\nOverwrite dataset info from restored data version if exists.\n[21:54:48] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:48] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[21:54:48] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:48] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:52] INFO - loading dataset fashion_mnist with args () from split test\n[21:54:52] INFO - loading dataset fashion_mnist from split test\nOverwrite dataset info from restored data version if exists.\n[21:54:53] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:53] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[21:54:53] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:53] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:54:53] INFO - split train into train/val [0.8, 0.2]\n[21:54:53] INFO - train: 48000 val: 12000, test: 10000\n\n\n\n# model\ncfg_model = OmegaConf.load('../config/model/image/convnetx.yaml')\nfeats_dim = [1, 8, 16, 32, 64, 128]\n# feats_dim = [1, 4, 8, 16, 8]\n# feats_dim = [1, 16, 32, 64, 32]\ncfg_model.nnet.n_features = feats_dim\nmodel = instantiate(cfg_model) #partial\ndo_lr_finder = False\n\nif do_lr_finder:\n    suggested_lr = lr_finder(model=model, datamodule=dm, plot=True)\nelse:\n    suggested_lr = 1e-3\n\n# train\nN_EPOCHS = 1\n\nproject_name = \"FASHION-MNIST-Classifier\"\nrun_name = f\"{model.func.__name__}-bs:{dm.batch_size}-epochs:{N_EPOCHS}\"\ntags = [f\"feats:{feats_dim}\", f\"bs:{dm.batch_size}\", f\"epochs:{N_EPOCHS}\"]\n\ntrained_model, best_ckpt = train_one_cycle(\n    model,\n    dm,\n    n_epochs=N_EPOCHS,\n    max_lr=suggested_lr,\n    project_name=project_name,\n    tags=tags,\n    run_name=run_name\n    )\n\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n[21:54:54] INFO - ConvNetX: init\n[21:54:54] INFO - Classifier: init\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nConvNet                                  [128, 10]                 --\n├─Sequential: 1-1                        [128, 10]                 --\n│    └─ConvLayer: 2-1                    [128, 8, 32, 32]          --\n│    │    └─Sequential: 3-1              [128, 8, 32, 32]          88\n│    └─ConvLayer: 2-2                    [128, 16, 16, 16]         --\n│    │    └─Sequential: 3-2              [128, 16, 16, 16]         1,184\n│    └─ConvLayer: 2-3                    [128, 32, 8, 8]           --\n│    │    └─Sequential: 3-3              [128, 32, 8, 8]           4,672\n│    └─ConvLayer: 2-4                    [128, 64, 4, 4]           --\n│    │    └─Sequential: 3-4              [128, 64, 4, 4]           18,560\n│    └─ConvLayer: 2-5                    [128, 128, 2, 2]          --\n│    │    └─Sequential: 3-5              [128, 128, 2, 2]          73,984\n│    └─ConvLayer: 2-6                    [128, 10, 1, 1]           --\n│    │    └─Sequential: 3-6              [128, 10, 1, 1]           11,540\n│    └─Flatten: 2-7                      [128, 10]                 --\n==========================================================================================\nTotal params: 110,028\nTrainable params: 110,028\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 161.97\n==========================================================================================\nInput size (MB): 0.52\nForward/backward pass size (MB): 32.53\nParams size (MB): 0.44\nEstimated Total Size (MB): 33.49\n==========================================================================================\n\n\n\nTracking run with wandb version 0.19.1\n\n\nRun data is saved locally in /tmp/wandb/run-20250206_215454-1fc1d7re\n\n\nSyncing run ConvNetX-bs:128-epochs:1 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/slegroux/FASHION-MNIST-Classifier\n\n\n View run at https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re\n\n\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1 exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n[21:54:54] INFO - Optimizer: &lt;class 'torch.optim.adamw.AdamW'&gt;\n[21:54:54] INFO - Scheduler: &lt;class 'torch.optim.lr_scheduler.OneCycleLR'&gt;\n\n  | Name         | Type               | Params | Mode \n------------------------------------------------------------\n0 | nnet         | ConvNet            | 110 K  | train\n1 | loss         | CrossEntropyLoss   | 0      | train\n2 | train_acc    | MulticlassAccuracy | 0      | train\n3 | val_acc      | MulticlassAccuracy | 0      | train\n4 | test_acc     | MulticlassAccuracy | 0      | train\n5 | train_loss   | MeanMetric         | 0      | train\n6 | val_loss     | MeanMetric         | 0      | train\n7 | test_loss    | MeanMetric         | 0      | train\n8 | val_acc_best | MaxMetric          | 0      | train\n------------------------------------------------------------\n110 K     Trainable params\n0         Non-trainable params\n110 K     Total params\n0.440     Total estimated model params size (MB)\n41        Modules in train mode\n0         Modules in eval mode\n\n\n\n\n\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n\n\n\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=1` reached.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │    0.8658000230789185     │\n│         test/loss         │    0.5964178442955017     │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[21:55:01] INFO - Best ckpt path: /user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1/0-0.57.ckpt\n\n\n\n\n\n    Run history:\n\n\n\nepoch\n▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nlr-AdamW\n▁▁▂▂▃▃▄▅▅▅▆▆▇██████████▇▇▆▅▅▅▅▄▄▄▃▃▂▁▁▁▁\n\n\ntest/acc\n▁\n\n\ntest/loss\n▁\n\n\ntrain/acc_epoch\n▁\n\n\ntrain/acc_step\n▁▂▄▅▅▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇██▇▇███▇▇██▇██▇█\n\n\ntrain/loss_epoch\n▁\n\n\ntrain/loss_step\n█▇▆▆▆▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁\n\n\ntrainer/global_step\n▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███████\n\n\nval/acc\n▁\n\n\nval/acc_best\n▁\n\n\nval/loss\n▁\n\n\n\nRun summary:\n\n\n\nepoch\n1\n\n\nlr-AdamW\n0.0\n\n\ntest/acc\n0.8658\n\n\ntest/loss\n0.59642\n\n\ntrain/acc_epoch\n0.79585\n\n\ntrain/acc_step\n0.89844\n\n\ntrain/loss_epoch\n0.84797\n\n\ntrain/loss_step\n0.56174\n\n\ntrainer/global_step\n375\n\n\nval/acc\n0.87608\n\n\nval/acc_best\n0.87608\n\n\nval/loss\n0.5739\n\n\n\n\n\n\n View run ConvNetX-bs:128-epochs:1 at: https://wandb.ai/slegroux/FASHION-MNIST-Classifier/runs/1fc1d7re View project at: https://wandb.ai/slegroux/FASHION-MNIST-ClassifierSynced 6 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)\n\n\nFind logs at: /tmp/wandb/run-20250206_215454-1fc1d7re/logs\n\n\n\nprint(best_ckpt)\nx = torch.randn(1, 1, 32, 32)\ntrained_model.eval()\ntrained_model(x)\n\n/user/s/slegroux/Projects/nimrod/nbs/checkpoints/FASHION-MNIST-Classifier/ConvNetX-bs:128-epochs:1/0-0.58.ckpt\n\n\n\ntensor([[0.0000, 0.0000, 0.5579, 0.1776, 0.0000, 0.3419, 0.0000, 0.0000, 2.2810,\n         0.0000]], grad_fn=&lt;ViewBackward0&gt;)",
    "crumbs": [
      "Misc",
      "Model Core Utils"
    ]
  },
  {
    "objectID": "models.superres.html",
    "href": "models.superres.html",
    "title": "Super Resolution",
    "section": "",
    "text": "source\n\n\n\n UpBlock (in_channels:int, out_channels:int, kernel_size:int=3,\n          activation:Optional[Type[torch.nn.modules.module.Module]]=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, normalization:Optional[Type\n          [torch.nn.modules.module.Module]]=&lt;class\n          'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nkernel_size\nint\n3\nKernel size\n\n\nactivation\nOptional\nReLU\nActivation function\n\n\nnormalization\nOptional\nBatchNorm2d\nNormalization function\n\n\n\n\nm = UpBlock(3, 8)\nx = torch.randn(1, 3, 64, 64)\ny = m(x)\nprint(y.shape)\n\ntorch.Size([1, 8, 128, 128])\n\n\n\n\nsource\n\n\n\n\n SuperResAutoencoder (n_features:List[int]=[3, 8, 16, 32, 64, 128],\n                      activation=functools.partial(&lt;class\n                      'torch.nn.modules.activation.LeakyReLU'&gt;,\n                      negative_slope=0.1), leaky:float=0.1,\n                      normalization=&lt;class\n                      'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[3, 8, 16, 32, 64, 128]\nNumber of features in each layer\n\n\nactivation\npartial\nfunctools.partial(&lt;class ‘torch.nn.modules.activation.LeakyReLU’&gt;, negative_slope=0.1)\nActivation function\n\n\nleaky\nfloat\n0.1\nLeaky ReLU negative slope\n\n\nnormalization\ntype\nBatchNorm2d\nNormalization function\n\n\n\n\nsource\n\n\n\n\n init_weights (m, leaky=0.0)\n\n\n# RGB \nmodel = SuperResAutoencoder(n_features=[3, 8, 16, 32, 64, 128])\nx = torch.randn(1, 3, 64, 64)\ny = model(x)\nprint(y.shape)\n\n# GRAY\nmodel = SuperResAutoencoder(n_features=[1, 8, 16, 32, 64])\nx = torch.randn(1, 1, 28, 28) # note dim is nearest power of 2\ny = model(x)\nprint(y.shape)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # RGB \n----&gt; 2 model = SuperResAutoencoder(n_features=[3, 8, 16, 32, 64, 128])\n      3 x = torch.randn(1, 3, 64, 64)\n      4 y = model(x)\n\nNameError: name 'SuperResAutoencoder' is not defined\n\n\n\n\nsource\n\n\n\n\n SuperResAutoencoderX (nnet:__main__.SuperResAutoencoder,\n                       optimizer:Callable[...,torch.optim.optimizer.Optimi\n                       zer], scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nSuperResAutoencoder\n\nsuper res autoencoder neural net\n\n\noptimizer\nCallable\n\noptimizer partial\n\n\nscheduler\nOptional\nNone\nscheduler partial\n\n\n\n\nm = SuperResAutoencoderX(\n    nnet=SuperResAutoencoder(),\n    optimizer=partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5),\n    scheduler=partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=10)\n)\n\nx = torch.randn(1,3,64,64)\ny = m(x)\nprint(y.shape)\n\n[23:45:38] INFO - SuperResAutoencoderX: init\n[23:45:38] INFO - Regressor: init\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[26], line 8\n      1 m = SuperResAutoencoderX(\n      2     nnet=SuperResAutoencoder(),\n      3     optimizer=partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5),\n      4     scheduler=partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=10)\n      5 )\n      7 x = torch.randn(1,1,64,64)\n----&gt; 8 y = m(x)\n      9 print(y.shape)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/core.py:278, in Regressor.forward(self, x)\n    277 def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n--&gt; 278     return self.nnet(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nCell In[22], line 25, in SuperResAutoencoder.forward(self, x)\n     24 def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n---&gt; 25     return self.autoencoder(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/resnet.py:69, in ResBlock.forward(self, x)\n     68 def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n---&gt; 69     return self.activation(self.conv_layer(x) + self.id(self.pooling(x)))\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/conv.py:83, in ConvLayer.forward(self, x)\n     77 def forward(\n     78         self,\n     79         x:torch.Tensor # input image tensor of dimension (B, C, W, H)\n     80         ) -&gt; torch.Tensor: # output image tensor of dimension (B, C, W/2, H/2)\n     82     \"\"\"forward method of the ConvLayer\"\"\"\n---&gt; 83     return self.net(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/conv.py:554, in Conv2d.forward(self, input)\n    553 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 554     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/conv.py:549, in Conv2d._conv_forward(self, input, weight, bias)\n    537 if self.padding_mode != \"zeros\":\n    538     return F.conv2d(\n    539         F.pad(\n    540             input, self._reversed_padding_repeated_twice, mode=self.padding_mode\n   (...)\n    547         self.groups,\n    548     )\n--&gt; 549 return F.conv2d(\n    550     input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n    551 )\n\nRuntimeError: Given groups=1, weight of size [8, 3, 3, 3], expected input[1, 1, 64, 64] to have 3 channels, but got 1 channels instead"
  },
  {
    "objectID": "models.superres.html#autoencoder",
    "href": "models.superres.html#autoencoder",
    "title": "Super Resolution",
    "section": "",
    "text": "source\n\n\n\n UpBlock (in_channels:int, out_channels:int, kernel_size:int=3,\n          activation:Optional[Type[torch.nn.modules.module.Module]]=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;, normalization:Optional[Type\n          [torch.nn.modules.module.Module]]=&lt;class\n          'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nkernel_size\nint\n3\nKernel size\n\n\nactivation\nOptional\nReLU\nActivation function\n\n\nnormalization\nOptional\nBatchNorm2d\nNormalization function\n\n\n\n\nm = UpBlock(3, 8)\nx = torch.randn(1, 3, 64, 64)\ny = m(x)\nprint(y.shape)\n\ntorch.Size([1, 8, 128, 128])\n\n\n\n\nsource\n\n\n\n\n SuperResAutoencoder (n_features:List[int]=[3, 8, 16, 32, 64, 128],\n                      activation=functools.partial(&lt;class\n                      'torch.nn.modules.activation.LeakyReLU'&gt;,\n                      negative_slope=0.1), leaky:float=0.1,\n                      normalization=&lt;class\n                      'torch.nn.modules.batchnorm.BatchNorm2d'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[3, 8, 16, 32, 64, 128]\nNumber of features in each layer\n\n\nactivation\npartial\nfunctools.partial(&lt;class ‘torch.nn.modules.activation.LeakyReLU’&gt;, negative_slope=0.1)\nActivation function\n\n\nleaky\nfloat\n0.1\nLeaky ReLU negative slope\n\n\nnormalization\ntype\nBatchNorm2d\nNormalization function\n\n\n\n\nsource\n\n\n\n\n init_weights (m, leaky=0.0)\n\n\n# RGB \nmodel = SuperResAutoencoder(n_features=[3, 8, 16, 32, 64, 128])\nx = torch.randn(1, 3, 64, 64)\ny = model(x)\nprint(y.shape)\n\n# GRAY\nmodel = SuperResAutoencoder(n_features=[1, 8, 16, 32, 64])\nx = torch.randn(1, 1, 28, 28) # note dim is nearest power of 2\ny = model(x)\nprint(y.shape)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 2\n      1 # RGB \n----&gt; 2 model = SuperResAutoencoder(n_features=[3, 8, 16, 32, 64, 128])\n      3 x = torch.randn(1, 3, 64, 64)\n      4 y = model(x)\n\nNameError: name 'SuperResAutoencoder' is not defined\n\n\n\n\nsource\n\n\n\n\n SuperResAutoencoderX (nnet:__main__.SuperResAutoencoder,\n                       optimizer:Callable[...,torch.optim.optimizer.Optimi\n                       zer], scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nSuperResAutoencoder\n\nsuper res autoencoder neural net\n\n\noptimizer\nCallable\n\noptimizer partial\n\n\nscheduler\nOptional\nNone\nscheduler partial\n\n\n\n\nm = SuperResAutoencoderX(\n    nnet=SuperResAutoencoder(),\n    optimizer=partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5),\n    scheduler=partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=10)\n)\n\nx = torch.randn(1,3,64,64)\ny = m(x)\nprint(y.shape)\n\n[23:45:38] INFO - SuperResAutoencoderX: init\n[23:45:38] INFO - Regressor: init\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[26], line 8\n      1 m = SuperResAutoencoderX(\n      2     nnet=SuperResAutoencoder(),\n      3     optimizer=partial(torch.optim.AdamW, lr=1e-4, weight_decay=1e-5),\n      4     scheduler=partial(torch.optim.lr_scheduler.ReduceLROnPlateau, mode='min', factor=0.1, patience=10)\n      5 )\n      7 x = torch.randn(1,1,64,64)\n----&gt; 8 y = m(x)\n      9 print(y.shape)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/core.py:278, in Regressor.forward(self, x)\n    277 def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n--&gt; 278     return self.nnet(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nCell In[22], line 25, in SuperResAutoencoder.forward(self, x)\n     24 def forward(self, x:torch.Tensor)-&gt;torch.Tensor:\n---&gt; 25     return self.autoencoder(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/resnet.py:69, in ResBlock.forward(self, x)\n     68 def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n---&gt; 69     return self.activation(self.conv_layer(x) + self.id(self.pooling(x)))\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/Projects/nimrod/nimrod/models/conv.py:83, in ConvLayer.forward(self, x)\n     77 def forward(\n     78         self,\n     79         x:torch.Tensor # input image tensor of dimension (B, C, W, H)\n     80         ) -&gt; torch.Tensor: # output image tensor of dimension (B, C, W/2, H/2)\n     82     \"\"\"forward method of the ConvLayer\"\"\"\n---&gt; 83     return self.net(x)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/container.py:250, in Sequential.forward(self, input)\n    248 def forward(self, input):\n    249     for module in self:\n--&gt; 250         input = module(input)\n    251     return input\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1736, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1734     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1735 else:\n-&gt; 1736     return self._call_impl(*args, **kwargs)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/module.py:1747, in Module._call_impl(self, *args, **kwargs)\n   1742 # If we don't have any hooks, we want to skip the rest of the logic in\n   1743 # this function, and just call forward.\n   1744 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1745         or _global_backward_pre_hooks or _global_backward_hooks\n   1746         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1747     return forward_call(*args, **kwargs)\n   1749 result = None\n   1750 called_always_called_hooks = set()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/conv.py:554, in Conv2d.forward(self, input)\n    553 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 554     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/torch/nn/modules/conv.py:549, in Conv2d._conv_forward(self, input, weight, bias)\n    537 if self.padding_mode != \"zeros\":\n    538     return F.conv2d(\n    539         F.pad(\n    540             input, self._reversed_padding_repeated_twice, mode=self.padding_mode\n   (...)\n    547         self.groups,\n    548     )\n--&gt; 549 return F.conv2d(\n    550     input, weight, bias, self.stride, self.padding, self.dilation, self.groups\n    551 )\n\nRuntimeError: Given groups=1, weight of size [8, 3, 3, 3], expected input[1, 1, 64, 64] to have 3 channels, but got 1 channels instead"
  },
  {
    "objectID": "audio.embeddings.html",
    "href": "audio.embeddings.html",
    "title": "Audio Embedders",
    "section": "",
    "text": "TODO: figure out encoder from hugging face lib",
    "crumbs": [
      "Audio",
      "Embeddings",
      "Audio Embedders"
    ]
  },
  {
    "objectID": "audio.embeddings.html#encodec",
    "href": "audio.embeddings.html#encodec",
    "title": "Audio Embedders",
    "section": "EncoDec",
    "text": "EncoDec\n\nmodel = EncodecModel.encodec_model_24khz()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 model = EncodecModel.encodec_model_24khz()\n\nAttributeError: type object 'EncodecModel' has no attribute 'encodec_model_24khz'\n\n\n\n\nUsage\n\nwav, sr = torchaudio.load(\"../data/audio/obama.wav\")\n# wav, sr = torch.rand((1, 24000)), 24000\n# wav, sr = np.random.random((1, 24000)), 24000\n\nencodec = EncoDec(device='cpu')\ncodes = encodec(wav,sr)\nprint(f\"wav: {wav.shape}, code: {codes.shape} \")\nplt.rcParams[\"figure.figsize\"] = (5,5)\nplt.xlabel('frames')\nplt.ylabel('quantization')\nplt.imshow(codes.squeeze().cpu().numpy())\ndecoded = encodec.decode(codes)\nplot_waveform(decoded.detach().cpu().squeeze(0), encodec.sample_rate)\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[12], line 5\n      1 wav, sr = torchaudio.load(\"../data/audio/obama.wav\")\n      2 # wav, sr = torch.rand((1, 24000)), 24000\n      3 # wav, sr = np.random.random((1, 24000)), 24000\n----&gt; 5 encodec = EncoDec(device='cpu')\n      6 codes = encodec(wav,sr)\n      7 print(f\"wav: {wav.shape}, code: {codes.shape} \")\n\nCell In[11], line 4, in EncoDec.__init__(self, device)\n      3 def __init__(self, device:str='cpu'):\n----&gt; 4     self.model = EncodecModel.encodec_model_24khz()\n      5     self._device = device\n      6     self.model.to(self._device)\n\nAttributeError: type object 'EncodecModel' has no attribute 'encodec_model_24khz'\n\n\n\n\nplt.plot(codes[0][0])\nprint(codes[0][0].shape)\n\n\n\nLhotse-style Encodec feature extractor\n\nencodec_extractor = EncoDecExtractor()\n# cuts = CutSet.from_file(\"../recipes/tts/ljspeech/data/first_3.jsonl.gz\")\ncuts = CutSet.from_file(\"../data/en/LJSpeech-1.1/first_3.encodec.jsonl.gz\")\nprint(cuts[0])\nprint(cuts[1])\n\n\n# torch.set_num_threads(1)\n# torch.set_num_interop_threads(1)\n\n\n# feats = cuts.compute_and_store_features(extractor=Fbank(), storage_path=\"../recipes/tts/ljspeech/data/feats\")\n\n\n# storage_path = \"../.data/en/LJSpeech-1.1\"\n# # storage_path = \"../recipes/tts/ljspeech/data/feats\"\n# # TODO: make it work with num_jobs&gt;1\n# cuts = cuts.compute_and_store_features(\n#     extractor=encodec_extractor,\n#     storage_path=storage_path,\n#     num_jobs=1,\n# )\n# cuts.to_file(\"../recipes/tts/ljspeech/data/cuts_encodec.jsonl.gz\")\n# print(cuts[0])\n# cuts[0].plot_features()\n# print(cuts)\n\n\nfiles = \"../data/en/LJSpeech-1.1/cuts_encodec.jsonl.gz\"\n# files = \"../recipes/tts/ljspeech/data/cuts_encodec.jsonl.gz\"\ncuts = CutSet.from_file(files)\nprint(cuts)\n\n\n### HF\n\n# dummy dataset, however you can swap this with an dataset on the 🤗 hub or bring your own\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n# load the model + processor (for pre-processing the audio)\nmodel = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\nprocessor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\nlibrispeech_dummy[0]\n# cast the audio data to the correct sampling rate for the model\nlibrispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\naudio_sample = librispeech_dummy[0][\"audio\"][\"array\"]",
    "crumbs": [
      "Audio",
      "Embeddings",
      "Audio Embedders"
    ]
  },
  {
    "objectID": "audio.embeddings.html#audiolm",
    "href": "audio.embeddings.html#audiolm",
    "title": "Audio Embedders",
    "section": "AudioLM",
    "text": "AudioLM\n\n# TO DO",
    "crumbs": [
      "Audio",
      "Embeddings",
      "Audio Embedders"
    ]
  },
  {
    "objectID": "models.mlp.html",
    "href": "models.mlp.html",
    "title": "Multi Layer Perceptron",
    "section": "",
    "text": "source\n\n\n\n MLP (n_in:int=784, n_h:int=64, n_out:int=10, dropout:float=0.2)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_in\nint\n784\ninput dimension e.g. (H,W) for image\n\n\nn_h\nint\n64\nhidden dimension\n\n\nn_out\nint\n10\noutput dimension (= number of classes for classification)\n\n\ndropout\nfloat\n0.2\n\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nimage = torch.rand((5, 28*28))\nmlp = MLP(n_in=28*28, n_h=64, n_out=10, dropout=0.1)\nout = mlp(image)\nprint(out.shape)\n# cfg = OmegaConf.load('../config/model/image/mlpx.yaml')\n# model = instantiate(cfg.nnet)\n# out = model(image)\n# print(out.shape)\n\n[00:06:00] INFO - MLP: init\n[00:06:00] INFO - MLP: init\n\n\ntorch.Size([5, 10])\ntorch.Size([5, 10])\n\n\n\n\n\n\n# load from config file\ncfg = OmegaConf.load('../config/image/data/mnist.yaml')\ndatamodule = instantiate(cfg.datamodule)\ndatamodule.prepare_data()\ndatamodule.setup()\n\nx = datamodule.test_ds[0][0] # (C, H, W)\nlabel = datamodule.test_ds[0][1] #(int)\nprint(\"original shape (C,H,W): \", x.shape)\nprint(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\nprint(x[0][1])\n\n# using nimrod datamodule\ntrain_loader = datamodule.train_dataloader()\nval_loader = datamodule.val_dataloader()\ntest_loader = datamodule.test_dataloader()\n\n[16:43:14] INFO - Init ImageDataModule for mnist\n[16:43:17] INFO - loading dataset mnist with args () from split train\n[16:43:24] INFO - loading dataset mnist with args () from split test\n[16:43:26] INFO - split train into train/val [0.8, 0.2]\n[16:43:26] INFO - train: 48000 val: 12000, test: 10000\n\n\noriginal shape (C,H,W):  torch.Size([1, 28, 28])\nreshape (C,HxW):  torch.Size([1, 784])\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.])\n\n\n\n\n\ndevice = get_device()\n\n# data\ncfg = OmegaConf.load('../config/image/data/mnist.yaml')\ncfg.batch_size = 2048\ndatamodule = instantiate(cfg.datamodule)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n# model\nmodel = mlp.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\nn_epochs = 2\nlosses = []\nlrs = []\ncurrent_step = 0\nsteps_per_epoch = len(datamodule.train_ds) // cfg.datamodule.batch_size\ntotal_steps = steps_per_epoch * n_epochs\nprint(f\"steps_per_epoch: {steps_per_epoch}, total_steps: {total_steps}\")\n\nfor epoch in range(n_epochs):\n    model.train()\n    for images, labels in datamodule.train_dataloader():\n        optimizer.zero_grad()\n        images = images.view(-1, 28*28)\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)        \n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        current_lr = optimizer.param_groups[0]['lr']\n        lrs.append(current_lr)\n        if not (current_step % 100):\n            print(f\"Loss {loss.item():.4f}, Current LR: {current_lr:.10f}, Step: {current_step}/{total_steps}\")\n        current_step += 1\n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in datamodule.test_dataloader():\n            # model expects input (B,H*W)\n            images = images.view(-1, 28*28).to(device)\n            images = images.to(device)\n            labels = labels.to(device)\n            # Pass the input through the model\n            outputs = model(images)\n            # Get the predicted labels\n            _, predicted = torch.max(outputs.data, 1)\n\n            # Update the total and correct counts\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n\n        # Print the accuracy\n        print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n\n[16:43:26] INFO - Using device: mps\n[16:43:26] INFO - Init ImageDataModule for mnist\n\n\nCPU times: user 1 μs, sys: 0 ns, total: 1 μs\nWall time: 2.62 μs\n\n\n[16:43:28] INFO - loading dataset mnist with args () from split train\n[16:43:35] INFO - loading dataset mnist with args () from split test\n[16:43:37] INFO - split train into train/val [0.8, 0.2]\n[16:43:37] INFO - train: 48000 val: 12000, test: 10000\n\n\nsteps_per_epoch: 750, total_steps: 1500\nLoss 2.2913, Current LR: 0.0010000000, Step: 0/1500\nLoss 0.6561, Current LR: 0.0010000000, Step: 100/1500\nLoss 0.5902, Current LR: 0.0010000000, Step: 200/1500\nLoss 0.4121, Current LR: 0.0010000000, Step: 300/1500\nLoss 0.4594, Current LR: 0.0010000000, Step: 400/1500\nLoss 0.1972, Current LR: 0.0010000000, Step: 500/1500\nLoss 0.2817, Current LR: 0.0010000000, Step: 600/1500\nLoss 0.2748, Current LR: 0.0010000000, Step: 700/1500\nEpoch 1: Accuracy = 93.09%\nLoss 0.2815, Current LR: 0.0010000000, Step: 800/1500\nLoss 0.5310, Current LR: 0.0010000000, Step: 900/1500\nLoss 0.1237, Current LR: 0.0010000000, Step: 1000/1500\nLoss 0.2789, Current LR: 0.0010000000, Step: 1100/1500\nLoss 0.3667, Current LR: 0.0010000000, Step: 1200/1500\nLoss 0.1536, Current LR: 0.0010000000, Step: 1300/1500\nLoss 0.4022, Current LR: 0.0010000000, Step: 1400/1500\nEpoch 2: Accuracy = 94.73%\n\n\n\n# plt.figure(1)\n# plt.subplot(211)\nplt.ylabel('loss')\nplt.xlabel('step')\nplt.plot(losses)\n# plt.subplot(212)\n# plt.ylabel('lr')\n# plt.xlabel('step')\n# plt.plot(lrs)",
    "crumbs": [
      "Image",
      "Models",
      "Multi Layer Perceptron"
    ]
  },
  {
    "objectID": "models.mlp.html#mlp",
    "href": "models.mlp.html#mlp",
    "title": "Multi Layer Perceptron",
    "section": "",
    "text": "source\n\n\n\n MLP (n_in:int=784, n_h:int=64, n_out:int=10, dropout:float=0.2)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_in\nint\n784\ninput dimension e.g. (H,W) for image\n\n\nn_h\nint\n64\nhidden dimension\n\n\nn_out\nint\n10\noutput dimension (= number of classes for classification)\n\n\ndropout\nfloat\n0.2\n\n\n\nReturns\nNone\n\n\n\n\n\n\n\n\n\nimage = torch.rand((5, 28*28))\nmlp = MLP(n_in=28*28, n_h=64, n_out=10, dropout=0.1)\nout = mlp(image)\nprint(out.shape)\n# cfg = OmegaConf.load('../config/model/image/mlpx.yaml')\n# model = instantiate(cfg.nnet)\n# out = model(image)\n# print(out.shape)\n\n[00:06:00] INFO - MLP: init\n[00:06:00] INFO - MLP: init\n\n\ntorch.Size([5, 10])\ntorch.Size([5, 10])\n\n\n\n\n\n\n# load from config file\ncfg = OmegaConf.load('../config/image/data/mnist.yaml')\ndatamodule = instantiate(cfg.datamodule)\ndatamodule.prepare_data()\ndatamodule.setup()\n\nx = datamodule.test_ds[0][0] # (C, H, W)\nlabel = datamodule.test_ds[0][1] #(int)\nprint(\"original shape (C,H,W): \", x.shape)\nprint(\"reshape (C,HxW): \", x.view(x.size(0), -1).shape)\nprint(x[0][1])\n\n# using nimrod datamodule\ntrain_loader = datamodule.train_dataloader()\nval_loader = datamodule.val_dataloader()\ntest_loader = datamodule.test_dataloader()\n\n[16:43:14] INFO - Init ImageDataModule for mnist\n[16:43:17] INFO - loading dataset mnist with args () from split train\n[16:43:24] INFO - loading dataset mnist with args () from split test\n[16:43:26] INFO - split train into train/val [0.8, 0.2]\n[16:43:26] INFO - train: 48000 val: 12000, test: 10000\n\n\noriginal shape (C,H,W):  torch.Size([1, 28, 28])\nreshape (C,HxW):  torch.Size([1, 784])\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0.])\n\n\n\n\n\ndevice = get_device()\n\n# data\ncfg = OmegaConf.load('../config/image/data/mnist.yaml')\ncfg.batch_size = 2048\ndatamodule = instantiate(cfg.datamodule)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n# model\nmodel = mlp.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n\nn_epochs = 2\nlosses = []\nlrs = []\ncurrent_step = 0\nsteps_per_epoch = len(datamodule.train_ds) // cfg.datamodule.batch_size\ntotal_steps = steps_per_epoch * n_epochs\nprint(f\"steps_per_epoch: {steps_per_epoch}, total_steps: {total_steps}\")\n\nfor epoch in range(n_epochs):\n    model.train()\n    for images, labels in datamodule.train_dataloader():\n        optimizer.zero_grad()\n        images = images.view(-1, 28*28)\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)        \n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        current_lr = optimizer.param_groups[0]['lr']\n        lrs.append(current_lr)\n        if not (current_step % 100):\n            print(f\"Loss {loss.item():.4f}, Current LR: {current_lr:.10f}, Step: {current_step}/{total_steps}\")\n        current_step += 1\n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in datamodule.test_dataloader():\n            # model expects input (B,H*W)\n            images = images.view(-1, 28*28).to(device)\n            images = images.to(device)\n            labels = labels.to(device)\n            # Pass the input through the model\n            outputs = model(images)\n            # Get the predicted labels\n            _, predicted = torch.max(outputs.data, 1)\n\n            # Update the total and correct counts\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n\n        # Print the accuracy\n        print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n\n[16:43:26] INFO - Using device: mps\n[16:43:26] INFO - Init ImageDataModule for mnist\n\n\nCPU times: user 1 μs, sys: 0 ns, total: 1 μs\nWall time: 2.62 μs\n\n\n[16:43:28] INFO - loading dataset mnist with args () from split train\n[16:43:35] INFO - loading dataset mnist with args () from split test\n[16:43:37] INFO - split train into train/val [0.8, 0.2]\n[16:43:37] INFO - train: 48000 val: 12000, test: 10000\n\n\nsteps_per_epoch: 750, total_steps: 1500\nLoss 2.2913, Current LR: 0.0010000000, Step: 0/1500\nLoss 0.6561, Current LR: 0.0010000000, Step: 100/1500\nLoss 0.5902, Current LR: 0.0010000000, Step: 200/1500\nLoss 0.4121, Current LR: 0.0010000000, Step: 300/1500\nLoss 0.4594, Current LR: 0.0010000000, Step: 400/1500\nLoss 0.1972, Current LR: 0.0010000000, Step: 500/1500\nLoss 0.2817, Current LR: 0.0010000000, Step: 600/1500\nLoss 0.2748, Current LR: 0.0010000000, Step: 700/1500\nEpoch 1: Accuracy = 93.09%\nLoss 0.2815, Current LR: 0.0010000000, Step: 800/1500\nLoss 0.5310, Current LR: 0.0010000000, Step: 900/1500\nLoss 0.1237, Current LR: 0.0010000000, Step: 1000/1500\nLoss 0.2789, Current LR: 0.0010000000, Step: 1100/1500\nLoss 0.3667, Current LR: 0.0010000000, Step: 1200/1500\nLoss 0.1536, Current LR: 0.0010000000, Step: 1300/1500\nLoss 0.4022, Current LR: 0.0010000000, Step: 1400/1500\nEpoch 2: Accuracy = 94.73%\n\n\n\n# plt.figure(1)\n# plt.subplot(211)\nplt.ylabel('loss')\nplt.xlabel('step')\nplt.plot(losses)\n# plt.subplot(212)\n# plt.ylabel('lr')\n# plt.xlabel('step')\n# plt.plot(lrs)",
    "crumbs": [
      "Image",
      "Models",
      "Multi Layer Perceptron"
    ]
  },
  {
    "objectID": "models.mlp.html#mlp_x",
    "href": "models.mlp.html#mlp_x",
    "title": "Multi Layer Perceptron",
    "section": "MLP_X",
    "text": "MLP_X\n\nsource\n\nMLP_X\n\n MLP_X (nnet:__main__.MLP, num_classes:int,\n        optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n        scheduler:Optional[Callable[...,Any]]=None)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nMLP\n\nmlp neural net\n\n\nnum_classes\nint\n\nnumber of classes\n\n\noptimizer\nCallable\n\noptimizer\n\n\nscheduler\nOptional\nNone\nscheduler\n\n\n\n\n\nUsage\n\ncfg = OmegaConf.load('../config/model/image/mlpx.yaml')\nmodel = instantiate(cfg.nnet)\nb = torch.rand((16,1, 28*28))\ny = model(b)\nprint(y.shape)\n\n[13:18:42] INFO - MLP: init\n\n\ntorch.Size([16, 1, 10])\n\n\n\n\nNimrod training\n\nMAX_EPOCHS = 5\n# data module config\ncfg = OmegaConf.load('../config/image/data/mnist.yaml')\ncfg.datamodule.batch_size = 512\ncfg.datamodule.num_workers = 0\ncfg.datamodule.pin_memory = True\ndatamodule = instantiate(cfg.datamodule)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n# lr monitor\ncfg = OmegaConf.load('../config/callbacks/learning_rate_monitor.yaml')\nlr_monitor = instantiate(cfg.learning_rate_monitor)\n\n# model\ncfg = OmegaConf.load('../config/model/image/mlp.yaml')\n\ntrainer = Trainer(\n    accelerator=\"auto\",\n    min_epochs=1,\n    max_epochs=MAX_EPOCHS,\n    logger=CSVLogger(\"logs\", name=\"mnist_mlp\"),\n    callbacks=[lr_monitor],\n    check_val_every_n_epoch=1\n    )\n\n\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(\n    model,\n    datamodule=datamodule,\n    min_lr=1e-6,\n    max_lr=1.0,\n    num_training=100,  # number of iterations\n    # attr_name=\"optimizer.lr\",\n)\nfig = lr_finder.plot(suggest=True)\nplt.show()\nprint(f\"Suggested learning rate: {lr_finder.suggestion()}\")\n\n[16:44:24] INFO - Init ImageDataModule for mnist\n[16:44:26] INFO - loading dataset mnist with args () from split train\n[16:44:33] INFO - loading dataset mnist with args () from split test\n[16:44:35] INFO - split train into train/val [0.8, 0.2]\n[16:44:35] INFO - train: 48000 val: 12000, test: 10000\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[23], line 31\n     20 trainer = Trainer(\n     21     accelerator=\"auto\",\n     22     min_epochs=1,\n   (...)\n     26     check_val_every_n_epoch=1\n     27     )\n     30 tuner = Tuner(trainer)\n---&gt; 31 lr_finder = tuner.lr_find(\n     32     model,\n     33     datamodule=datamodule,\n     34     min_lr=1e-6,\n     35     max_lr=1.0,\n     36     num_training=100,  # number of iterations\n     37     # attr_name=\"optimizer.lr\",\n     38 )\n     39 fig = lr_finder.plot(suggest=True)\n     40 plt.show()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:180, in Tuner.lr_find(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\n    177 lr_finder_callback._early_exit = True\n    178 self._trainer.callbacks = [lr_finder_callback] + self._trainer.callbacks\n--&gt; 180 self._trainer.fit(model, train_dataloaders, val_dataloaders, datamodule)\n    182 self._trainer.callbacks = [cb for cb in self._trainer.callbacks if cb is not lr_finder_callback]\n    184 return lr_finder_callback.optimal_lr\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:532, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\n    498 def fit(\n    499     self,\n    500     model: \"pl.LightningModule\",\n   (...)\n    504     ckpt_path: Optional[_PATH] = None,\n    505 ) -&gt; None:\n    506     r\"\"\"Runs the full optimization routine.\n    507 \n    508     Args:\n   (...)\n    530 \n    531     \"\"\"\n--&gt; 532     model = _maybe_unwrap_optimized(model)\n    533     self.strategy._lightning_module = model\n    534     _verify_strategy_supports_compile(model, self.strategy)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/compile.py:111, in _maybe_unwrap_optimized(model)\n    109     return model\n    110 _check_mixed_imports(model)\n--&gt; 111 raise TypeError(\n    112     f\"`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `{type(model).__qualname__}`\"\n    113 )\n\nTypeError: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `MLP`\n\n\n\n\n# 1-cycle sched\ncfg = OmegaConf.load('../config/model/image/mlp.yaml')\ncfg.scheduler.total_steps = len(datamodule.train_dataloader()) * MAX_EPOCHS\ncfg.scheduler.max_lr = lr_finder.suggestion()\nmodel = instantiate(cfg)\ntrainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n\n[19:57:34] INFO - MLP: init\n[19:57:34] INFO - MLP_X init\n[19:57:34] INFO - Classifier: init\n[19:57:47] INFO - Optimizer: AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 1e-05\n)\n[19:57:47] INFO - Scheduler: &lt;torch.optim.lr_scheduler.OneCycleLR object&gt;\n\n  | Name         | Type               | Params | Mode \n------------------------------------------------------------\n0 | loss         | CrossEntropyLoss   | 0      | train\n1 | train_acc    | MulticlassAccuracy | 0      | train\n2 | val_acc      | MulticlassAccuracy | 0      | train\n3 | test_acc     | MulticlassAccuracy | 0      | train\n4 | train_loss   | MeanMetric         | 0      | train\n5 | val_loss     | MeanMetric         | 0      | train\n6 | test_loss    | MeanMetric         | 0      | train\n7 | val_acc_best | MaxMetric          | 0      | train\n8 | nnet         | MLP                | 50.9 K | train\n------------------------------------------------------------\n50.9 K    Trainable params\n0         Non-trainable params\n50.9 K    Total params\n0.204     Total estimated model params size (MB)\n14        Modules in train mode\n0         Modules in eval mode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`Trainer.fit` stopped: `max_epochs=5` reached.\n\n\n\nplt.plot(lr_monitor.lrs['lr-AdamW'])\n\n\n\n\n\n\n\n\n\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head(5)\n\n\n\n\n\n\n\n\nepoch\nlr-AdamW\nstep\ntrain/acc_epoch\ntrain/acc_step\ntrain/loss_epoch\ntrain/loss_step\nval/acc\nval/acc_best\nval/loss\n\n\n\n\n0\nNaN\n0.002384\n49\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n0.0\nNaN\n49\nNaN\n0.800781\nNaN\n0.653588\nNaN\nNaN\nNaN\n\n\n2\nNaN\n0.005817\n99\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n0.0\nNaN\n99\nNaN\n0.894531\nNaN\n0.378841\nNaN\nNaN\nNaN\n\n\n4\n0.0\nNaN\n117\nNaN\nNaN\nNaN\nNaN\n0.924917\n0.924917\n0.262162\n\n\n\n\n\n\n\n\nplt.figure()\nplt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.show()\n\n\n\n\n\n\n\n\n\ntrainer.test(model, datamodule.test_dataloader())\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │    0.9690999984741211     │\n│         test/loss         │    0.10579969733953476    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[{'test/loss': 0.10579969733953476, 'test/acc': 0.9690999984741211}]",
    "crumbs": [
      "Image",
      "Models",
      "Multi Layer Perceptron"
    ]
  },
  {
    "objectID": "image.clip.html",
    "href": "image.clip.html",
    "title": "CLIP",
    "section": "",
    "text": "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1)  # we can \nprint(probs)\n\nCPU times: user 1e+03 ns, sys: 1 μs, total: 2 μs\nWall time: 3.81 μs\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\ntensor([[0.9949, 0.0051]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ndef embed_image(images):\n    if not isinstance(images, list): images = [images]\n    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n    with torch.no_grad(): return model.get_image_features(**inputs)\n\ndef embed_text(text):\n    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n    with torch.no_grad(): return model.get_text_features(**inputs)\n\ndef normalize(a): return a / a.norm(dim=-1, keepdim=True)\n\ndef cosine_sim(a, b): return normalize(a) @ normalize(b).T\n\ndef logits(a, b): return model.logit_scale.exp() * cosine_sim(a, b)\n\ndef probs(a, b): return logits(a, b).softmax(dim=0)\n\ndef classify(image, classes, template=\"a photo of a {}\"):\n    image_embs = embed_image(image)\n    text_embs = embed_text([template.format(o) for o in classes])\n    return probs(text_embs, image_embs)\n\ndef search(image_embs, query_embs):\n    sims = cosine_sim(image_embs, query_embs).flatten()\n    indices = sims.argsort(descending=True)\n    return indices, sims[indices]\n\ndef thumbnail(image, scale=3):\n    return image.resize(np.array(image.size)//scale)\n\n\npaintings = load_dataset(\"huggan/few-shot-art-painting\")\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\nall_image_embs_path = Path(\"paintings_embeddings.npy\")\nif all_image_embs_path.exists():\n    all_image_embs = torch.tensor(np.load(all_image_embs_path))\nelse:\n    all_image_embs = torch.cat([embed_image(row['image']) for row in tqdm(paintings['train'])])\n    np.save(all_image_embs_path, np.array(all_image_embs))"
  },
  {
    "objectID": "image.clip.html#hf",
    "href": "image.clip.html#hf",
    "title": "CLIP",
    "section": "",
    "text": "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image  # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1)  # we can \nprint(probs)\n\nCPU times: user 1e+03 ns, sys: 1 μs, total: 2 μs\nWall time: 3.81 μs\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n\n\ntensor([[0.9949, 0.0051]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ndef embed_image(images):\n    if not isinstance(images, list): images = [images]\n    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n    with torch.no_grad(): return model.get_image_features(**inputs)\n\ndef embed_text(text):\n    inputs = processor(text=text, return_tensors=\"pt\", padding=True)\n    with torch.no_grad(): return model.get_text_features(**inputs)\n\ndef normalize(a): return a / a.norm(dim=-1, keepdim=True)\n\ndef cosine_sim(a, b): return normalize(a) @ normalize(b).T\n\ndef logits(a, b): return model.logit_scale.exp() * cosine_sim(a, b)\n\ndef probs(a, b): return logits(a, b).softmax(dim=0)\n\ndef classify(image, classes, template=\"a photo of a {}\"):\n    image_embs = embed_image(image)\n    text_embs = embed_text([template.format(o) for o in classes])\n    return probs(text_embs, image_embs)\n\ndef search(image_embs, query_embs):\n    sims = cosine_sim(image_embs, query_embs).flatten()\n    indices = sims.argsort(descending=True)\n    return indices, sims[indices]\n\ndef thumbnail(image, scale=3):\n    return image.resize(np.array(image.size)//scale)\n\n\npaintings = load_dataset(\"huggan/few-shot-art-painting\")\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\nall_image_embs_path = Path(\"paintings_embeddings.npy\")\nif all_image_embs_path.exists():\n    all_image_embs = torch.tensor(np.load(all_image_embs_path))\nelse:\n    all_image_embs = torch.cat([embed_image(row['image']) for row in tqdm(paintings['train'])])\n    np.save(all_image_embs_path, np.array(all_image_embs))"
  },
  {
    "objectID": "image.datasets.html",
    "href": "image.datasets.html",
    "title": "Image Datasets",
    "section": "",
    "text": "source\n\n\n\n make_grid (images, size=64)\n\nGiven a list of PIL images, stack them together into a line for easy viewing\n\nsource\n\n\n\n\n show_images (x:torch.Tensor, ncols:int=8)\n\nGiven a batch of images x, make a grid and convert to PIL",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#plots",
    "href": "image.datasets.html#plots",
    "title": "Image Datasets",
    "section": "",
    "text": "source\n\n\n\n make_grid (images, size=64)\n\nGiven a list of PIL images, stack them together into a line for easy viewing\n\nsource\n\n\n\n\n show_images (x:torch.Tensor, ncols:int=8)\n\nGiven a batch of images x, make a grid and convert to PIL",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#imagedataset-mixin",
    "href": "image.datasets.html#imagedataset-mixin",
    "title": "Image Datasets",
    "section": "ImageDataset Mixin",
    "text": "ImageDataset Mixin\n\nsource\n\nImagePlotMixin\n\n ImagePlotMixin ()\n\nMixin class for image datasets providing visualization of (image, label) samples\n\n# ImagePlotMixin.plot(test, 0)\n# ImagePlotMixin.plot_grid(test, 2,2)\n# ImagePlotMixin.plot(test, 0, int2label = {0:'zero', 1:'one', 2:'two', 3:'three', 4:'four', 5:'five', 6:'six', 7:'seven', 8:'eight', 9:'nine'})\n# ImagePlotMixin.plot(test, 0, int2label = test.hf_ds.features['label'].int2str)\n# ImagePlotMixin.plot_grid(test, 2,2, int2label = test.hf_ds.features['label'].int2str)",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#image-dataset",
    "href": "image.datasets.html#image-dataset",
    "title": "Image Datasets",
    "section": "Image Dataset",
    "text": "Image Dataset\n\nsource\n\nImageDataset\n\n ImageDataset (name:str='mnist', *args,\n               data_dir:Optional[str]='../data/image', split='train', tran\n               sforms:Optional[torchvision.transforms.v2._container.Compos\n               e]=Compose(    ToTensor()), streaming:bool=False,\n               exclude_grey_scale=False, verification_mode='no_checks',\n               from_image_folder=False, from_disk=False)\n\nImage dataset\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\nmnist\n\n\n\nargs\nVAR_POSITIONAL\n\n\n\n\ndata_dir\nOptional\n../data/image\npath where data is saved if None default to hugging face cache\n\n\nsplit\nstr\ntrain\ntrain or test dataset\n\n\ntransforms\nOptional\nCompose( ToTensor())\n\n\n\nstreaming\nbool\nFalse\nTODO: support and test streaming datasest\n\n\nexclude_grey_scale\nbool\nFalse\n\n\n\nverification_mode\nstr\nno_checks\n\n\n\nfrom_image_folder\nbool\nFalse\n\n\n\nfrom_disk\nbool\nFalse\n\n\n\n\n\n\nImage normalization\nif both train and validation splits are available normalize both. else just train\n\nsource\n\n\nnormalize_image_datasets\n\n normalize_image_datasets (name, data_dir='../data/image',\n                           splits=['train', 'validation'])\n\n\nmean, std = normalize_image_datasets('slegroux/tiny-imagenet-200-clean')\nprint(f\"mean:{mean}, std: {std}\")",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#image-datamodule",
    "href": "image.datasets.html#image-datamodule",
    "title": "Image Datasets",
    "section": "Image DataModule",
    "text": "Image DataModule\n\nsource\n\nImageDataModule\n\n ImageDataModule (name:str, *args, data_dir:Optional[str]='~/Data/', trans\n                  forms:Union[torchvision.transforms.v2._container.Compose\n                  ,Callable,NoneType]=Compose(       ToTensor()\n                  Normalize(mean=[0.1307], std=[0.3081], inplace=False) ),\n                  train_val_split:List[float]=[0.8, 0.2],\n                  batch_size:int=64, num_workers:int=0,\n                  pin_memory:bool=False, persistent_workers:bool=False,\n                  **kwargs)\n\nMixin class for image datasets providing visualization of (image, label) samples\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nname of dataset from hugging face\n\n\nargs\nVAR_POSITIONAL\n\narguments to pass to hugging face dataset\n\n\ndata_dir\nOptional\n~/Data/\npath to source data dir where data is stored\n\n\ntransforms\nUnion\nCompose( ToTensor() Normalize(mean=[0.1307], std=[0.3081], inplace=False))\ntransform to apply to each sample\n\n\ntrain_val_split\nList\n[0.8, 0.2]\ntrain val test percentage\n\n\nbatch_size\nint\n64\nsize of compute batch\n\n\nnum_workers\nint\n0\nnum_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow\n\n\npin_memory\nbool\nFalse\nIf you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer\n\n\npersistent_workers\nbool\nFalse\npersist\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\n\nUsage\n\ndm = ImageDataModule(\n    'frgfm/imagenette','160px',\n    transforms=transforms.Compose([transforms.ToTensor(),transforms.Resize((128, 128))]),\n    data_dir='../data/image',\n    train_val_split=[0.8, 0.2],\n    batch_size = 16,\n    num_workers = 0, # main process\n    pin_memory= False,\n    persistent_workers=False,\n    exclude_grey_scale = True\n)\n\n# download or reference data from dir\ndm.prepare_data()\n\n# define train, eval, test subsets\ndm.setup()\nprint(f\" num_classes: {dm.num_classes}, labels: {dm.label_names}, img shape: {dm.train_ds[0][0].shape}\")\n# show data\ndm.show(1)\ndm.show_grid(3,3)\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n[18:42:31] INFO - Init ImageDataModule for frgfm/imagenette\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transforms' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transforms'])`.\n[18:42:33] INFO - loading dataset frgfm/imagenette with args ('160px',) from split train\n[18:42:34] WARNING - filtering out grey scale images\n[18:42:37] INFO - loading dataset frgfm/imagenette with args ('160px',) from split validation\n[18:42:38] WARNING - filtering out grey scale images\n[18:42:38] INFO - split train into train/val [0.8, 0.2]\n[18:42:38] INFO - train: 7437 val: 1859, test: 3856\n\n\n num_classes: 10, labels: ['tench', 'English springer', 'cassette player', 'chain saw', 'church', 'French horn', 'garbage truck', 'gas pump', 'golf ball', 'parachute'], img shape: torch.Size([3, 128, 128])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndm.hparams.batch_size = 32\nprint(dm.hparams)\n\nxb, yb = next(iter(dm.train_dataloader()))\n# print(xb.shape)\ndm.show_batch(xb)\n\n\"batch_size\":         32\n\"data_dir\":           ../data/image\n\"exclude_grey_scale\": True\n\"name\":               frgfm/imagenette\n\"num_workers\":        0\n\"persistent_workers\": False\n\"pin_memory\":         False\n\"train_val_split\":    [0.8, 0.2]\n\"transforms\":         Compose(\n      ToTensor()\n      Resize(size=[128, 128], interpolation=InterpolationMode.BILINEAR, antialias=True)\n)\n\n\n/var/folders/b5/v9y3kpzs29g41d99xvrdp3yr0000gn/T/ipykernel_78438/575801441.py:163: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n  grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n\n\n\n\n\n\n\n\n\n\n# dm.label_names\n\n['tench',\n 'English springer',\n 'cassette player',\n 'chain saw',\n 'church',\n 'French horn',\n 'garbage truck',\n 'gas pump',\n 'golf ball',\n 'parachute']\n\n\n\n# access data batches via dataloader\ntest_dl = dm.test_dataloader()\n# X,Y = next(iter(test_dl))\n# print(\"X dim(B,C,W,H): \", X.shape, \"Y: dim(B)\", Y.shape)\n\n\n\nConfig\n\n# cfg = OmegaConf.load(\"../config/image/data/mnist.yaml\")\n# print(cfg.datamodule)\n# dm = instantiate(cfg.datamodule)\n# dm.prepare_data()\n# dm.setup()\n# test_dl = dm.test_dataloader()\n# len(dm.test_ds), len(dm.train_ds), len(dm.val_ds)\ncfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\ndm = instantiate(cfg)\ndm.prepare_data()\ndm.setup()\nprint(f\"num_classes: {dm.num_classes}, batch_size: {dm.batch_size}\")\nprint(f\"labels: {dm.label_names}\")\nx, y = dm.test_ds[0]\nprint(f\"X: {x.shape}, Y: {y}\")\ndm.show(1)\ndm.show_grid(3,3)\n\n[17:28:55] INFO - Init ImageDataModule for fashion_mnist\n[17:29:08] INFO - split train into train/val [0.8, 0.2]\n[17:29:08] INFO - train: 48000 val: 12000, test: 10000\n\n\nnum_classes: 10, batch_size: 128\nlabels: ['T - shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nX: torch.Size([1, 32, 32]), Y: 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncfg = OmegaConf.load('../config/data/image/smithsonian_butterflies.yaml')\ndm = instantiate(cfg)\ndm.prepare_data()\ndm.setup()\nprint(f\"num_classes: {dm.num_classes}, batch_size: {dm.batch_size}\")\nprint(f\"labels: {dm.label_names}\")\nx, y = dm.test_ds[0]\nprint(f\"X: {x.shape}, Y: {y}\")\nx,y = next(iter(dm.train_dataloader()))\nprint(f\"X: {x.shape}, Y: {y.shape}\")\ndm.show(1)\n\n[17:40:22] INFO - Init ImageDataModule for huggan/smithsonian_butterflies_subset\nRepo card metadata block was not found. Setting CardData to empty.\n[17:40:22] WARNING - Repo card metadata block was not found. Setting CardData to empty.\nRepo card metadata block was not found. Setting CardData to empty.\n[17:40:24] WARNING - Repo card metadata block was not found. Setting CardData to empty.\n[17:40:29] WARNING - split train into train/val/test [0.8, 0.2] \n[17:40:29] INFO - train: 800 val: 200, test: 200\n[17:40:29] INFO - split train into train/val [0.8, 0.2]\nWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.8029231..1.0000005].\n[17:40:29] WARNING - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.8029231..1.0000005].\n\n\nnum_classes: 45, batch_size: 128\nlabels: ['Animalia, Arthropoda, Insecta, Lepidoptera, Pyralidae, Epipaschiinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Geometridae, Larentiinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Nymphalinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Satyrinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Saturniidae, Saturniinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Limenitidinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Papilionidae, Papilioninae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Pieridae, Dismorphiinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Arctiidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Lycaenidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Papilionidae, Parnassiinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Pyralidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Pieridae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Heliconinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Arctiidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Noctuidae, Erebinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Morphinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Pieridae, Coliadinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Pieridae, Pierinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Papilionidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Geometridae, Ennominae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Pieridae, Dismorphiinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Sphingidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Lasiocampidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Danainae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Pieridae, Pierinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Pieridae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Nymphalidae, Nymphalinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Saturniidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Papilionidae, Papilioninae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Geometridae, Oenochrominae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Glossata, Gelechiidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Pieridae, Coliadinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Lycaenidae, Lycaeninae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Uraniidae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Heliconiinae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Nymphalidae, Charaxinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Pterygota, Holometabola, Lepidoptera, Nymphalidae, Danainae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Geometridae, Sterrhinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Tortricidae', 'Animalia, Arthropoda, Insecta, Lepidoptera, Geometridae, Geometrinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Biblidinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Nymphalidae, Charaxinae', 'Animalia, Arthropoda, Hexapoda, Insecta, Lepidoptera, Lycaenidae, Polyommatinae']\nX: torch.Size([3, 128, 128]), Y: 7\nX: torch.Size([128, 3, 128, 128]), Y: torch.Size([128])",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#image-super-resolution-dataset",
    "href": "image.datasets.html#image-super-resolution-dataset",
    "title": "Image Datasets",
    "section": "Image Super Resolution Dataset",
    "text": "Image Super Resolution Dataset\n\nsource\n\nImageSuperResDataset\n\n ImageSuperResDataset (name:str='fashion_mnist',\n                       data_dir:str='../data/image', split='train', transf\n                       orm_x:Optional[torchvision.transforms.v2._container\n                       .Compose]=Compose(       ToTensor()       Compose(\n                       Resize(size=[32, 32],\n                       interpolation=InterpolationMode.BILINEAR,\n                       antialias=True)         Resize(size=[64, 64],\n                       interpolation=InterpolationMode.BILINEAR,\n                       antialias=True)   ) ), transform_y:Optional[torchvi\n                       sion.transforms.v2._container.Compose]=Compose(\n                       ToTensor()))\n\nImage dataset\n\nds = ImageSuperResDataset(\n    'slegroux/tiny-imagenet-200-clean',\n    data_dir='../data/image',\n    split='test'\n    )\n\n[20:53:06] INFO - loading dataset slegroux/tiny-imagenet-200-clean with args () from split test\n[20:53:06] INFO - loading dataset slegroux/tiny-imagenet-200-clean from split test\nOverwrite dataset info from restored data version if exists.\n[20:53:07] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[20:53:07] INFO - Loading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\nFound cached dataset tiny-imagenet-200-clean (/user/s/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\n[20:53:07] INFO - Found cached dataset tiny-imagenet-200-clean (/user/s/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[20:53:07] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n\n\n\nidx = torch.randint(0, len(ds), ())\nx,y = ds[idx]\nprint(x.shape, y.shape)\nfig, ax = plt.subplots(1,2)\nfig.suptitle('Super resolution')\nfig.tight_layout()\nax[0].imshow(x.permute(1,2,0).squeeze())\nax[0].set_title('low res')\nax[1].imshow(y.permute(1,2,0).squeeze())\nax[1].set_title('high res')\n\ntorch.Size([3, 64, 64])\ntorch.Size([3, 64, 64])\n\n\n\nText(0.5, 1.0, 'high res')",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "image.datasets.html#image-superres-datamodule",
    "href": "image.datasets.html#image-superres-datamodule",
    "title": "Image Datasets",
    "section": "Image SuperRes DataModule",
    "text": "Image SuperRes DataModule\n\nsource\n\nImageSuperResDataModule\n\n ImageSuperResDataModule (name:str='slegroux/tiny-imagenet-200-clean',\n                          data_dir:str='../data/image', transform_x:Option\n                          al[torchvision.transforms.v2._container.Compose]\n                          =Sequential(   (0): ToTensor()   (1):\n                          Sequential(     (0): Resize(size=[32, 32],\n                          interpolation=InterpolationMode.BILINEAR,\n                          antialias=True)     (1): Resize(size=[64, 64],\n                          interpolation=InterpolationMode.BILINEAR,\n                          antialias=True)   ) ), transform_y:Optional[torc\n                          hvision.transforms.v2._container.Compose]=Sequen\n                          tial(   (0): ToTensor() ),\n                          train_val_split:Optional[List[float]]=[0.8,\n                          0.2], batch_size:int=64, num_workers:int=0,\n                          pin_memory:bool=False,\n                          persistent_workers:bool=False)\n\nMixin class for image datasets providing visualization of (image, label) samples\n\ndm = ImageSuperResDataModule(\n    'fashion_mnist',\n    data_dir='../data/image',\n    transform_x=transforms.Compose([transforms.ToTensor(), transforms.Resize(32)]),\n    transform_y=transforms.Compose([transforms.ToTensor(), transforms.Resize(32)]),\n)\n\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n  warnings.warn(\n[21:24:09] INFO - Init ImageSuperResDataModule for fashion_mnist\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transform_x' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transform_x'])`.\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transform_y' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transform_y'])`.\n[21:24:09] INFO - Init ImageDataModule for fashion_mnist\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transforms' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transforms'])`.\n\n\n\ndm.prepare_data()\ndm.setup()\n\n[21:09:03] INFO - loading dataset fashion_mnist with args () from split train\n[21:09:03] INFO - loading dataset fashion_mnist from split train\nOverwrite dataset info from restored data version if exists.\n[21:09:05] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:05] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[21:09:05] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:05] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:09] INFO - loading dataset fashion_mnist with args () from split test\n[21:09:09] INFO - loading dataset fashion_mnist from split test\nOverwrite dataset info from restored data version if exists.\n[21:09:11] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:11] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[21:09:11] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:11] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:13] INFO - loading dataset fashion_mnist with args () from split test\n[21:09:13] INFO - loading dataset fashion_mnist from split test\nOverwrite dataset info from restored data version if exists.\n[21:09:15] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:15] INFO - Loading Dataset info from ../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\nFound cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\n[21:09:15] INFO - Found cached dataset fashion_mnist (/user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2)\nLoading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:15] INFO - Loading Dataset info from /user/s/slegroux/Projects/nimrod/nbs/../data/image/fashion_mnist/fashion_mnist/0.0.0/531be5e2ccc9dba0c201ad3ae567a4f3d16ecdd2\n[21:09:15] WARNING - same dataset for validation and test\n\n\n\ndm.show(torch.randint(0, len(dm.train_dataloader()),(1,)))\n\n\n\n\n\n\n\n\n\ncfg = OmegaConf.load('../config/data/image/tiny_imagenet_superres.yaml')\ndm = instantiate(cfg)\ndm.prepare_data()\n# dm.setup()\ndm.show(0)\n\n[10:59:53] INFO - Init ImageSuperResDataModule for slegroux/tiny-imagenet-200-clean\n[10:59:53] INFO - Init ImageDataModule for slegroux/tiny-imagenet-200-clean\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'transforms' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['transforms'])`.\n[10:59:56] INFO - loading dataset slegroux/tiny-imagenet-200-clean with args () from split train\n[10:59:56] INFO - loading dataset slegroux/tiny-imagenet-200-clean from split train\nOverwrite dataset info from restored data version if exists.\n[10:59:58] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[10:59:58] INFO - Loading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\nFound cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\n[10:59:58] INFO - Found cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\nLoading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[10:59:58] INFO - Loading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:05] INFO - loading dataset slegroux/tiny-imagenet-200-clean with args () from split test\n[11:00:05] INFO - loading dataset slegroux/tiny-imagenet-200-clean from split test\nOverwrite dataset info from restored data version if exists.\n[11:00:07] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:07] INFO - Loading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\nFound cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\n[11:00:07] INFO - Found cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\nLoading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:07] INFO - Loading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:10] INFO - loading dataset slegroux/tiny-imagenet-200-clean with args () from split validation\n[11:00:10] INFO - loading dataset slegroux/tiny-imagenet-200-clean from split validation\nOverwrite dataset info from restored data version if exists.\n[11:00:12] INFO - Overwrite dataset info from restored data version if exists.\nLoading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:12] INFO - Loading Dataset info from ../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\nFound cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\n[11:00:12] INFO - Found cached dataset tiny-imagenet-200-clean (/Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2)\nLoading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n[11:00:12] INFO - Loading Dataset info from /Users/slegroux/Projects/nimrod/nbs/../data/image/slegroux___tiny-imagenet-200-clean/default/0.0.0/4b908d89fab3eb36aa8ebcd41c1996b28da7d6f2\n\n\n\n\n\n\n\n\n\n\nx_mean = torch.tensor([0.4822, 0.4495, 0.3985])\nx_std = torch.tensor([0.2771, 0.2690, 0.2826])\n\ntfm_norm = transforms.Normalize(mean=x_mean, std=x_std)\ntfm_denorm = transforms.Compose([transforms.Normalize(mean=[0,0,0], std=1/x_std), transforms.Normalize(mean=-x_mean, std=[1,1,1])])\n\nx, y = dm.train_ds[0]\nx, y = tfm_denorm(x), tfm_denorm(y)\nfig, ax = plt.subplots(1,2, figsize=(4,4))\nfig.tight_layout()\nax[0].imshow(x.permute(1,2,0).squeeze())\nax[0].set_title('low res')\nax[1].imshow(y.permute(1,2,0).squeeze())\nax[1].set_title('high res')\n\nText(0.5, 1.0, 'high res')",
    "crumbs": [
      "Image",
      "Data",
      "Image Datasets"
    ]
  },
  {
    "objectID": "models.lm.html",
    "href": "models.lm.html",
    "title": "Neural Net Language Models",
    "section": "",
    "text": "# N_EPOCHS for training debuggging\nITER_MAX = 1\nset_seed(42)\n\nSeed set to 42\n# reading with pandas\ndf = pd.read_csv('../data/text/names.txt', header=None, names=['name'])\ndata = list(df.name)\nprint(\"names: \", data[:3])\n\nnames:  ['emma', 'olivia', 'ava']",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#data-formatting",
    "href": "models.lm.html#data-formatting",
    "title": "Neural Net Language Models",
    "section": "Data formatting",
    "text": "Data formatting\ngiven last n tokens we predict token n+1\n\ns = list(\"alexandra\")\nprint(s)\nbigram = [(x,y) for x, y in zip(s, s[1:])]\nprint(bigram)\ntrigram = [ (x,y,z) for x, y, z in zip(s, s[1:], s[2:])]\nprint(trigram)\n\n['a', 'l', 'e', 'x', 'a', 'n', 'd', 'r', 'a']\n[('a', 'l'), ('l', 'e'), ('e', 'x'), ('x', 'a'), ('a', 'n'), ('n', 'd'), ('d', 'r'), ('r', 'a')]\n[('a', 'l', 'e'), ('l', 'e', 'x'), ('e', 'x', 'a'), ('x', 'a', 'n'), ('a', 'n', 'd'), ('n', 'd', 'r'), ('d', 'r', 'a')]\n\n\n\nTiny shakespeare LM char dataset\n\n# reading directly in plain python\nlines = []\nwith open('../data/text/tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n    for line in f.readlines():\n        if line.strip():\n            # only append non blank lines\n            lines.append(line)\n\n# add sentence tokens\n# data = [['&lt;bos&gt;'] +list(line.strip()) + ['&lt;eos&gt;'] for line in lines]\n# data = [list(line.strip()) for line in lines]\ndata = [list(line) for line in lines]\nprint(\"data: \", data[:3])\n\ndata:  [['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n'], ['B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n'], ['A', 'l', 'l', ':', '\\n']]\n\n\n\ndef make_dataset(\n        words:List[str], # data is a list of sentences which are a list of words\n        v:Vocab,# vocabulary class for mapping words to indices\n        verbose:bool=False, # print debug info\n        context_length=3 # number of words/tokens to use as context\n        ):\n    X = []\n    y = []\n    for word in words:\n        s = list(word)\n        if verbose:\n            print('row: ', s)\n        # init prefix with padding while len &lt; context_length\n        for i in range(context_length-1):\n            sequence = v.stoi(s[:i+1])\n            pad_len = context_length - len(sequence)\n            pad = [v.stoi(\"&lt;pad&gt;\")] * pad_len\n            X.append(pad + sequence)\n            y.append(v.stoi(s[i+1]))\n\n            if verbose:\n                print([\"&lt;pad&gt;\"]+ s[:i+1], s[i+1])\n\n        # for length seq = context_length\n        i = 0\n        while i &lt; (len(s) - context_length):\n            X.append(v.stoi(s[i:context_length+i]))\n            y.append(v.stoi(s[i+context_length]))\n            if verbose:\n                print(s[i:context_length+i], s[i+context_length])\n            i += 1\n    return torch.tensor(X),torch.tensor(y)\n\nfor each row in the dataset we expand all the combinations of ngrams\n\nv = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['&lt;unk&gt;','&lt;pad&gt;'])\nprint(\"vocabulary: \", v.vocabulary)\nprint(\"vocabulary size: \", len(v))\n\n[21:05:26] INFO - Vocab: read text file\n\n\nvocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '&lt;pad&gt;', '&lt;unk&gt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nvocabulary size:  67\n\n\n\nCONTEXT_LEN = 3\nX, y = make_dataset(data[:80], v, verbose=True, context_length=CONTEXT_LEN)\nprint(\"X: \", X.shape, \"y:\", y.shape)\n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'a', 'n', 'y', ' ', 'f', 'u', 'r', 't', 'h', 'e', 'r', ',', ' ', 'h', 'e', 'a', 'r', ' ', 'm', 'e', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n']\n['&lt;pad&gt;', 'B'] e\n['&lt;pad&gt;', 'B', 'e'] f\n['B', 'e', 'f'] o\n['e', 'f', 'o'] r\n['f', 'o', 'r'] e\n['o', 'r', 'e']  \n['r', 'e', ' '] w\n['e', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] p\n['e', ' ', 'p'] r\n[' ', 'p', 'r'] o\n['p', 'r', 'o'] c\n['r', 'o', 'c'] e\n['o', 'c', 'e'] e\n['c', 'e', 'e'] d\n['e', 'e', 'd']  \n['e', 'd', ' '] a\n['d', ' ', 'a'] n\n[' ', 'a', 'n'] y\n['a', 'n', 'y']  \n['n', 'y', ' '] f\n['y', ' ', 'f'] u\n[' ', 'f', 'u'] r\n['f', 'u', 'r'] t\n['u', 'r', 't'] h\n['r', 't', 'h'] e\n['t', 'h', 'e'] r\n['h', 'e', 'r'] ,\n['e', 'r', ',']  \n['r', ',', ' '] h\n[',', ' ', 'h'] e\n[' ', 'h', 'e'] a\n['h', 'e', 'a'] r\n['e', 'a', 'r']  \n['a', 'r', ' '] m\n['r', ' ', 'm'] e\n[' ', 'm', 'e']  \n['m', 'e', ' '] s\n['e', ' ', 's'] p\n[' ', 's', 'p'] e\n['s', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k'] .\n['a', 'k', '.'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['S', 'p', 'e', 'a', 'k', ',', ' ', 's', 'p', 'e', 'a', 'k', '.', '\\n']\n['&lt;pad&gt;', 'S'] p\n['&lt;pad&gt;', 'S', 'p'] e\n['S', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k'] ,\n['a', 'k', ',']  \n['k', ',', ' '] s\n[',', ' ', 's'] p\n[' ', 's', 'p'] e\n['s', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k'] .\n['a', 'k', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['Y', 'o', 'u', ' ', 'a', 'r', 'e', ' ', 'a', 'l', 'l', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', ' ', 'r', 'a', 't', 'h', 'e', 'r', ' ', 't', 'o', ' ', 'd', 'i', 'e', ' ', 't', 'h', 'a', 'n', ' ', 't', 'o', ' ', 'f', 'a', 'm', 'i', 's', 'h', '?', '\\n']\n['&lt;pad&gt;', 'Y'] o\n['&lt;pad&gt;', 'Y', 'o'] u\n['Y', 'o', 'u']  \n['o', 'u', ' '] a\n['u', ' ', 'a'] r\n[' ', 'a', 'r'] e\n['a', 'r', 'e']  \n['r', 'e', ' '] a\n['e', ' ', 'a'] l\n[' ', 'a', 'l'] l\n['a', 'l', 'l']  \n['l', 'l', ' '] r\n['l', ' ', 'r'] e\n[' ', 'r', 'e'] s\n['r', 'e', 's'] o\n['e', 's', 'o'] l\n['s', 'o', 'l'] v\n['o', 'l', 'v'] e\n['l', 'v', 'e'] d\n['v', 'e', 'd']  \n['e', 'd', ' '] r\n['d', ' ', 'r'] a\n[' ', 'r', 'a'] t\n['r', 'a', 't'] h\n['a', 't', 'h'] e\n['t', 'h', 'e'] r\n['h', 'e', 'r']  \n['e', 'r', ' '] t\n['r', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] d\n['o', ' ', 'd'] i\n[' ', 'd', 'i'] e\n['d', 'i', 'e']  \n['i', 'e', ' '] t\n['e', ' ', 't'] h\n[' ', 't', 'h'] a\n['t', 'h', 'a'] n\n['h', 'a', 'n']  \n['a', 'n', ' '] t\n['n', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] f\n['o', ' ', 'f'] a\n[' ', 'f', 'a'] m\n['f', 'a', 'm'] i\n['a', 'm', 'i'] s\n['m', 'i', 's'] h\n['i', 's', 'h'] ?\n['s', 'h', '?'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['R', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', ' ', 'r', 'e', 's', 'o', 'l', 'v', 'e', 'd', '.', '\\n']\n['&lt;pad&gt;', 'R'] e\n['&lt;pad&gt;', 'R', 'e'] s\n['R', 'e', 's'] o\n['e', 's', 'o'] l\n['s', 'o', 'l'] v\n['o', 'l', 'v'] e\n['l', 'v', 'e'] d\n['v', 'e', 'd'] .\n['e', 'd', '.']  \n['d', '.', ' '] r\n['.', ' ', 'r'] e\n[' ', 'r', 'e'] s\n['r', 'e', 's'] o\n['e', 's', 'o'] l\n['s', 'o', 'l'] v\n['o', 'l', 'v'] e\n['l', 'v', 'e'] d\n['v', 'e', 'd'] .\n['e', 'd', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['F', 'i', 'r', 's', 't', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ' ', 'C', 'a', 'i', 'u', 's', ' ', 'M', 'a', 'r', 'c', 'i', 'u', 's', ' ', 'i', 's', ' ', 'c', 'h', 'i', 'e', 'f', ' ', 'e', 'n', 'e', 'm', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't'] ,\n['s', 't', ',']  \n['t', ',', ' '] y\n[',', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u']  \n['o', 'u', ' '] k\n['u', ' ', 'k'] n\n[' ', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w']  \n['o', 'w', ' '] C\n['w', ' ', 'C'] a\n[' ', 'C', 'a'] i\n['C', 'a', 'i'] u\n['a', 'i', 'u'] s\n['i', 'u', 's']  \n['u', 's', ' '] M\n['s', ' ', 'M'] a\n[' ', 'M', 'a'] r\n['M', 'a', 'r'] c\n['a', 'r', 'c'] i\n['r', 'c', 'i'] u\n['c', 'i', 'u'] s\n['i', 'u', 's']  \n['u', 's', ' '] i\n['s', ' ', 'i'] s\n[' ', 'i', 's']  \n['i', 's', ' '] c\n['s', ' ', 'c'] h\n[' ', 'c', 'h'] i\n['c', 'h', 'i'] e\n['h', 'i', 'e'] f\n['i', 'e', 'f']  \n['e', 'f', ' '] e\n['f', ' ', 'e'] n\n[' ', 'e', 'n'] e\n['e', 'n', 'e'] m\n['n', 'e', 'm'] y\n['e', 'm', 'y']  \n['m', 'y', ' '] t\n['y', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] p\n['e', ' ', 'p'] e\n[' ', 'p', 'e'] o\n['p', 'e', 'o'] p\n['e', 'o', 'p'] l\n['o', 'p', 'l'] e\n['p', 'l', 'e'] .\n['l', 'e', '.'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['W', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', ',', ' ', 'w', 'e', ' ', 'k', 'n', 'o', 'w', \"'\", 't', '.', '\\n']\n['&lt;pad&gt;', 'W'] e\n['&lt;pad&gt;', 'W', 'e']  \n['W', 'e', ' '] k\n['e', ' ', 'k'] n\n[' ', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w'] '\n['o', 'w', \"'\"] t\n['w', \"'\", 't'] ,\n[\"'\", 't', ',']  \n['t', ',', ' '] w\n[',', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] k\n['e', ' ', 'k'] n\n[' ', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w'] '\n['o', 'w', \"'\"] t\n['w', \"'\", 't'] .\n[\"'\", 't', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['L', 'e', 't', ' ', 'u', 's', ' ', 'k', 'i', 'l', 'l', ' ', 'h', 'i', 'm', ',', ' ', 'a', 'n', 'd', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 'h', 'a', 'v', 'e', ' ', 'c', 'o', 'r', 'n', ' ', 'a', 't', ' ', 'o', 'u', 'r', ' ', 'o', 'w', 'n', ' ', 'p', 'r', 'i', 'c', 'e', '.', '\\n']\n['&lt;pad&gt;', 'L'] e\n['&lt;pad&gt;', 'L', 'e'] t\n['L', 'e', 't']  \n['e', 't', ' '] u\n['t', ' ', 'u'] s\n[' ', 'u', 's']  \n['u', 's', ' '] k\n['s', ' ', 'k'] i\n[' ', 'k', 'i'] l\n['k', 'i', 'l'] l\n['i', 'l', 'l']  \n['l', 'l', ' '] h\n['l', ' ', 'h'] i\n[' ', 'h', 'i'] m\n['h', 'i', 'm'] ,\n['i', 'm', ',']  \n['m', ',', ' '] a\n[',', ' ', 'a'] n\n[' ', 'a', 'n'] d\n['a', 'n', 'd']  \n['n', 'd', ' '] w\n['d', ' ', 'w'] e\n[' ', 'w', 'e'] '\n['w', 'e', \"'\"] l\n['e', \"'\", 'l'] l\n[\"'\", 'l', 'l']  \n['l', 'l', ' '] h\n['l', ' ', 'h'] a\n[' ', 'h', 'a'] v\n['h', 'a', 'v'] e\n['a', 'v', 'e']  \n['v', 'e', ' '] c\n['e', ' ', 'c'] o\n[' ', 'c', 'o'] r\n['c', 'o', 'r'] n\n['o', 'r', 'n']  \n['r', 'n', ' '] a\n['n', ' ', 'a'] t\n[' ', 'a', 't']  \n['a', 't', ' '] o\n['t', ' ', 'o'] u\n[' ', 'o', 'u'] r\n['o', 'u', 'r']  \n['u', 'r', ' '] o\n['r', ' ', 'o'] w\n[' ', 'o', 'w'] n\n['o', 'w', 'n']  \n['w', 'n', ' '] p\n['n', ' ', 'p'] r\n[' ', 'p', 'r'] i\n['p', 'r', 'i'] c\n['r', 'i', 'c'] e\n['i', 'c', 'e'] .\n['c', 'e', '.'] \n\nrow:  ['I', 's', \"'\", 't', ' ', 'a', ' ', 'v', 'e', 'r', 'd', 'i', 'c', 't', '?', '\\n']\n['&lt;pad&gt;', 'I'] s\n['&lt;pad&gt;', 'I', 's'] '\n['I', 's', \"'\"] t\n['s', \"'\", 't']  \n[\"'\", 't', ' '] a\n['t', ' ', 'a']  \n[' ', 'a', ' '] v\n['a', ' ', 'v'] e\n[' ', 'v', 'e'] r\n['v', 'e', 'r'] d\n['e', 'r', 'd'] i\n['r', 'd', 'i'] c\n['d', 'i', 'c'] t\n['i', 'c', 't'] ?\n['c', 't', '?'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['N', 'o', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'o', 'n', \"'\", 't', ';', ' ', 'l', 'e', 't', ' ', 'i', 't', ' ', 'b', 'e', ' ', 'd', 'o', 'n', 'e', ':', ' ', 'a', 'w', 'a', 'y', ',', ' ', 'a', 'w', 'a', 'y', '!', '\\n']\n['&lt;pad&gt;', 'N'] o\n['&lt;pad&gt;', 'N', 'o']  \n['N', 'o', ' '] m\n['o', ' ', 'm'] o\n[' ', 'm', 'o'] r\n['m', 'o', 'r'] e\n['o', 'r', 'e']  \n['r', 'e', ' '] t\n['e', ' ', 't'] a\n[' ', 't', 'a'] l\n['t', 'a', 'l'] k\n['a', 'l', 'k'] i\n['l', 'k', 'i'] n\n['k', 'i', 'n'] g\n['i', 'n', 'g']  \n['n', 'g', ' '] o\n['g', ' ', 'o'] n\n[' ', 'o', 'n'] '\n['o', 'n', \"'\"] t\n['n', \"'\", 't'] ;\n[\"'\", 't', ';']  \n['t', ';', ' '] l\n[';', ' ', 'l'] e\n[' ', 'l', 'e'] t\n['l', 'e', 't']  \n['e', 't', ' '] i\n['t', ' ', 'i'] t\n[' ', 'i', 't']  \n['i', 't', ' '] b\n['t', ' ', 'b'] e\n[' ', 'b', 'e']  \n['b', 'e', ' '] d\n['e', ' ', 'd'] o\n[' ', 'd', 'o'] n\n['d', 'o', 'n'] e\n['o', 'n', 'e'] :\n['n', 'e', ':']  \n['e', ':', ' '] a\n[':', ' ', 'a'] w\n[' ', 'a', 'w'] a\n['a', 'w', 'a'] y\n['w', 'a', 'y'] ,\n['a', 'y', ',']  \n['y', ',', ' '] a\n[',', ' ', 'a'] w\n[' ', 'a', 'w'] a\n['a', 'w', 'a'] y\n['w', 'a', 'y'] !\n['a', 'y', '!'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['O', 'n', 'e', ' ', 'w', 'o', 'r', 'd', ',', ' ', 'g', 'o', 'o', 'd', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', '.', '\\n']\n['&lt;pad&gt;', 'O'] n\n['&lt;pad&gt;', 'O', 'n'] e\n['O', 'n', 'e']  \n['n', 'e', ' '] w\n['e', ' ', 'w'] o\n[' ', 'w', 'o'] r\n['w', 'o', 'r'] d\n['o', 'r', 'd'] ,\n['r', 'd', ',']  \n['d', ',', ' '] g\n[',', ' ', 'g'] o\n[' ', 'g', 'o'] o\n['g', 'o', 'o'] d\n['o', 'o', 'd']  \n['o', 'd', ' '] c\n['d', ' ', 'c'] i\n[' ', 'c', 'i'] t\n['c', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] s\n['e', 'n', 's'] .\n['n', 's', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['W', 'e', ' ', 'a', 'r', 'e', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', 'e', 'd', ' ', 'p', 'o', 'o', 'r', ' ', 'c', 'i', 't', 'i', 'z', 'e', 'n', 's', ',', ' ', 't', 'h', 'e', ' ', 'p', 'a', 't', 'r', 'i', 'c', 'i', 'a', 'n', 's', ' ', 'g', 'o', 'o', 'd', '.', '\\n']\n['&lt;pad&gt;', 'W'] e\n['&lt;pad&gt;', 'W', 'e']  \n['W', 'e', ' '] a\n['e', ' ', 'a'] r\n[' ', 'a', 'r'] e\n['a', 'r', 'e']  \n['r', 'e', ' '] a\n['e', ' ', 'a'] c\n[' ', 'a', 'c'] c\n['a', 'c', 'c'] o\n['c', 'c', 'o'] u\n['c', 'o', 'u'] n\n['o', 'u', 'n'] t\n['u', 'n', 't'] e\n['n', 't', 'e'] d\n['t', 'e', 'd']  \n['e', 'd', ' '] p\n['d', ' ', 'p'] o\n[' ', 'p', 'o'] o\n['p', 'o', 'o'] r\n['o', 'o', 'r']  \n['o', 'r', ' '] c\n['r', ' ', 'c'] i\n[' ', 'c', 'i'] t\n['c', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] s\n['e', 'n', 's'] ,\n['n', 's', ',']  \n['s', ',', ' '] t\n[',', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] p\n['e', ' ', 'p'] a\n[' ', 'p', 'a'] t\n['p', 'a', 't'] r\n['a', 't', 'r'] i\n['t', 'r', 'i'] c\n['r', 'i', 'c'] i\n['i', 'c', 'i'] a\n['c', 'i', 'a'] n\n['i', 'a', 'n'] s\n['a', 'n', 's']  \n['n', 's', ' '] g\n['s', ' ', 'g'] o\n[' ', 'g', 'o'] o\n['g', 'o', 'o'] d\n['o', 'o', 'd'] .\n['o', 'd', '.'] \n\nrow:  ['W', 'h', 'a', 't', ' ', 'a', 'u', 't', 'h', 'o', 'r', 'i', 't', 'y', ' ', 's', 'u', 'r', 'f', 'e', 'i', 't', 's', ' ', 'o', 'n', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'r', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'u', 's', ':', ' ', 'i', 'f', ' ', 't', 'h', 'e', 'y', '\\n']\n['&lt;pad&gt;', 'W'] h\n['&lt;pad&gt;', 'W', 'h'] a\n['W', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] a\n['t', ' ', 'a'] u\n[' ', 'a', 'u'] t\n['a', 'u', 't'] h\n['u', 't', 'h'] o\n['t', 'h', 'o'] r\n['h', 'o', 'r'] i\n['o', 'r', 'i'] t\n['r', 'i', 't'] y\n['i', 't', 'y']  \n['t', 'y', ' '] s\n['y', ' ', 's'] u\n[' ', 's', 'u'] r\n['s', 'u', 'r'] f\n['u', 'r', 'f'] e\n['r', 'f', 'e'] i\n['f', 'e', 'i'] t\n['e', 'i', 't'] s\n['i', 't', 's']  \n['t', 's', ' '] o\n['s', ' ', 'o'] n\n[' ', 'o', 'n']  \n['o', 'n', ' '] w\n['n', ' ', 'w'] o\n[' ', 'w', 'o'] u\n['w', 'o', 'u'] l\n['o', 'u', 'l'] d\n['u', 'l', 'd']  \n['l', 'd', ' '] r\n['d', ' ', 'r'] e\n[' ', 'r', 'e'] l\n['r', 'e', 'l'] i\n['e', 'l', 'i'] e\n['l', 'i', 'e'] v\n['i', 'e', 'v'] e\n['e', 'v', 'e']  \n['v', 'e', ' '] u\n['e', ' ', 'u'] s\n[' ', 'u', 's'] :\n['u', 's', ':']  \n['s', ':', ' '] i\n[':', ' ', 'i'] f\n[' ', 'i', 'f']  \n['i', 'f', ' '] t\n['f', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] y\n['h', 'e', 'y'] \n\nrow:  ['w', 'o', 'u', 'l', 'd', ' ', 'y', 'i', 'e', 'l', 'd', ' ', 'u', 's', ' ', 'b', 'u', 't', ' ', 't', 'h', 'e', ' ', 's', 'u', 'p', 'e', 'r', 'f', 'l', 'u', 'i', 't', 'y', ',', ' ', 'w', 'h', 'i', 'l', 'e', ' ', 'i', 't', ' ', 'w', 'e', 'r', 'e', '\\n']\n['&lt;pad&gt;', 'w'] o\n['&lt;pad&gt;', 'w', 'o'] u\n['w', 'o', 'u'] l\n['o', 'u', 'l'] d\n['u', 'l', 'd']  \n['l', 'd', ' '] y\n['d', ' ', 'y'] i\n[' ', 'y', 'i'] e\n['y', 'i', 'e'] l\n['i', 'e', 'l'] d\n['e', 'l', 'd']  \n['l', 'd', ' '] u\n['d', ' ', 'u'] s\n[' ', 'u', 's']  \n['u', 's', ' '] b\n['s', ' ', 'b'] u\n[' ', 'b', 'u'] t\n['b', 'u', 't']  \n['u', 't', ' '] t\n['t', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] s\n['e', ' ', 's'] u\n[' ', 's', 'u'] p\n['s', 'u', 'p'] e\n['u', 'p', 'e'] r\n['p', 'e', 'r'] f\n['e', 'r', 'f'] l\n['r', 'f', 'l'] u\n['f', 'l', 'u'] i\n['l', 'u', 'i'] t\n['u', 'i', 't'] y\n['i', 't', 'y'] ,\n['t', 'y', ',']  \n['y', ',', ' '] w\n[',', ' ', 'w'] h\n[' ', 'w', 'h'] i\n['w', 'h', 'i'] l\n['h', 'i', 'l'] e\n['i', 'l', 'e']  \n['l', 'e', ' '] i\n['e', ' ', 'i'] t\n[' ', 'i', 't']  \n['i', 't', ' '] w\n['t', ' ', 'w'] e\n[' ', 'w', 'e'] r\n['w', 'e', 'r'] e\n['e', 'r', 'e'] \n\nrow:  ['w', 'h', 'o', 'l', 'e', 's', 'o', 'm', 'e', ',', ' ', 'w', 'e', ' ', 'm', 'i', 'g', 'h', 't', ' ', 'g', 'u', 'e', 's', 's', ' ', 't', 'h', 'e', 'y', ' ', 'r', 'e', 'l', 'i', 'e', 'v', 'e', 'd', ' ', 'u', 's', ' ', 'h', 'u', 'm', 'a', 'n', 'e', 'l', 'y', ';', '\\n']\n['&lt;pad&gt;', 'w'] h\n['&lt;pad&gt;', 'w', 'h'] o\n['w', 'h', 'o'] l\n['h', 'o', 'l'] e\n['o', 'l', 'e'] s\n['l', 'e', 's'] o\n['e', 's', 'o'] m\n['s', 'o', 'm'] e\n['o', 'm', 'e'] ,\n['m', 'e', ',']  \n['e', ',', ' '] w\n[',', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] m\n['e', ' ', 'm'] i\n[' ', 'm', 'i'] g\n['m', 'i', 'g'] h\n['i', 'g', 'h'] t\n['g', 'h', 't']  \n['h', 't', ' '] g\n['t', ' ', 'g'] u\n[' ', 'g', 'u'] e\n['g', 'u', 'e'] s\n['u', 'e', 's'] s\n['e', 's', 's']  \n['s', 's', ' '] t\n['s', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] y\n['h', 'e', 'y']  \n['e', 'y', ' '] r\n['y', ' ', 'r'] e\n[' ', 'r', 'e'] l\n['r', 'e', 'l'] i\n['e', 'l', 'i'] e\n['l', 'i', 'e'] v\n['i', 'e', 'v'] e\n['e', 'v', 'e'] d\n['v', 'e', 'd']  \n['e', 'd', ' '] u\n['d', ' ', 'u'] s\n[' ', 'u', 's']  \n['u', 's', ' '] h\n['s', ' ', 'h'] u\n[' ', 'h', 'u'] m\n['h', 'u', 'm'] a\n['u', 'm', 'a'] n\n['m', 'a', 'n'] e\n['a', 'n', 'e'] l\n['n', 'e', 'l'] y\n['e', 'l', 'y'] ;\n['l', 'y', ';'] \n\nrow:  ['b', 'u', 't', ' ', 't', 'h', 'e', 'y', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 't', 'o', 'o', ' ', 'd', 'e', 'a', 'r', ':', ' ', 't', 'h', 'e', ' ', 'l', 'e', 'a', 'n', 'n', 'e', 's', 's', ' ', 't', 'h', 'a', 't', '\\n']\n['&lt;pad&gt;', 'b'] u\n['&lt;pad&gt;', 'b', 'u'] t\n['b', 'u', 't']  \n['u', 't', ' '] t\n['t', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] y\n['h', 'e', 'y']  \n['e', 'y', ' '] t\n['y', ' ', 't'] h\n[' ', 't', 'h'] i\n['t', 'h', 'i'] n\n['h', 'i', 'n'] k\n['i', 'n', 'k']  \n['n', 'k', ' '] w\n['k', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] a\n['e', ' ', 'a'] r\n[' ', 'a', 'r'] e\n['a', 'r', 'e']  \n['r', 'e', ' '] t\n['e', ' ', 't'] o\n[' ', 't', 'o'] o\n['t', 'o', 'o']  \n['o', 'o', ' '] d\n['o', ' ', 'd'] e\n[' ', 'd', 'e'] a\n['d', 'e', 'a'] r\n['e', 'a', 'r'] :\n['a', 'r', ':']  \n['r', ':', ' '] t\n[':', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] l\n['e', ' ', 'l'] e\n[' ', 'l', 'e'] a\n['l', 'e', 'a'] n\n['e', 'a', 'n'] n\n['a', 'n', 'n'] e\n['n', 'n', 'e'] s\n['n', 'e', 's'] s\n['e', 's', 's']  \n['s', 's', ' '] t\n['s', ' ', 't'] h\n[' ', 't', 'h'] a\n['t', 'h', 'a'] t\n['h', 'a', 't'] \n\nrow:  ['a', 'f', 'f', 'l', 'i', 'c', 't', 's', ' ', 'u', 's', ',', ' ', 't', 'h', 'e', ' ', 'o', 'b', 'j', 'e', 'c', 't', ' ', 'o', 'f', ' ', 'o', 'u', 'r', ' ', 'm', 'i', 's', 'e', 'r', 'y', ',', ' ', 'i', 's', ' ', 'a', 's', ' ', 'a', 'n', '\\n']\n['&lt;pad&gt;', 'a'] f\n['&lt;pad&gt;', 'a', 'f'] f\n['a', 'f', 'f'] l\n['f', 'f', 'l'] i\n['f', 'l', 'i'] c\n['l', 'i', 'c'] t\n['i', 'c', 't'] s\n['c', 't', 's']  \n['t', 's', ' '] u\n['s', ' ', 'u'] s\n[' ', 'u', 's'] ,\n['u', 's', ',']  \n['s', ',', ' '] t\n[',', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] o\n['e', ' ', 'o'] b\n[' ', 'o', 'b'] j\n['o', 'b', 'j'] e\n['b', 'j', 'e'] c\n['j', 'e', 'c'] t\n['e', 'c', 't']  \n['c', 't', ' '] o\n['t', ' ', 'o'] f\n[' ', 'o', 'f']  \n['o', 'f', ' '] o\n['f', ' ', 'o'] u\n[' ', 'o', 'u'] r\n['o', 'u', 'r']  \n['u', 'r', ' '] m\n['r', ' ', 'm'] i\n[' ', 'm', 'i'] s\n['m', 'i', 's'] e\n['i', 's', 'e'] r\n['s', 'e', 'r'] y\n['e', 'r', 'y'] ,\n['r', 'y', ',']  \n['y', ',', ' '] i\n[',', ' ', 'i'] s\n[' ', 'i', 's']  \n['i', 's', ' '] a\n['s', ' ', 'a'] s\n[' ', 'a', 's']  \n['a', 's', ' '] a\n['s', ' ', 'a'] n\n[' ', 'a', 'n'] \n\nrow:  ['i', 'n', 'v', 'e', 'n', 't', 'o', 'r', 'y', ' ', 't', 'o', ' ', 'p', 'a', 'r', 't', 'i', 'c', 'u', 'l', 'a', 'r', 'i', 's', 'e', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'a', 'b', 'u', 'n', 'd', 'a', 'n', 'c', 'e', ';', ' ', 'o', 'u', 'r', '\\n']\n['&lt;pad&gt;', 'i'] n\n['&lt;pad&gt;', 'i', 'n'] v\n['i', 'n', 'v'] e\n['n', 'v', 'e'] n\n['v', 'e', 'n'] t\n['e', 'n', 't'] o\n['n', 't', 'o'] r\n['t', 'o', 'r'] y\n['o', 'r', 'y']  \n['r', 'y', ' '] t\n['y', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] p\n['o', ' ', 'p'] a\n[' ', 'p', 'a'] r\n['p', 'a', 'r'] t\n['a', 'r', 't'] i\n['r', 't', 'i'] c\n['t', 'i', 'c'] u\n['i', 'c', 'u'] l\n['c', 'u', 'l'] a\n['u', 'l', 'a'] r\n['l', 'a', 'r'] i\n['a', 'r', 'i'] s\n['r', 'i', 's'] e\n['i', 's', 'e']  \n['s', 'e', ' '] t\n['e', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] i\n['h', 'e', 'i'] r\n['e', 'i', 'r']  \n['i', 'r', ' '] a\n['r', ' ', 'a'] b\n[' ', 'a', 'b'] u\n['a', 'b', 'u'] n\n['b', 'u', 'n'] d\n['u', 'n', 'd'] a\n['n', 'd', 'a'] n\n['d', 'a', 'n'] c\n['a', 'n', 'c'] e\n['n', 'c', 'e'] ;\n['c', 'e', ';']  \n['e', ';', ' '] o\n[';', ' ', 'o'] u\n[' ', 'o', 'u'] r\n['o', 'u', 'r'] \n\nrow:  ['s', 'u', 'f', 'f', 'e', 'r', 'a', 'n', 'c', 'e', ' ', 'i', 's', ' ', 'a', ' ', 'g', 'a', 'i', 'n', ' ', 't', 'o', ' ', 't', 'h', 'e', 'm', ' ', 'L', 'e', 't', ' ', 'u', 's', ' ', 'r', 'e', 'v', 'e', 'n', 'g', 'e', ' ', 't', 'h', 'i', 's', ' ', 'w', 'i', 't', 'h', '\\n']\n['&lt;pad&gt;', 's'] u\n['&lt;pad&gt;', 's', 'u'] f\n['s', 'u', 'f'] f\n['u', 'f', 'f'] e\n['f', 'f', 'e'] r\n['f', 'e', 'r'] a\n['e', 'r', 'a'] n\n['r', 'a', 'n'] c\n['a', 'n', 'c'] e\n['n', 'c', 'e']  \n['c', 'e', ' '] i\n['e', ' ', 'i'] s\n[' ', 'i', 's']  \n['i', 's', ' '] a\n['s', ' ', 'a']  \n[' ', 'a', ' '] g\n['a', ' ', 'g'] a\n[' ', 'g', 'a'] i\n['g', 'a', 'i'] n\n['a', 'i', 'n']  \n['i', 'n', ' '] t\n['n', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] m\n['h', 'e', 'm']  \n['e', 'm', ' '] L\n['m', ' ', 'L'] e\n[' ', 'L', 'e'] t\n['L', 'e', 't']  \n['e', 't', ' '] u\n['t', ' ', 'u'] s\n[' ', 'u', 's']  \n['u', 's', ' '] r\n['s', ' ', 'r'] e\n[' ', 'r', 'e'] v\n['r', 'e', 'v'] e\n['e', 'v', 'e'] n\n['v', 'e', 'n'] g\n['e', 'n', 'g'] e\n['n', 'g', 'e']  \n['g', 'e', ' '] t\n['e', ' ', 't'] h\n[' ', 't', 'h'] i\n['t', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] w\n['s', ' ', 'w'] i\n[' ', 'w', 'i'] t\n['w', 'i', 't'] h\n['i', 't', 'h'] \n\nrow:  ['o', 'u', 'r', ' ', 'p', 'i', 'k', 'e', 's', ',', ' ', 'e', 'r', 'e', ' ', 'w', 'e', ' ', 'b', 'e', 'c', 'o', 'm', 'e', ' ', 'r', 'a', 'k', 'e', 's', ':', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'g', 'o', 'd', 's', ' ', 'k', 'n', 'o', 'w', ' ', 'I', '\\n']\n['&lt;pad&gt;', 'o'] u\n['&lt;pad&gt;', 'o', 'u'] r\n['o', 'u', 'r']  \n['u', 'r', ' '] p\n['r', ' ', 'p'] i\n[' ', 'p', 'i'] k\n['p', 'i', 'k'] e\n['i', 'k', 'e'] s\n['k', 'e', 's'] ,\n['e', 's', ',']  \n['s', ',', ' '] e\n[',', ' ', 'e'] r\n[' ', 'e', 'r'] e\n['e', 'r', 'e']  \n['r', 'e', ' '] w\n['e', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] b\n['e', ' ', 'b'] e\n[' ', 'b', 'e'] c\n['b', 'e', 'c'] o\n['e', 'c', 'o'] m\n['c', 'o', 'm'] e\n['o', 'm', 'e']  \n['m', 'e', ' '] r\n['e', ' ', 'r'] a\n[' ', 'r', 'a'] k\n['r', 'a', 'k'] e\n['a', 'k', 'e'] s\n['k', 'e', 's'] :\n['e', 's', ':']  \n['s', ':', ' '] f\n[':', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r']  \n['o', 'r', ' '] t\n['r', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] g\n['e', ' ', 'g'] o\n[' ', 'g', 'o'] d\n['g', 'o', 'd'] s\n['o', 'd', 's']  \n['d', 's', ' '] k\n['s', ' ', 'k'] n\n[' ', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w']  \n['o', 'w', ' '] I\n['w', ' ', 'I'] \n\nrow:  ['s', 'p', 'e', 'a', 'k', ' ', 't', 'h', 'i', 's', ' ', 'i', 'n', ' ', 'h', 'u', 'n', 'g', 'e', 'r', ' ', 'f', 'o', 'r', ' ', 'b', 'r', 'e', 'a', 'd', ',', ' ', 'n', 'o', 't', ' ', 'i', 'n', ' ', 't', 'h', 'i', 'r', 's', 't', ' ', 'f', 'o', 'r', ' ', 'r', 'e', 'v', 'e', 'n', 'g', 'e', '.', '\\n']\n['&lt;pad&gt;', 's'] p\n['&lt;pad&gt;', 's', 'p'] e\n['s', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k']  \n['a', 'k', ' '] t\n['k', ' ', 't'] h\n[' ', 't', 'h'] i\n['t', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] i\n['s', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] h\n['n', ' ', 'h'] u\n[' ', 'h', 'u'] n\n['h', 'u', 'n'] g\n['u', 'n', 'g'] e\n['n', 'g', 'e'] r\n['g', 'e', 'r']  \n['e', 'r', ' '] f\n['r', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r']  \n['o', 'r', ' '] b\n['r', ' ', 'b'] r\n[' ', 'b', 'r'] e\n['b', 'r', 'e'] a\n['r', 'e', 'a'] d\n['e', 'a', 'd'] ,\n['a', 'd', ',']  \n['d', ',', ' '] n\n[',', ' ', 'n'] o\n[' ', 'n', 'o'] t\n['n', 'o', 't']  \n['o', 't', ' '] i\n['t', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] t\n['n', ' ', 't'] h\n[' ', 't', 'h'] i\n['t', 'h', 'i'] r\n['h', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] f\n['t', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r']  \n['o', 'r', ' '] r\n['r', ' ', 'r'] e\n[' ', 'r', 'e'] v\n['r', 'e', 'v'] e\n['e', 'v', 'e'] n\n['v', 'e', 'n'] g\n['e', 'n', 'g'] e\n['n', 'g', 'e'] .\n['g', 'e', '.'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['W', 'o', 'u', 'l', 'd', ' ', 'y', 'o', 'u', ' ', 'p', 'r', 'o', 'c', 'e', 'e', 'd', ' ', 'e', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'l', 'y', ' ', 'a', 'g', 'a', 'i', 'n', 's', 't', ' ', 'C', 'a', 'i', 'u', 's', ' ', 'M', 'a', 'r', 'c', 'i', 'u', 's', '?', '\\n']\n['&lt;pad&gt;', 'W'] o\n['&lt;pad&gt;', 'W', 'o'] u\n['W', 'o', 'u'] l\n['o', 'u', 'l'] d\n['u', 'l', 'd']  \n['l', 'd', ' '] y\n['d', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u']  \n['o', 'u', ' '] p\n['u', ' ', 'p'] r\n[' ', 'p', 'r'] o\n['p', 'r', 'o'] c\n['r', 'o', 'c'] e\n['o', 'c', 'e'] e\n['c', 'e', 'e'] d\n['e', 'e', 'd']  \n['e', 'd', ' '] e\n['d', ' ', 'e'] s\n[' ', 'e', 's'] p\n['e', 's', 'p'] e\n['s', 'p', 'e'] c\n['p', 'e', 'c'] i\n['e', 'c', 'i'] a\n['c', 'i', 'a'] l\n['i', 'a', 'l'] l\n['a', 'l', 'l'] y\n['l', 'l', 'y']  \n['l', 'y', ' '] a\n['y', ' ', 'a'] g\n[' ', 'a', 'g'] a\n['a', 'g', 'a'] i\n['g', 'a', 'i'] n\n['a', 'i', 'n'] s\n['i', 'n', 's'] t\n['n', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] a\n[' ', 'C', 'a'] i\n['C', 'a', 'i'] u\n['a', 'i', 'u'] s\n['i', 'u', 's']  \n['u', 's', ' '] M\n['s', ' ', 'M'] a\n[' ', 'M', 'a'] r\n['M', 'a', 'r'] c\n['a', 'r', 'c'] i\n['r', 'c', 'i'] u\n['c', 'i', 'u'] s\n['i', 'u', 's'] ?\n['u', 's', '?'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['A', 'g', 'a', 'i', 'n', 's', 't', ' ', 'h', 'i', 'm', ' ', 'f', 'i', 'r', 's', 't', ':', ' ', 'h', 'e', \"'\", 's', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 'd', 'o', 'g', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'c', 'o', 'm', 'm', 'o', 'n', 'a', 'l', 't', 'y', '.', '\\n']\n['&lt;pad&gt;', 'A'] g\n['&lt;pad&gt;', 'A', 'g'] a\n['A', 'g', 'a'] i\n['g', 'a', 'i'] n\n['a', 'i', 'n'] s\n['i', 'n', 's'] t\n['n', 's', 't']  \n['s', 't', ' '] h\n['t', ' ', 'h'] i\n[' ', 'h', 'i'] m\n['h', 'i', 'm']  \n['i', 'm', ' '] f\n['m', ' ', 'f'] i\n[' ', 'f', 'i'] r\n['f', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't'] :\n['s', 't', ':']  \n['t', ':', ' '] h\n[':', ' ', 'h'] e\n[' ', 'h', 'e'] '\n['h', 'e', \"'\"] s\n['e', \"'\", 's']  \n[\"'\", 's', ' '] a\n['s', ' ', 'a']  \n[' ', 'a', ' '] v\n['a', ' ', 'v'] e\n[' ', 'v', 'e'] r\n['v', 'e', 'r'] y\n['e', 'r', 'y']  \n['r', 'y', ' '] d\n['y', ' ', 'd'] o\n[' ', 'd', 'o'] g\n['d', 'o', 'g']  \n['o', 'g', ' '] t\n['g', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] c\n['e', ' ', 'c'] o\n[' ', 'c', 'o'] m\n['c', 'o', 'm'] m\n['o', 'm', 'm'] o\n['m', 'm', 'o'] n\n['m', 'o', 'n'] a\n['o', 'n', 'a'] l\n['n', 'a', 'l'] t\n['a', 'l', 't'] y\n['l', 't', 'y'] .\n['t', 'y', '.'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['C', 'o', 'n', 's', 'i', 'd', 'e', 'r', ' ', 'y', 'o', 'u', ' ', 'w', 'h', 'a', 't', ' ', 's', 'e', 'r', 'v', 'i', 'c', 'e', 's', ' ', 'h', 'e', ' ', 'h', 'a', 's', ' ', 'd', 'o', 'n', 'e', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 's', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', '?', '\\n']\n['&lt;pad&gt;', 'C'] o\n['&lt;pad&gt;', 'C', 'o'] n\n['C', 'o', 'n'] s\n['o', 'n', 's'] i\n['n', 's', 'i'] d\n['s', 'i', 'd'] e\n['i', 'd', 'e'] r\n['d', 'e', 'r']  \n['e', 'r', ' '] y\n['r', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u']  \n['o', 'u', ' '] w\n['u', ' ', 'w'] h\n[' ', 'w', 'h'] a\n['w', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] s\n['t', ' ', 's'] e\n[' ', 's', 'e'] r\n['s', 'e', 'r'] v\n['e', 'r', 'v'] i\n['r', 'v', 'i'] c\n['v', 'i', 'c'] e\n['i', 'c', 'e'] s\n['c', 'e', 's']  \n['e', 's', ' '] h\n['s', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] h\n['e', ' ', 'h'] a\n[' ', 'h', 'a'] s\n['h', 'a', 's']  \n['a', 's', ' '] d\n['s', ' ', 'd'] o\n[' ', 'd', 'o'] n\n['d', 'o', 'n'] e\n['o', 'n', 'e']  \n['n', 'e', ' '] f\n['e', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r']  \n['o', 'r', ' '] h\n['r', ' ', 'h'] i\n[' ', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] c\n['s', ' ', 'c'] o\n[' ', 'c', 'o'] u\n['c', 'o', 'u'] n\n['o', 'u', 'n'] t\n['u', 'n', 't'] r\n['n', 't', 'r'] y\n['t', 'r', 'y'] ?\n['r', 'y', '?'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['V', 'e', 'r', 'y', ' ', 'w', 'e', 'l', 'l', ';', ' ', 'a', 'n', 'd', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'c', 'o', 'n', 't', 'e', 'n', 't', ' ', 't', 'o', ' ', 'g', 'i', 'v', 'e', ' ', 'h', 'i', 'm', ' ', 'g', 'o', 'o', 'd', '\\n']\n['&lt;pad&gt;', 'V'] e\n['&lt;pad&gt;', 'V', 'e'] r\n['V', 'e', 'r'] y\n['e', 'r', 'y']  \n['r', 'y', ' '] w\n['y', ' ', 'w'] e\n[' ', 'w', 'e'] l\n['w', 'e', 'l'] l\n['e', 'l', 'l'] ;\n['l', 'l', ';']  \n['l', ';', ' '] a\n[';', ' ', 'a'] n\n[' ', 'a', 'n'] d\n['a', 'n', 'd']  \n['n', 'd', ' '] c\n['d', ' ', 'c'] o\n[' ', 'c', 'o'] u\n['c', 'o', 'u'] l\n['o', 'u', 'l'] d\n['u', 'l', 'd']  \n['l', 'd', ' '] b\n['d', ' ', 'b'] e\n[' ', 'b', 'e']  \n['b', 'e', ' '] c\n['e', ' ', 'c'] o\n[' ', 'c', 'o'] n\n['c', 'o', 'n'] t\n['o', 'n', 't'] e\n['n', 't', 'e'] n\n['t', 'e', 'n'] t\n['e', 'n', 't']  \n['n', 't', ' '] t\n['t', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] g\n['o', ' ', 'g'] i\n[' ', 'g', 'i'] v\n['g', 'i', 'v'] e\n['i', 'v', 'e']  \n['v', 'e', ' '] h\n['e', ' ', 'h'] i\n[' ', 'h', 'i'] m\n['h', 'i', 'm']  \n['i', 'm', ' '] g\n['m', ' ', 'g'] o\n[' ', 'g', 'o'] o\n['g', 'o', 'o'] d\n['o', 'o', 'd'] \n\nrow:  ['r', 'e', 'p', 'o', 'r', 't', ' ', 'f', 'o', 'r', 't', ',', ' ', 'b', 'u', 't', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'p', 'a', 'y', 's', ' ', 'h', 'i', 'm', 's', 'e', 'l', 'f', ' ', 'w', 'i', 't', 'h', ' ', 'b', 'e', 'i', 'n', 'g', ' ', 'p', 'r', 'o', 'u', 'd', '.', '\\n']\n['&lt;pad&gt;', 'r'] e\n['&lt;pad&gt;', 'r', 'e'] p\n['r', 'e', 'p'] o\n['e', 'p', 'o'] r\n['p', 'o', 'r'] t\n['o', 'r', 't']  \n['r', 't', ' '] f\n['t', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r'] t\n['o', 'r', 't'] ,\n['r', 't', ',']  \n['t', ',', ' '] b\n[',', ' ', 'b'] u\n[' ', 'b', 'u'] t\n['b', 'u', 't']  \n['u', 't', ' '] t\n['t', ' ', 't'] h\n[' ', 't', 'h'] a\n['t', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] h\n['t', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] p\n['e', ' ', 'p'] a\n[' ', 'p', 'a'] y\n['p', 'a', 'y'] s\n['a', 'y', 's']  \n['y', 's', ' '] h\n['s', ' ', 'h'] i\n[' ', 'h', 'i'] m\n['h', 'i', 'm'] s\n['i', 'm', 's'] e\n['m', 's', 'e'] l\n['s', 'e', 'l'] f\n['e', 'l', 'f']  \n['l', 'f', ' '] w\n['f', ' ', 'w'] i\n[' ', 'w', 'i'] t\n['w', 'i', 't'] h\n['i', 't', 'h']  \n['t', 'h', ' '] b\n['h', ' ', 'b'] e\n[' ', 'b', 'e'] i\n['b', 'e', 'i'] n\n['e', 'i', 'n'] g\n['i', 'n', 'g']  \n['n', 'g', ' '] p\n['g', ' ', 'p'] r\n[' ', 'p', 'r'] o\n['p', 'r', 'o'] u\n['r', 'o', 'u'] d\n['o', 'u', 'd'] .\n['u', 'd', '.'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['N', 'a', 'y', ',', ' ', 'b', 'u', 't', ' ', 's', 'p', 'e', 'a', 'k', ' ', 'n', 'o', 't', ' ', 'm', 'a', 'l', 'i', 'c', 'i', 'o', 'u', 's', 'l', 'y', '.', '\\n']\n['&lt;pad&gt;', 'N'] a\n['&lt;pad&gt;', 'N', 'a'] y\n['N', 'a', 'y'] ,\n['a', 'y', ',']  \n['y', ',', ' '] b\n[',', ' ', 'b'] u\n[' ', 'b', 'u'] t\n['b', 'u', 't']  \n['u', 't', ' '] s\n['t', ' ', 's'] p\n[' ', 's', 'p'] e\n['s', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k']  \n['a', 'k', ' '] n\n['k', ' ', 'n'] o\n[' ', 'n', 'o'] t\n['n', 'o', 't']  \n['o', 't', ' '] m\n['t', ' ', 'm'] a\n[' ', 'm', 'a'] l\n['m', 'a', 'l'] i\n['a', 'l', 'i'] c\n['l', 'i', 'c'] i\n['i', 'c', 'i'] o\n['c', 'i', 'o'] u\n['i', 'o', 'u'] s\n['o', 'u', 's'] l\n['u', 's', 'l'] y\n['s', 'l', 'y'] .\n['l', 'y', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['I', ' ', 's', 'a', 'y', ' ', 'u', 'n', 't', 'o', ' ', 'y', 'o', 'u', ',', ' ', 'w', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'h', 'a', 't', 'h', ' ', 'd', 'o', 'n', 'e', ' ', 'f', 'a', 'm', 'o', 'u', 's', 'l', 'y', ',', ' ', 'h', 'e', ' ', 'd', 'i', 'd', '\\n']\n['&lt;pad&gt;', 'I']  \n['&lt;pad&gt;', 'I', ' '] s\n['I', ' ', 's'] a\n[' ', 's', 'a'] y\n['s', 'a', 'y']  \n['a', 'y', ' '] u\n['y', ' ', 'u'] n\n[' ', 'u', 'n'] t\n['u', 'n', 't'] o\n['n', 't', 'o']  \n['t', 'o', ' '] y\n['o', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u'] ,\n['o', 'u', ',']  \n['u', ',', ' '] w\n[',', ' ', 'w'] h\n[' ', 'w', 'h'] a\n['w', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] h\n['t', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] h\n['e', ' ', 'h'] a\n[' ', 'h', 'a'] t\n['h', 'a', 't'] h\n['a', 't', 'h']  \n['t', 'h', ' '] d\n['h', ' ', 'd'] o\n[' ', 'd', 'o'] n\n['d', 'o', 'n'] e\n['o', 'n', 'e']  \n['n', 'e', ' '] f\n['e', ' ', 'f'] a\n[' ', 'f', 'a'] m\n['f', 'a', 'm'] o\n['a', 'm', 'o'] u\n['m', 'o', 'u'] s\n['o', 'u', 's'] l\n['u', 's', 'l'] y\n['s', 'l', 'y'] ,\n['l', 'y', ',']  \n['y', ',', ' '] h\n[',', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] d\n['e', ' ', 'd'] i\n[' ', 'd', 'i'] d\n['d', 'i', 'd'] \n\nrow:  ['i', 't', ' ', 't', 'o', ' ', 't', 'h', 'a', 't', ' ', 'e', 'n', 'd', ':', ' ', 't', 'h', 'o', 'u', 'g', 'h', ' ', 's', 'o', 'f', 't', '-', 'c', 'o', 'n', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'd', ' ', 'm', 'e', 'n', ' ', 'c', 'a', 'n', ' ', 'b', 'e', '\\n']\n['&lt;pad&gt;', 'i'] t\n['&lt;pad&gt;', 'i', 't']  \n['i', 't', ' '] t\n['t', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] a\n['t', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] e\n['t', ' ', 'e'] n\n[' ', 'e', 'n'] d\n['e', 'n', 'd'] :\n['n', 'd', ':']  \n['d', ':', ' '] t\n[':', ' ', 't'] h\n[' ', 't', 'h'] o\n['t', 'h', 'o'] u\n['h', 'o', 'u'] g\n['o', 'u', 'g'] h\n['u', 'g', 'h']  \n['g', 'h', ' '] s\n['h', ' ', 's'] o\n[' ', 's', 'o'] f\n['s', 'o', 'f'] t\n['o', 'f', 't'] -\n['f', 't', '-'] c\n['t', '-', 'c'] o\n['-', 'c', 'o'] n\n['c', 'o', 'n'] s\n['o', 'n', 's'] c\n['n', 's', 'c'] i\n['s', 'c', 'i'] e\n['c', 'i', 'e'] n\n['i', 'e', 'n'] c\n['e', 'n', 'c'] e\n['n', 'c', 'e'] d\n['c', 'e', 'd']  \n['e', 'd', ' '] m\n['d', ' ', 'm'] e\n[' ', 'm', 'e'] n\n['m', 'e', 'n']  \n['e', 'n', ' '] c\n['n', ' ', 'c'] a\n[' ', 'c', 'a'] n\n['c', 'a', 'n']  \n['a', 'n', ' '] b\n['n', ' ', 'b'] e\n[' ', 'b', 'e'] \n\nrow:  ['c', 'o', 'n', 't', 'e', 'n', 't', ' ', 't', 'o', ' ', 's', 'a', 'y', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'f', 'o', 'r', ' ', 'h', 'i', 's', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', ' ', 'h', 'e', ' ', 'd', 'i', 'd', ' ', 'i', 't', ' ', 't', 'o', '\\n']\n['&lt;pad&gt;', 'c'] o\n['&lt;pad&gt;', 'c', 'o'] n\n['c', 'o', 'n'] t\n['o', 'n', 't'] e\n['n', 't', 'e'] n\n['t', 'e', 'n'] t\n['e', 'n', 't']  \n['n', 't', ' '] t\n['t', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] s\n['o', ' ', 's'] a\n[' ', 's', 'a'] y\n['s', 'a', 'y']  \n['a', 'y', ' '] i\n['y', ' ', 'i'] t\n[' ', 'i', 't']  \n['i', 't', ' '] w\n['t', ' ', 'w'] a\n[' ', 'w', 'a'] s\n['w', 'a', 's']  \n['a', 's', ' '] f\n['s', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r']  \n['o', 'r', ' '] h\n['r', ' ', 'h'] i\n[' ', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] c\n['s', ' ', 'c'] o\n[' ', 'c', 'o'] u\n['c', 'o', 'u'] n\n['o', 'u', 'n'] t\n['u', 'n', 't'] r\n['n', 't', 'r'] y\n['t', 'r', 'y']  \n['r', 'y', ' '] h\n['y', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] d\n['e', ' ', 'd'] i\n[' ', 'd', 'i'] d\n['d', 'i', 'd']  \n['i', 'd', ' '] i\n['d', ' ', 'i'] t\n[' ', 'i', 't']  \n['i', 't', ' '] t\n['t', ' ', 't'] o\n[' ', 't', 'o'] \n\nrow:  ['p', 'l', 'e', 'a', 's', 'e', ' ', 'h', 'i', 's', ' ', 'm', 'o', 't', 'h', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'p', 'a', 'r', 't', 'l', 'y', ' ', 'p', 'r', 'o', 'u', 'd', ';', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'h', 'e', '\\n']\n['&lt;pad&gt;', 'p'] l\n['&lt;pad&gt;', 'p', 'l'] e\n['p', 'l', 'e'] a\n['l', 'e', 'a'] s\n['e', 'a', 's'] e\n['a', 's', 'e']  \n['s', 'e', ' '] h\n['e', ' ', 'h'] i\n[' ', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] m\n['s', ' ', 'm'] o\n[' ', 'm', 'o'] t\n['m', 'o', 't'] h\n['o', 't', 'h'] e\n['t', 'h', 'e'] r\n['h', 'e', 'r']  \n['e', 'r', ' '] a\n['r', ' ', 'a'] n\n[' ', 'a', 'n'] d\n['a', 'n', 'd']  \n['n', 'd', ' '] t\n['d', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] b\n['o', ' ', 'b'] e\n[' ', 'b', 'e']  \n['b', 'e', ' '] p\n['e', ' ', 'p'] a\n[' ', 'p', 'a'] r\n['p', 'a', 'r'] t\n['a', 'r', 't'] l\n['r', 't', 'l'] y\n['t', 'l', 'y']  \n['l', 'y', ' '] p\n['y', ' ', 'p'] r\n[' ', 'p', 'r'] o\n['p', 'r', 'o'] u\n['r', 'o', 'u'] d\n['o', 'u', 'd'] ;\n['u', 'd', ';']  \n['d', ';', ' '] w\n[';', ' ', 'w'] h\n[' ', 'w', 'h'] i\n['w', 'h', 'i'] c\n['h', 'i', 'c'] h\n['i', 'c', 'h']  \n['c', 'h', ' '] h\n['h', ' ', 'h'] e\n[' ', 'h', 'e'] \n\nrow:  ['i', 's', ',', ' ', 'e', 'v', 'e', 'n', ' ', 't', 'i', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'a', 'l', 't', 'i', 't', 'u', 'd', 'e', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'v', 'i', 'r', 't', 'u', 'e', '.', '\\n']\n['&lt;pad&gt;', 'i'] s\n['&lt;pad&gt;', 'i', 's'] ,\n['i', 's', ',']  \n['s', ',', ' '] e\n[',', ' ', 'e'] v\n[' ', 'e', 'v'] e\n['e', 'v', 'e'] n\n['v', 'e', 'n']  \n['e', 'n', ' '] t\n['n', ' ', 't'] i\n[' ', 't', 'i'] l\n['t', 'i', 'l'] l\n['i', 'l', 'l']  \n['l', 'l', ' '] t\n['l', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] a\n['e', ' ', 'a'] l\n[' ', 'a', 'l'] t\n['a', 'l', 't'] i\n['l', 't', 'i'] t\n['t', 'i', 't'] u\n['i', 't', 'u'] d\n['t', 'u', 'd'] e\n['u', 'd', 'e']  \n['d', 'e', ' '] o\n['e', ' ', 'o'] f\n[' ', 'o', 'f']  \n['o', 'f', ' '] h\n['f', ' ', 'h'] i\n[' ', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] v\n['s', ' ', 'v'] i\n[' ', 'v', 'i'] r\n['v', 'i', 'r'] t\n['i', 'r', 't'] u\n['r', 't', 'u'] e\n['t', 'u', 'e'] .\n['u', 'e', '.'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['W', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'c', 'a', 'n', 'n', 'o', 't', ' ', 'h', 'e', 'l', 'p', ' ', 'i', 'n', ' ', 'h', 'i', 's', ' ', 'n', 'a', 't', 'u', 'r', 'e', ',', ' ', 'y', 'o', 'u', ' ', 'a', 'c', 'c', 'o', 'u', 'n', 't', ' ', 'a', '\\n']\n['&lt;pad&gt;', 'W'] h\n['&lt;pad&gt;', 'W', 'h'] a\n['W', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] h\n['t', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] c\n['e', ' ', 'c'] a\n[' ', 'c', 'a'] n\n['c', 'a', 'n'] n\n['a', 'n', 'n'] o\n['n', 'n', 'o'] t\n['n', 'o', 't']  \n['o', 't', ' '] h\n['t', ' ', 'h'] e\n[' ', 'h', 'e'] l\n['h', 'e', 'l'] p\n['e', 'l', 'p']  \n['l', 'p', ' '] i\n['p', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] h\n['n', ' ', 'h'] i\n[' ', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] n\n['s', ' ', 'n'] a\n[' ', 'n', 'a'] t\n['n', 'a', 't'] u\n['a', 't', 'u'] r\n['t', 'u', 'r'] e\n['u', 'r', 'e'] ,\n['r', 'e', ',']  \n['e', ',', ' '] y\n[',', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u']  \n['o', 'u', ' '] a\n['u', ' ', 'a'] c\n[' ', 'a', 'c'] c\n['a', 'c', 'c'] o\n['c', 'c', 'o'] u\n['c', 'o', 'u'] n\n['o', 'u', 'n'] t\n['u', 'n', 't']  \n['n', 't', ' '] a\n['t', ' ', 'a'] \n\nrow:  ['v', 'i', 'c', 'e', ' ', 'i', 'n', ' ', 'h', 'i', 'm', '.', ' ', 'Y', 'o', 'u', ' ', 'm', 'u', 's', 't', ' ', 'i', 'n', ' ', 'n', 'o', ' ', 'w', 'a', 'y', ' ', 's', 'a', 'y', ' ', 'h', 'e', ' ', 'i', 's', ' ', 'c', 'o', 'v', 'e', 't', 'o', 'u', 's', '.', '\\n']\n['&lt;pad&gt;', 'v'] i\n['&lt;pad&gt;', 'v', 'i'] c\n['v', 'i', 'c'] e\n['i', 'c', 'e']  \n['c', 'e', ' '] i\n['e', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] h\n['n', ' ', 'h'] i\n[' ', 'h', 'i'] m\n['h', 'i', 'm'] .\n['i', 'm', '.']  \n['m', '.', ' '] Y\n['.', ' ', 'Y'] o\n[' ', 'Y', 'o'] u\n['Y', 'o', 'u']  \n['o', 'u', ' '] m\n['u', ' ', 'm'] u\n[' ', 'm', 'u'] s\n['m', 'u', 's'] t\n['u', 's', 't']  \n['s', 't', ' '] i\n['t', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] n\n['n', ' ', 'n'] o\n[' ', 'n', 'o']  \n['n', 'o', ' '] w\n['o', ' ', 'w'] a\n[' ', 'w', 'a'] y\n['w', 'a', 'y']  \n['a', 'y', ' '] s\n['y', ' ', 's'] a\n[' ', 's', 'a'] y\n['s', 'a', 'y']  \n['a', 'y', ' '] h\n['y', ' ', 'h'] e\n[' ', 'h', 'e']  \n['h', 'e', ' '] i\n['e', ' ', 'i'] s\n[' ', 'i', 's']  \n['i', 's', ' '] c\n['s', ' ', 'c'] o\n[' ', 'c', 'o'] v\n['c', 'o', 'v'] e\n['o', 'v', 'e'] t\n['v', 'e', 't'] o\n['e', 't', 'o'] u\n['t', 'o', 'u'] s\n['o', 'u', 's'] .\n['u', 's', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['I', 'f', ' ', 'I', ' ', 'm', 'u', 's', 't', ' ', 'n', 'o', 't', ',', ' ', 'I', ' ', 'n', 'e', 'e', 'd', ' ', 'n', 'o', 't', ' ', 'b', 'e', ' ', 'b', 'a', 'r', 'r', 'e', 'n', ' ', 'o', 'f', ' ', 'a', 'c', 'c', 'u', 's', 'a', 't', 'i', 'o', 'n', 's', ';', '\\n']\n['&lt;pad&gt;', 'I'] f\n['&lt;pad&gt;', 'I', 'f']  \n['I', 'f', ' '] I\n['f', ' ', 'I']  \n[' ', 'I', ' '] m\n['I', ' ', 'm'] u\n[' ', 'm', 'u'] s\n['m', 'u', 's'] t\n['u', 's', 't']  \n['s', 't', ' '] n\n['t', ' ', 'n'] o\n[' ', 'n', 'o'] t\n['n', 'o', 't'] ,\n['o', 't', ',']  \n['t', ',', ' '] I\n[',', ' ', 'I']  \n[' ', 'I', ' '] n\n['I', ' ', 'n'] e\n[' ', 'n', 'e'] e\n['n', 'e', 'e'] d\n['e', 'e', 'd']  \n['e', 'd', ' '] n\n['d', ' ', 'n'] o\n[' ', 'n', 'o'] t\n['n', 'o', 't']  \n['o', 't', ' '] b\n['t', ' ', 'b'] e\n[' ', 'b', 'e']  \n['b', 'e', ' '] b\n['e', ' ', 'b'] a\n[' ', 'b', 'a'] r\n['b', 'a', 'r'] r\n['a', 'r', 'r'] e\n['r', 'r', 'e'] n\n['r', 'e', 'n']  \n['e', 'n', ' '] o\n['n', ' ', 'o'] f\n[' ', 'o', 'f']  \n['o', 'f', ' '] a\n['f', ' ', 'a'] c\n[' ', 'a', 'c'] c\n['a', 'c', 'c'] u\n['c', 'c', 'u'] s\n['c', 'u', 's'] a\n['u', 's', 'a'] t\n['s', 'a', 't'] i\n['a', 't', 'i'] o\n['t', 'i', 'o'] n\n['i', 'o', 'n'] s\n['o', 'n', 's'] ;\n['n', 's', ';'] \n\nrow:  ['h', 'e', ' ', 'h', 'a', 't', 'h', ' ', 'f', 'a', 'u', 'l', 't', 's', ',', ' ', 'w', 'i', 't', 'h', ' ', 's', 'u', 'r', 'p', 'l', 'u', 's', ',', ' ', 't', 'o', ' ', 't', 'i', 'r', 'e', ' ', 'i', 'n', ' ', 'r', 'e', 'p', 'e', 't', 'i', 't', 'i', 'o', 'n', '.', '\\n']\n['&lt;pad&gt;', 'h'] e\n['&lt;pad&gt;', 'h', 'e']  \n['h', 'e', ' '] h\n['e', ' ', 'h'] a\n[' ', 'h', 'a'] t\n['h', 'a', 't'] h\n['a', 't', 'h']  \n['t', 'h', ' '] f\n['h', ' ', 'f'] a\n[' ', 'f', 'a'] u\n['f', 'a', 'u'] l\n['a', 'u', 'l'] t\n['u', 'l', 't'] s\n['l', 't', 's'] ,\n['t', 's', ',']  \n['s', ',', ' '] w\n[',', ' ', 'w'] i\n[' ', 'w', 'i'] t\n['w', 'i', 't'] h\n['i', 't', 'h']  \n['t', 'h', ' '] s\n['h', ' ', 's'] u\n[' ', 's', 'u'] r\n['s', 'u', 'r'] p\n['u', 'r', 'p'] l\n['r', 'p', 'l'] u\n['p', 'l', 'u'] s\n['l', 'u', 's'] ,\n['u', 's', ',']  \n['s', ',', ' '] t\n[',', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] i\n[' ', 't', 'i'] r\n['t', 'i', 'r'] e\n['i', 'r', 'e']  \n['r', 'e', ' '] i\n['e', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] r\n['n', ' ', 'r'] e\n[' ', 'r', 'e'] p\n['r', 'e', 'p'] e\n['e', 'p', 'e'] t\n['p', 'e', 't'] i\n['e', 't', 'i'] t\n['t', 'i', 't'] i\n['i', 't', 'i'] o\n['t', 'i', 'o'] n\n['i', 'o', 'n'] .\n['o', 'n', '.'] \n\nrow:  ['W', 'h', 'a', 't', ' ', 's', 'h', 'o', 'u', 't', 's', ' ', 'a', 'r', 'e', ' ', 't', 'h', 'e', 's', 'e', '?', ' ', 'T', 'h', 'e', ' ', 'o', 't', 'h', 'e', 'r', ' ', 's', 'i', 'd', 'e', ' ', 'o', \"'\", ' ', 't', 'h', 'e', ' ', 'c', 'i', 't', 'y', '\\n']\n['&lt;pad&gt;', 'W'] h\n['&lt;pad&gt;', 'W', 'h'] a\n['W', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] s\n['t', ' ', 's'] h\n[' ', 's', 'h'] o\n['s', 'h', 'o'] u\n['h', 'o', 'u'] t\n['o', 'u', 't'] s\n['u', 't', 's']  \n['t', 's', ' '] a\n['s', ' ', 'a'] r\n[' ', 'a', 'r'] e\n['a', 'r', 'e']  \n['r', 'e', ' '] t\n['e', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] s\n['h', 'e', 's'] e\n['e', 's', 'e'] ?\n['s', 'e', '?']  \n['e', '?', ' '] T\n['?', ' ', 'T'] h\n[' ', 'T', 'h'] e\n['T', 'h', 'e']  \n['h', 'e', ' '] o\n['e', ' ', 'o'] t\n[' ', 'o', 't'] h\n['o', 't', 'h'] e\n['t', 'h', 'e'] r\n['h', 'e', 'r']  \n['e', 'r', ' '] s\n['r', ' ', 's'] i\n[' ', 's', 'i'] d\n['s', 'i', 'd'] e\n['i', 'd', 'e']  \n['d', 'e', ' '] o\n['e', ' ', 'o'] '\n[' ', 'o', \"'\"]  \n['o', \"'\", ' '] t\n[\"'\", ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] c\n['e', ' ', 'c'] i\n[' ', 'c', 'i'] t\n['c', 'i', 't'] y\n['i', 't', 'y'] \n\nrow:  ['i', 's', ' ', 'r', 'i', 's', 'e', 'n', ':', ' ', 'w', 'h', 'y', ' ', 's', 't', 'a', 'y', ' ', 'w', 'e', ' ', 'p', 'r', 'a', 't', 'i', 'n', 'g', ' ', 'h', 'e', 'r', 'e', '?', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'C', 'a', 'p', 'i', 't', 'o', 'l', '!', '\\n']\n['&lt;pad&gt;', 'i'] s\n['&lt;pad&gt;', 'i', 's']  \n['i', 's', ' '] r\n['s', ' ', 'r'] i\n[' ', 'r', 'i'] s\n['r', 'i', 's'] e\n['i', 's', 'e'] n\n['s', 'e', 'n'] :\n['e', 'n', ':']  \n['n', ':', ' '] w\n[':', ' ', 'w'] h\n[' ', 'w', 'h'] y\n['w', 'h', 'y']  \n['h', 'y', ' '] s\n['y', ' ', 's'] t\n[' ', 's', 't'] a\n['s', 't', 'a'] y\n['t', 'a', 'y']  \n['a', 'y', ' '] w\n['y', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] p\n['e', ' ', 'p'] r\n[' ', 'p', 'r'] a\n['p', 'r', 'a'] t\n['r', 'a', 't'] i\n['a', 't', 'i'] n\n['t', 'i', 'n'] g\n['i', 'n', 'g']  \n['n', 'g', ' '] h\n['g', ' ', 'h'] e\n[' ', 'h', 'e'] r\n['h', 'e', 'r'] e\n['e', 'r', 'e'] ?\n['r', 'e', '?']  \n['e', '?', ' '] t\n['?', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] C\n['e', ' ', 'C'] a\n[' ', 'C', 'a'] p\n['C', 'a', 'p'] i\n['a', 'p', 'i'] t\n['p', 'i', 't'] o\n['i', 't', 'o'] l\n['t', 'o', 'l'] !\n['o', 'l', '!'] \n\nrow:  ['A', 'l', 'l', ':', '\\n']\n['&lt;pad&gt;', 'A'] l\n['&lt;pad&gt;', 'A', 'l'] l\n['A', 'l', 'l'] :\n['l', 'l', ':'] \n\nrow:  ['C', 'o', 'm', 'e', ',', ' ', 'c', 'o', 'm', 'e', '.', '\\n']\n['&lt;pad&gt;', 'C'] o\n['&lt;pad&gt;', 'C', 'o'] m\n['C', 'o', 'm'] e\n['o', 'm', 'e'] ,\n['m', 'e', ',']  \n['e', ',', ' '] c\n[',', ' ', 'c'] o\n[' ', 'c', 'o'] m\n['c', 'o', 'm'] e\n['o', 'm', 'e'] .\n['m', 'e', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['S', 'o', 'f', 't', '!', ' ', 'w', 'h', 'o', ' ', 'c', 'o', 'm', 'e', 's', ' ', 'h', 'e', 'r', 'e', '?', '\\n']\n['&lt;pad&gt;', 'S'] o\n['&lt;pad&gt;', 'S', 'o'] f\n['S', 'o', 'f'] t\n['o', 'f', 't'] !\n['f', 't', '!']  \n['t', '!', ' '] w\n['!', ' ', 'w'] h\n[' ', 'w', 'h'] o\n['w', 'h', 'o']  \n['h', 'o', ' '] c\n['o', ' ', 'c'] o\n[' ', 'c', 'o'] m\n['c', 'o', 'm'] e\n['o', 'm', 'e'] s\n['m', 'e', 's']  \n['e', 's', ' '] h\n['s', ' ', 'h'] e\n[' ', 'h', 'e'] r\n['h', 'e', 'r'] e\n['e', 'r', 'e'] ?\n['r', 'e', '?'] \n\nrow:  ['S', 'e', 'c', 'o', 'n', 'd', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'S'] e\n['&lt;pad&gt;', 'S', 'e'] c\n['S', 'e', 'c'] o\n['e', 'c', 'o'] n\n['c', 'o', 'n'] d\n['o', 'n', 'd']  \n['n', 'd', ' '] C\n['d', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['W', 'o', 'r', 't', 'h', 'y', ' ', 'M', 'e', 'n', 'e', 'n', 'i', 'u', 's', ' ', 'A', 'g', 'r', 'i', 'p', 'p', 'a', ';', ' ', 'o', 'n', 'e', ' ', 't', 'h', 'a', 't', ' ', 'h', 'a', 't', 'h', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'l', 'o', 'v', 'e', 'd', '\\n']\n['&lt;pad&gt;', 'W'] o\n['&lt;pad&gt;', 'W', 'o'] r\n['W', 'o', 'r'] t\n['o', 'r', 't'] h\n['r', 't', 'h'] y\n['t', 'h', 'y']  \n['h', 'y', ' '] M\n['y', ' ', 'M'] e\n[' ', 'M', 'e'] n\n['M', 'e', 'n'] e\n['e', 'n', 'e'] n\n['n', 'e', 'n'] i\n['e', 'n', 'i'] u\n['n', 'i', 'u'] s\n['i', 'u', 's']  \n['u', 's', ' '] A\n['s', ' ', 'A'] g\n[' ', 'A', 'g'] r\n['A', 'g', 'r'] i\n['g', 'r', 'i'] p\n['r', 'i', 'p'] p\n['i', 'p', 'p'] a\n['p', 'p', 'a'] ;\n['p', 'a', ';']  \n['a', ';', ' '] o\n[';', ' ', 'o'] n\n[' ', 'o', 'n'] e\n['o', 'n', 'e']  \n['n', 'e', ' '] t\n['e', ' ', 't'] h\n[' ', 't', 'h'] a\n['t', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] h\n['t', ' ', 'h'] a\n[' ', 'h', 'a'] t\n['h', 'a', 't'] h\n['a', 't', 'h']  \n['t', 'h', ' '] a\n['h', ' ', 'a'] l\n[' ', 'a', 'l'] w\n['a', 'l', 'w'] a\n['l', 'w', 'a'] y\n['w', 'a', 'y'] s\n['a', 'y', 's']  \n['y', 's', ' '] l\n['s', ' ', 'l'] o\n[' ', 'l', 'o'] v\n['l', 'o', 'v'] e\n['o', 'v', 'e'] d\n['v', 'e', 'd'] \n\nrow:  ['t', 'h', 'e', ' ', 'p', 'e', 'o', 'p', 'l', 'e', '.', '\\n']\n['&lt;pad&gt;', 't'] h\n['&lt;pad&gt;', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] p\n['e', ' ', 'p'] e\n[' ', 'p', 'e'] o\n['p', 'e', 'o'] p\n['e', 'o', 'p'] l\n['o', 'p', 'l'] e\n['p', 'l', 'e'] .\n['l', 'e', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['H', 'e', \"'\", 's', ' ', 'o', 'n', 'e', ' ', 'h', 'o', 'n', 'e', 's', 't', ' ', 'e', 'n', 'o', 'u', 'g', 'h', ':', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'a', 'l', 'l', ' ', 't', 'h', 'e', ' ', 'r', 'e', 's', 't', ' ', 'w', 'e', 'r', 'e', ' ', 's', 'o', '!', '\\n']\n['&lt;pad&gt;', 'H'] e\n['&lt;pad&gt;', 'H', 'e'] '\n['H', 'e', \"'\"] s\n['e', \"'\", 's']  \n[\"'\", 's', ' '] o\n['s', ' ', 'o'] n\n[' ', 'o', 'n'] e\n['o', 'n', 'e']  \n['n', 'e', ' '] h\n['e', ' ', 'h'] o\n[' ', 'h', 'o'] n\n['h', 'o', 'n'] e\n['o', 'n', 'e'] s\n['n', 'e', 's'] t\n['e', 's', 't']  \n['s', 't', ' '] e\n['t', ' ', 'e'] n\n[' ', 'e', 'n'] o\n['e', 'n', 'o'] u\n['n', 'o', 'u'] g\n['o', 'u', 'g'] h\n['u', 'g', 'h'] :\n['g', 'h', ':']  \n['h', ':', ' '] w\n[':', ' ', 'w'] o\n[' ', 'w', 'o'] u\n['w', 'o', 'u'] l\n['o', 'u', 'l'] d\n['u', 'l', 'd']  \n['l', 'd', ' '] a\n['d', ' ', 'a'] l\n[' ', 'a', 'l'] l\n['a', 'l', 'l']  \n['l', 'l', ' '] t\n['l', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] r\n['e', ' ', 'r'] e\n[' ', 'r', 'e'] s\n['r', 'e', 's'] t\n['e', 's', 't']  \n['s', 't', ' '] w\n['t', ' ', 'w'] e\n[' ', 'w', 'e'] r\n['w', 'e', 'r'] e\n['e', 'r', 'e']  \n['r', 'e', ' '] s\n['e', ' ', 's'] o\n[' ', 's', 'o'] !\n['s', 'o', '!'] \n\nrow:  ['M', 'E', 'N', 'E', 'N', 'I', 'U', 'S', ':', '\\n']\n['&lt;pad&gt;', 'M'] E\n['&lt;pad&gt;', 'M', 'E'] N\n['M', 'E', 'N'] E\n['E', 'N', 'E'] N\n['N', 'E', 'N'] I\n['E', 'N', 'I'] U\n['N', 'I', 'U'] S\n['I', 'U', 'S'] :\n['U', 'S', ':'] \n\nrow:  ['W', 'h', 'a', 't', ' ', 'w', 'o', 'r', 'k', \"'\", 's', ',', ' ', 'm', 'y', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', 'm', 'e', 'n', ',', ' ', 'i', 'n', ' ', 'h', 'a', 'n', 'd', '?', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'g', 'o', ' ', 'y', 'o', 'u', '\\n']\n['&lt;pad&gt;', 'W'] h\n['&lt;pad&gt;', 'W', 'h'] a\n['W', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] w\n['t', ' ', 'w'] o\n[' ', 'w', 'o'] r\n['w', 'o', 'r'] k\n['o', 'r', 'k'] '\n['r', 'k', \"'\"] s\n['k', \"'\", 's'] ,\n[\"'\", 's', ',']  \n['s', ',', ' '] m\n[',', ' ', 'm'] y\n[' ', 'm', 'y']  \n['m', 'y', ' '] c\n['y', ' ', 'c'] o\n[' ', 'c', 'o'] u\n['c', 'o', 'u'] n\n['o', 'u', 'n'] t\n['u', 'n', 't'] r\n['n', 't', 'r'] y\n['t', 'r', 'y'] m\n['r', 'y', 'm'] e\n['y', 'm', 'e'] n\n['m', 'e', 'n'] ,\n['e', 'n', ',']  \n['n', ',', ' '] i\n[',', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] h\n['n', ' ', 'h'] a\n[' ', 'h', 'a'] n\n['h', 'a', 'n'] d\n['a', 'n', 'd'] ?\n['n', 'd', '?']  \n['d', '?', ' '] w\n['?', ' ', 'w'] h\n[' ', 'w', 'h'] e\n['w', 'h', 'e'] r\n['h', 'e', 'r'] e\n['e', 'r', 'e']  \n['r', 'e', ' '] g\n['e', ' ', 'g'] o\n[' ', 'g', 'o']  \n['g', 'o', ' '] y\n['o', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u'] \n\nrow:  ['W', 'i', 't', 'h', ' ', 'b', 'a', 't', 's', ' ', 'a', 'n', 'd', ' ', 'c', 'l', 'u', 'b', 's', '?', ' ', 'T', 'h', 'e', ' ', 'm', 'a', 't', 't', 'e', 'r', '?', ' ', 's', 'p', 'e', 'a', 'k', ',', ' ', 'I', ' ', 'p', 'r', 'a', 'y', ' ', 'y', 'o', 'u', '.', '\\n']\n['&lt;pad&gt;', 'W'] i\n['&lt;pad&gt;', 'W', 'i'] t\n['W', 'i', 't'] h\n['i', 't', 'h']  \n['t', 'h', ' '] b\n['h', ' ', 'b'] a\n[' ', 'b', 'a'] t\n['b', 'a', 't'] s\n['a', 't', 's']  \n['t', 's', ' '] a\n['s', ' ', 'a'] n\n[' ', 'a', 'n'] d\n['a', 'n', 'd']  \n['n', 'd', ' '] c\n['d', ' ', 'c'] l\n[' ', 'c', 'l'] u\n['c', 'l', 'u'] b\n['l', 'u', 'b'] s\n['u', 'b', 's'] ?\n['b', 's', '?']  \n['s', '?', ' '] T\n['?', ' ', 'T'] h\n[' ', 'T', 'h'] e\n['T', 'h', 'e']  \n['h', 'e', ' '] m\n['e', ' ', 'm'] a\n[' ', 'm', 'a'] t\n['m', 'a', 't'] t\n['a', 't', 't'] e\n['t', 't', 'e'] r\n['t', 'e', 'r'] ?\n['e', 'r', '?']  \n['r', '?', ' '] s\n['?', ' ', 's'] p\n[' ', 's', 'p'] e\n['s', 'p', 'e'] a\n['p', 'e', 'a'] k\n['e', 'a', 'k'] ,\n['a', 'k', ',']  \n['k', ',', ' '] I\n[',', ' ', 'I']  \n[' ', 'I', ' '] p\n['I', ' ', 'p'] r\n[' ', 'p', 'r'] a\n['p', 'r', 'a'] y\n['r', 'a', 'y']  \n['a', 'y', ' '] y\n['y', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u'] .\n['o', 'u', '.'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['O', 'u', 'r', ' ', 'b', 'u', 's', 'i', 'n', 'e', 's', 's', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'u', 'n', 'k', 'n', 'o', 'w', 'n', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 's', 'e', 'n', 'a', 't', 'e', ';', ' ', 't', 'h', 'e', 'y', ' ', 'h', 'a', 'v', 'e', '\\n']\n['&lt;pad&gt;', 'O'] u\n['&lt;pad&gt;', 'O', 'u'] r\n['O', 'u', 'r']  \n['u', 'r', ' '] b\n['r', ' ', 'b'] u\n[' ', 'b', 'u'] s\n['b', 'u', 's'] i\n['u', 's', 'i'] n\n['s', 'i', 'n'] e\n['i', 'n', 'e'] s\n['n', 'e', 's'] s\n['e', 's', 's']  \n['s', 's', ' '] i\n['s', ' ', 'i'] s\n[' ', 'i', 's']  \n['i', 's', ' '] n\n['s', ' ', 'n'] o\n[' ', 'n', 'o'] t\n['n', 'o', 't']  \n['o', 't', ' '] u\n['t', ' ', 'u'] n\n[' ', 'u', 'n'] k\n['u', 'n', 'k'] n\n['n', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w'] n\n['o', 'w', 'n']  \n['w', 'n', ' '] t\n['n', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] t\n['o', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e']  \n['h', 'e', ' '] s\n['e', ' ', 's'] e\n[' ', 's', 'e'] n\n['s', 'e', 'n'] a\n['e', 'n', 'a'] t\n['n', 'a', 't'] e\n['a', 't', 'e'] ;\n['t', 'e', ';']  \n['e', ';', ' '] t\n[';', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] y\n['h', 'e', 'y']  \n['e', 'y', ' '] h\n['y', ' ', 'h'] a\n[' ', 'h', 'a'] v\n['h', 'a', 'v'] e\n['a', 'v', 'e'] \n\nrow:  ['h', 'a', 'd', ' ', 'i', 'n', 'k', 'l', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'f', 'o', 'r', 't', 'n', 'i', 'g', 'h', 't', ' ', 'w', 'h', 'a', 't', ' ', 'w', 'e', ' ', 'i', 'n', 't', 'e', 'n', 'd', ' ', 't', 'o', ' ', 'd', 'o', ',', '\\n']\n['&lt;pad&gt;', 'h'] a\n['&lt;pad&gt;', 'h', 'a'] d\n['h', 'a', 'd']  \n['a', 'd', ' '] i\n['d', ' ', 'i'] n\n[' ', 'i', 'n'] k\n['i', 'n', 'k'] l\n['n', 'k', 'l'] i\n['k', 'l', 'i'] n\n['l', 'i', 'n'] g\n['i', 'n', 'g']  \n['n', 'g', ' '] t\n['g', ' ', 't'] h\n[' ', 't', 'h'] i\n['t', 'h', 'i'] s\n['h', 'i', 's']  \n['i', 's', ' '] f\n['s', ' ', 'f'] o\n[' ', 'f', 'o'] r\n['f', 'o', 'r'] t\n['o', 'r', 't'] n\n['r', 't', 'n'] i\n['t', 'n', 'i'] g\n['n', 'i', 'g'] h\n['i', 'g', 'h'] t\n['g', 'h', 't']  \n['h', 't', ' '] w\n['t', ' ', 'w'] h\n[' ', 'w', 'h'] a\n['w', 'h', 'a'] t\n['h', 'a', 't']  \n['a', 't', ' '] w\n['t', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] i\n['e', ' ', 'i'] n\n[' ', 'i', 'n'] t\n['i', 'n', 't'] e\n['n', 't', 'e'] n\n['t', 'e', 'n'] d\n['e', 'n', 'd']  \n['n', 'd', ' '] t\n['d', ' ', 't'] o\n[' ', 't', 'o']  \n['t', 'o', ' '] d\n['o', ' ', 'd'] o\n[' ', 'd', 'o'] ,\n['d', 'o', ','] \n\nrow:  ['w', 'h', 'i', 'c', 'h', ' ', 'n', 'o', 'w', ' ', 'w', 'e', \"'\", 'l', 'l', ' ', 's', 'h', 'o', 'w', ' ', \"'\", 'e', 'm', ' ', 'i', 'n', ' ', 'd', 'e', 'e', 'd', 's', '.', ' ', 'T', 'h', 'e', 'y', ' ', 's', 'a', 'y', ' ', 'p', 'o', 'o', 'r', '\\n']\n['&lt;pad&gt;', 'w'] h\n['&lt;pad&gt;', 'w', 'h'] i\n['w', 'h', 'i'] c\n['h', 'i', 'c'] h\n['i', 'c', 'h']  \n['c', 'h', ' '] n\n['h', ' ', 'n'] o\n[' ', 'n', 'o'] w\n['n', 'o', 'w']  \n['o', 'w', ' '] w\n['w', ' ', 'w'] e\n[' ', 'w', 'e'] '\n['w', 'e', \"'\"] l\n['e', \"'\", 'l'] l\n[\"'\", 'l', 'l']  \n['l', 'l', ' '] s\n['l', ' ', 's'] h\n[' ', 's', 'h'] o\n['s', 'h', 'o'] w\n['h', 'o', 'w']  \n['o', 'w', ' '] '\n['w', ' ', \"'\"] e\n[' ', \"'\", 'e'] m\n[\"'\", 'e', 'm']  \n['e', 'm', ' '] i\n['m', ' ', 'i'] n\n[' ', 'i', 'n']  \n['i', 'n', ' '] d\n['n', ' ', 'd'] e\n[' ', 'd', 'e'] e\n['d', 'e', 'e'] d\n['e', 'e', 'd'] s\n['e', 'd', 's'] .\n['d', 's', '.']  \n['s', '.', ' '] T\n['.', ' ', 'T'] h\n[' ', 'T', 'h'] e\n['T', 'h', 'e'] y\n['h', 'e', 'y']  \n['e', 'y', ' '] s\n['y', ' ', 's'] a\n[' ', 's', 'a'] y\n['s', 'a', 'y']  \n['a', 'y', ' '] p\n['y', ' ', 'p'] o\n[' ', 'p', 'o'] o\n['p', 'o', 'o'] r\n['o', 'o', 'r'] \n\nrow:  ['s', 'u', 'i', 't', 'o', 'r', 's', ' ', 'h', 'a', 'v', 'e', ' ', 's', 't', 'r', 'o', 'n', 'g', ' ', 'b', 'r', 'e', 'a', 't', 'h', 's', ':', ' ', 't', 'h', 'e', 'y', ' ', 's', 'h', 'a', 'l', 'l', ' ', 'k', 'n', 'o', 'w', ' ', 'w', 'e', '\\n']\n['&lt;pad&gt;', 's'] u\n['&lt;pad&gt;', 's', 'u'] i\n['s', 'u', 'i'] t\n['u', 'i', 't'] o\n['i', 't', 'o'] r\n['t', 'o', 'r'] s\n['o', 'r', 's']  \n['r', 's', ' '] h\n['s', ' ', 'h'] a\n[' ', 'h', 'a'] v\n['h', 'a', 'v'] e\n['a', 'v', 'e']  \n['v', 'e', ' '] s\n['e', ' ', 's'] t\n[' ', 's', 't'] r\n['s', 't', 'r'] o\n['t', 'r', 'o'] n\n['r', 'o', 'n'] g\n['o', 'n', 'g']  \n['n', 'g', ' '] b\n['g', ' ', 'b'] r\n[' ', 'b', 'r'] e\n['b', 'r', 'e'] a\n['r', 'e', 'a'] t\n['e', 'a', 't'] h\n['a', 't', 'h'] s\n['t', 'h', 's'] :\n['h', 's', ':']  \n['s', ':', ' '] t\n[':', ' ', 't'] h\n[' ', 't', 'h'] e\n['t', 'h', 'e'] y\n['h', 'e', 'y']  \n['e', 'y', ' '] s\n['y', ' ', 's'] h\n[' ', 's', 'h'] a\n['s', 'h', 'a'] l\n['h', 'a', 'l'] l\n['a', 'l', 'l']  \n['l', 'l', ' '] k\n['l', ' ', 'k'] n\n[' ', 'k', 'n'] o\n['k', 'n', 'o'] w\n['n', 'o', 'w']  \n['o', 'w', ' '] w\n['w', ' ', 'w'] e\n[' ', 'w', 'e'] \n\nrow:  ['h', 'a', 'v', 'e', ' ', 's', 't', 'r', 'o', 'n', 'g', ' ', 'a', 'r', 'm', 's', ' ', 't', 'o', 'o', '.', '\\n']\n['&lt;pad&gt;', 'h'] a\n['&lt;pad&gt;', 'h', 'a'] v\n['h', 'a', 'v'] e\n['a', 'v', 'e']  \n['v', 'e', ' '] s\n['e', ' ', 's'] t\n[' ', 's', 't'] r\n['s', 't', 'r'] o\n['t', 'r', 'o'] n\n['r', 'o', 'n'] g\n['o', 'n', 'g']  \n['n', 'g', ' '] a\n['g', ' ', 'a'] r\n[' ', 'a', 'r'] m\n['a', 'r', 'm'] s\n['r', 'm', 's']  \n['m', 's', ' '] t\n['s', ' ', 't'] o\n[' ', 't', 'o'] o\n['t', 'o', 'o'] .\n['o', 'o', '.'] \n\nrow:  ['M', 'E', 'N', 'E', 'N', 'I', 'U', 'S', ':', '\\n']\n['&lt;pad&gt;', 'M'] E\n['&lt;pad&gt;', 'M', 'E'] N\n['M', 'E', 'N'] E\n['E', 'N', 'E'] N\n['N', 'E', 'N'] I\n['E', 'N', 'I'] U\n['N', 'I', 'U'] S\n['I', 'U', 'S'] :\n['U', 'S', ':'] \n\nrow:  ['W', 'h', 'y', ',', ' ', 'm', 'a', 's', 't', 'e', 'r', 's', ',', ' ', 'm', 'y', ' ', 'g', 'o', 'o', 'd', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ',', ' ', 'm', 'i', 'n', 'e', ' ', 'h', 'o', 'n', 'e', 's', 't', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 's', ',', '\\n']\n['&lt;pad&gt;', 'W'] h\n['&lt;pad&gt;', 'W', 'h'] y\n['W', 'h', 'y'] ,\n['h', 'y', ',']  \n['y', ',', ' '] m\n[',', ' ', 'm'] a\n[' ', 'm', 'a'] s\n['m', 'a', 's'] t\n['a', 's', 't'] e\n['s', 't', 'e'] r\n['t', 'e', 'r'] s\n['e', 'r', 's'] ,\n['r', 's', ',']  \n['s', ',', ' '] m\n[',', ' ', 'm'] y\n[' ', 'm', 'y']  \n['m', 'y', ' '] g\n['y', ' ', 'g'] o\n[' ', 'g', 'o'] o\n['g', 'o', 'o'] d\n['o', 'o', 'd']  \n['o', 'd', ' '] f\n['d', ' ', 'f'] r\n[' ', 'f', 'r'] i\n['f', 'r', 'i'] e\n['r', 'i', 'e'] n\n['i', 'e', 'n'] d\n['e', 'n', 'd'] s\n['n', 'd', 's'] ,\n['d', 's', ',']  \n['s', ',', ' '] m\n[',', ' ', 'm'] i\n[' ', 'm', 'i'] n\n['m', 'i', 'n'] e\n['i', 'n', 'e']  \n['n', 'e', ' '] h\n['e', ' ', 'h'] o\n[' ', 'h', 'o'] n\n['h', 'o', 'n'] e\n['o', 'n', 'e'] s\n['n', 'e', 's'] t\n['e', 's', 't']  \n['s', 't', ' '] n\n['t', ' ', 'n'] e\n[' ', 'n', 'e'] i\n['n', 'e', 'i'] g\n['e', 'i', 'g'] h\n['i', 'g', 'h'] b\n['g', 'h', 'b'] o\n['h', 'b', 'o'] u\n['b', 'o', 'u'] r\n['o', 'u', 'r'] s\n['u', 'r', 's'] ,\n['r', 's', ','] \n\nrow:  ['W', 'i', 'l', 'l', ' ', 'y', 'o', 'u', ' ', 'u', 'n', 'd', 'o', ' ', 'y', 'o', 'u', 'r', 's', 'e', 'l', 'v', 'e', 's', '?', '\\n']\n['&lt;pad&gt;', 'W'] i\n['&lt;pad&gt;', 'W', 'i'] l\n['W', 'i', 'l'] l\n['i', 'l', 'l']  \n['l', 'l', ' '] y\n['l', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u']  \n['o', 'u', ' '] u\n['u', ' ', 'u'] n\n[' ', 'u', 'n'] d\n['u', 'n', 'd'] o\n['n', 'd', 'o']  \n['d', 'o', ' '] y\n['o', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u'] r\n['o', 'u', 'r'] s\n['u', 'r', 's'] e\n['r', 's', 'e'] l\n['s', 'e', 'l'] v\n['e', 'l', 'v'] e\n['l', 'v', 'e'] s\n['v', 'e', 's'] ?\n['e', 's', '?'] \n\nrow:  ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n']\n['&lt;pad&gt;', 'F'] i\n['&lt;pad&gt;', 'F', 'i'] r\n['F', 'i', 'r'] s\n['i', 'r', 's'] t\n['r', 's', 't']  \n['s', 't', ' '] C\n['t', ' ', 'C'] i\n[' ', 'C', 'i'] t\n['C', 'i', 't'] i\n['i', 't', 'i'] z\n['t', 'i', 'z'] e\n['i', 'z', 'e'] n\n['z', 'e', 'n'] :\n['e', 'n', ':'] \n\nrow:  ['W', 'e', ' ', 'c', 'a', 'n', 'n', 'o', 't', ',', ' ', 's', 'i', 'r', ',', ' ', 'w', 'e', ' ', 'a', 'r', 'e', ' ', 'u', 'n', 'd', 'o', 'n', 'e', ' ', 'a', 'l', 'r', 'e', 'a', 'd', 'y', '.', '\\n']\n['&lt;pad&gt;', 'W'] e\n['&lt;pad&gt;', 'W', 'e']  \n['W', 'e', ' '] c\n['e', ' ', 'c'] a\n[' ', 'c', 'a'] n\n['c', 'a', 'n'] n\n['a', 'n', 'n'] o\n['n', 'n', 'o'] t\n['n', 'o', 't'] ,\n['o', 't', ',']  \n['t', ',', ' '] s\n[',', ' ', 's'] i\n[' ', 's', 'i'] r\n['s', 'i', 'r'] ,\n['i', 'r', ',']  \n['r', ',', ' '] w\n[',', ' ', 'w'] e\n[' ', 'w', 'e']  \n['w', 'e', ' '] a\n['e', ' ', 'a'] r\n[' ', 'a', 'r'] e\n['a', 'r', 'e']  \n['r', 'e', ' '] u\n['e', ' ', 'u'] n\n[' ', 'u', 'n'] d\n['u', 'n', 'd'] o\n['n', 'd', 'o'] n\n['d', 'o', 'n'] e\n['o', 'n', 'e']  \n['n', 'e', ' '] a\n['e', ' ', 'a'] l\n[' ', 'a', 'l'] r\n['a', 'l', 'r'] e\n['l', 'r', 'e'] a\n['r', 'e', 'a'] d\n['e', 'a', 'd'] y\n['a', 'd', 'y'] .\n['d', 'y', '.'] \n\nrow:  ['M', 'E', 'N', 'E', 'N', 'I', 'U', 'S', ':', '\\n']\n['&lt;pad&gt;', 'M'] E\n['&lt;pad&gt;', 'M', 'E'] N\n['M', 'E', 'N'] E\n['E', 'N', 'E'] N\n['N', 'E', 'N'] I\n['E', 'N', 'I'] U\n['N', 'I', 'U'] S\n['I', 'U', 'S'] :\n['U', 'S', ':'] \n\nrow:  ['I', ' ', 't', 'e', 'l', 'l', ' ', 'y', 'o', 'u', ',', ' ', 'f', 'r', 'i', 'e', 'n', 'd', 's', ',', ' ', 'm', 'o', 's', 't', ' ', 'c', 'h', 'a', 'r', 'i', 't', 'a', 'b', 'l', 'e', ' ', 'c', 'a', 'r', 'e', '\\n']\n['&lt;pad&gt;', 'I']  \n['&lt;pad&gt;', 'I', ' '] t\n['I', ' ', 't'] e\n[' ', 't', 'e'] l\n['t', 'e', 'l'] l\n['e', 'l', 'l']  \n['l', 'l', ' '] y\n['l', ' ', 'y'] o\n[' ', 'y', 'o'] u\n['y', 'o', 'u'] ,\n['o', 'u', ',']  \n['u', ',', ' '] f\n[',', ' ', 'f'] r\n[' ', 'f', 'r'] i\n['f', 'r', 'i'] e\n['r', 'i', 'e'] n\n['i', 'e', 'n'] d\n['e', 'n', 'd'] s\n['n', 'd', 's'] ,\n['d', 's', ',']  \n['s', ',', ' '] m\n[',', ' ', 'm'] o\n[' ', 'm', 'o'] s\n['m', 'o', 's'] t\n['o', 's', 't']  \n['s', 't', ' '] c\n['t', ' ', 'c'] h\n[' ', 'c', 'h'] a\n['c', 'h', 'a'] r\n['h', 'a', 'r'] i\n['a', 'r', 'i'] t\n['r', 'i', 't'] a\n['i', 't', 'a'] b\n['t', 'a', 'b'] l\n['a', 'b', 'l'] e\n['b', 'l', 'e']  \n['l', 'e', ' '] c\n['e', ' ', 'c'] a\n[' ', 'c', 'a'] r\n['c', 'a', 'r'] e\n['a', 'r', 'e'] \n\nX:  torch.Size([2623, 3]) y: torch.Size([2623])",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#mlp-lm-model",
    "href": "models.lm.html#mlp-lm-model",
    "title": "Neural Net Language Models",
    "section": "MLP LM Model",
    "text": "MLP LM Model\nhttps://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n\n# F.one_hot(torch.tensor(5), num_classes=n_vocab).float()@C # == C[5]\n\n\nsource\n\nNNLM\n\n NNLM (n_vocab:int=30, n_emb:int=10, n_context:int=3, n_h:int=100)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_vocab\nint\n30\nvocabulary size\n\n\nn_emb\nint\n10\nembedding dimension\n\n\nn_context\nint\n3\ncontext size bigram/trigram, etc.\n\n\nn_h\nint\n100\nhidden layer size\n\n\n\n\nsource\n\n\nNNLMConfig\n\n NNLMConfig (n_vocab:int=30, n_emb:int=10, n_context:int=3, n_h:int=100)\n\n\n\nUsage\n\n# config model\nconf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\nlm = NNLM(**asdict(conf))\n\n# test data\nbs = 25\nx = torch.randint(conf.n_vocab, (bs, conf.n_context)) # (B, T) with values between 0 and n_vocab\nprint(\"X (B, T):\", x.shape)\n\n# prediction\ny = lm(x)\nprint(\"Y_hat logits (B, n_vocab):\", y.shape)\n\n[21:05:27] INFO - NNLM: Init\n\n\nX (B, T): torch.Size([25, 3])\nY_hat logits (B, n_vocab): torch.Size([25, 67])",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#train-mlp-lm",
    "href": "models.lm.html#train-mlp-lm",
    "title": "Neural Net Language Models",
    "section": "Train MLP LM",
    "text": "Train MLP LM\n\nhandmade dataset\n\nXtr, Ytr = make_dataset(data[:80], v, context_length=CONTEXT_LEN)\nXdev, Ydev = make_dataset(data[80:90], v)\nXte, Yte = make_dataset(data[90:100], v)\nprint(\"Xtr (B, T): \", Xtr.shape, \"Ytr (B): \", Ytr.shape, \"data:\", len(data[:80]))\nprint(\"len Xtr: \", len(Xtr))\nprint(\"CONTEXT_LEN: \", CONTEXT_LEN)\n\nXtr (B, T):  torch.Size([2623, 3]) Ytr (B):  torch.Size([2623]) data: 80\nlen Xtr:  2623\nCONTEXT_LEN:  3\n\n\n\nOverfit on subset of 80 first rows\n\ndevice = get_device()\ndevice = 'cpu'\n# lm.to(device)\n\n# overfit on one big batch\noptim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\ntrain_loss = []\nITER_MAX = 1000\nfor i in tqdm(range(ITER_MAX)):\n    # for batch in dm.train_dataloader():\n        # Xtr, Ytr = batch\n        # Ytr = Ytr[:, -1]\n        Xtr = Xtr.to(device)\n        Ytr = Ytr.to(device)\n        optim.zero_grad()\n        logits = lm(Xtr)\n        loss = F.cross_entropy(logits, Ytr)\n        loss.backward()\n        optim.step()\n        train_loss.append(loss.item())\n        if not(i%250):\n            print(loss.item())\n\n[21:05:27] INFO - Using device: mps\n\n\nCPU times: user 5 μs, sys: 4 μs, total: 9 μs\nWall time: 2.15 μs\n\n\n  6%|▌         | 59/1000 [00:00&lt;00:01, 588.98it/s]\n\n\n4.189066410064697\n\n\n 38%|███▊      | 376/1000 [00:00&lt;00:01, 611.20it/s]\n\n\n2.6202232837677\n\n\n 62%|██████▏   | 623/1000 [00:01&lt;00:00, 599.64it/s]\n\n\n2.3751416206359863\n\n\n 86%|████████▌ | 860/1000 [00:01&lt;00:00, 535.46it/s]\n\n\n2.199942111968994\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 575.01it/s]\n\n\n\nplt.plot(train_loss)\n\n\n\n\n\n\n\n\n\n\nSample\n\n# infer on CPU\nlm.to('cpu')\nprompt = \"The country of \"\nsequences = lm.sample(prompt, v, max_new_tokens=250, temperature=0.6)\nprint(sequences)\n\nThe country of that Citiz he that ss aer he the the hat yire to den fim to the the bu fh thas you ko gour puicen: cat Citizen:\n\nYYde we pa coond Citizes aft iaizet arr you arat we poor pour, bat peo:\n peap hith hame her to the fat you apithey hiss bou to the so the\n\n\n\n\n\nDataloader\n\ncfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\n# use &lt;unk&gt; and &lt;pad&gt; to be consistent with manual data preprocessing and have smae vocabulary size\nv = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['&lt;unk&gt;','&lt;pad&gt;'])\nprint(\"vocabulary: \", v.vocabulary)\nprint(\"vocabulary size: \", len(v))\nprint(cfg)\ncfg.train_val_test_split = [0.8, 0.1, 0.1]\n# by default data_path is relative to the recipe folder so need to update for nbs\ncfg.data_path = \"../data/text/tiny_shakespeare.txt\"\ncfg.context_size = CONTEXT_LEN\ncfg.batch_size = 2700 # large batch to mimic manual data order of magnitude\ncfg.random_split = False\ncfg.specials=['&lt;unk&gt;', '&lt;pad&gt;']\ncfg.add_sentence_tokens = False\nprint(cfg)\ndm = instantiate(cfg)\ndm.setup()\nprint(\"vocab size: \", dm.vocab_size)\n# setup large batch to overfit / test model\nXtr, Ytr= next(iter(dm.train_dataloader()))\n# target is last token in sequence\nYtr = Ytr[:, -1]\nprint(\"Xtr (B, T): \", Xtr.shape, \"Ytr (B): \", Ytr.shape)\nX, Y = dm.train_ds[0]\nprint(dm.ds.from_tokens(X), dm.ds.from_tokens(Y))\n\n[21:05:29] INFO - Vocab: read text file\n[21:05:29] INFO - CharDataModule: init\n[21:05:29] INFO - CharDataModule: setup, split datasets\n[21:05:29] INFO - CharDataset: init\n[21:05:29] INFO - Vocab: read text file\n[21:05:29] INFO - Split dataset into train/val/test. Keep sequence order.\n\n\nvocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '&lt;pad&gt;', '&lt;unk&gt;', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\nvocabulary size:  67\n{'_target_': 'nimrod.text.datasets.CharDataModule', 'data_path': '../data/text/tiny_shakespeare.txt', 'specials': ['&lt;pad&gt;', '&lt;unk&gt;'], 'add_sentence_tokens': False, 'train_val_test_split': [0.8, 0.1, 0.1], 'random_split': False, 'batch_size': 64, 'context_size': 3, 'num_workers': 0, 'pin_memory': False, 'persistent_workers': False}\n{'_target_': 'nimrod.text.datasets.CharDataModule', 'data_path': '../data/text/tiny_shakespeare.txt', 'specials': ['&lt;unk&gt;', '&lt;pad&gt;'], 'add_sentence_tokens': False, 'train_val_test_split': [0.8, 0.1, 0.1], 'random_split': False, 'batch_size': 2700, 'context_size': 3, 'num_workers': 0, 'pin_memory': False, 'persistent_workers': False}\nvocab size:  67\nXtr (B, T):  torch.Size([2700, 3]) Ytr (B):  torch.Size([2700])\nFir irs\n\n\n\nconf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\nprint(len(v), CONTEXT_LEN)\nlm = NNLM(**asdict(conf))\nbs = 10\nx = torch.randint(conf.n_vocab, (bs, conf.n_context)) # (B, T) with values between 0 and n_vocab\nprint(\"X (B, T):\", x.shape)\nlm(x).shape\n\n[21:05:29] INFO - NNLM: Init\n\n\n67 3\nX (B, T): torch.Size([10, 3])\n\n\ntorch.Size([10, 67])\n\n\n\n# checking data tokens are between 0 and vocab size\nprint(Xtr.min(),  Xtr.max())\n\ntensor(0) tensor(66)\n\n\n\noverfit\n\n# device = get_device()\ndevice = 'cpu'\nlm.to(device)\n\n# overfit on one big batch\noptim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\ntrain_loss = []\nITER_MAX = 1000\nfor i in tqdm(range(ITER_MAX)):\n    # for batch in dm.train_dataloader():\n        # Xtr, Ytr = batch\n        # Ytr = Ytr[:, -1]\n        Xtr = Xtr.to(device)\n        Ytr = Ytr.to(device)\n        optim.zero_grad()\n        logits = lm(Xtr)\n        loss = F.cross_entropy(logits, Ytr)\n        loss.backward()\n        optim.step()\n        train_loss.append(loss.item())\n        if not(i%250):\n            print(loss.item())\n\nCPU times: user 5 μs, sys: 3 μs, total: 8 μs\nWall time: 14.1 μs\n\n\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]\n\n\n4.245690822601318\n\n\n 32%|███▏      | 315/1000 [00:00&lt;00:01, 632.60it/s]\n\n\n2.785951852798462\n\n\n 58%|█████▊    | 584/1000 [00:00&lt;00:00, 658.39it/s]\n\n\n2.5501415729522705\n\n\n 88%|████████▊ | 877/1000 [00:01&lt;00:00, 710.10it/s]\n\n\n2.4038991928100586\n\n\n100%|██████████| 1000/1000 [00:01&lt;00:00, 660.55it/s]\n\n\n\nplt.plot(train_loss)\n\n\n\n\n\n\n\n\n\n# infer on CPU\nlm.to('cpu')\nprompt = \"The country of \"\nsequences = lm.sample(prompt, v, max_new_tokens=250, temperature=0.6)\nprint(sequences)\n\nThe country of the er thoR mis thath the en sean en od thath\nThe Itrer lower wy he forat ime they thands siee s and, in thand and sourus is has that R her bovertho for hat wom tuer hand ny deme tharpit us me the ond Iore forr,aty de chee utrer str and ne tore yode\n\n\n\n\n\nBatching with dataloaders\n\n# mini batch gradient descent with datamodule\ncfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\ncfg.train_val_test_split = [0.8, 0.1, 0.1]\ncfg.data_path = \"../data/text/tiny_shakespeare.txt\"\ncfg.context_size = CONTEXT_LEN\ncfg.batch_size = 2048\ncfg.random_split = False\ncfg.specials=['&lt;unk&gt;', '&lt;pad&gt;']\ncfg.add_sentence_tokens = False\ndm = instantiate(cfg)\ndm.setup()\n\nconf = NNLMConfig(n_vocab=len(v), n_context=CONTEXT_LEN)\nlm = NNLM(**asdict(conf))\n\n[21:05:31] INFO - CharDataModule: init\n[21:05:31] INFO - CharDataModule: setup, split datasets\n[21:05:31] INFO - CharDataset: init\n[21:05:31] INFO - Vocab: read text file\n[21:05:31] INFO - Split dataset into train/val/test. Keep sequence order.\n[21:05:31] INFO - NNLM: Init\n\n\n\noptim = SGD(lm.parameters(), lr=0.01, momentum=0.9)\ntrain_loss = []\n# device = get_device()\ndevice = 'cpu'\nlm.to(device)\ni = 0\nEPOCHS = 1\nfor epoch in tqdm(range(EPOCHS)):\n    print(f\"epoch {epoch}\")\n    for batch in tqdm(dm.train_dataloader()):\n        Xtr, Ytr = batch\n        # target is last token in sequence\n        Ytr = Ytr[:, -1] # BxT\n        Xtr = Xtr.to(device)\n        Ytr = Ytr.to(device)\n        \n        logits = lm(Xtr)\n        loss = F.cross_entropy(logits, Ytr)\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n\n        train_loss.append(loss.item())\n        if not(i%1000):\n            print(loss.item())\n        i += 1\n\nCPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\nWall time: 2.86 μs\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\nepoch 0\n\n\n\n\n\n4.255069255828857\n\n\n100%|██████████| 436/436 [00:04&lt;00:00, 102.77it/s]\n100%|██████████| 1/1 [00:04&lt;00:00,  4.24s/it]\n\n\n\nplt.plot(train_loss)\n\n\n\n\n\n\n\n\n\n\nSample\n\n# infer on CPU\nlm.to('cpu')\nprompt = \"The country of \"\nsequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\nprint(sequences)\n\nThe country of taana, barN.g! eat th re dae b, lod tend his anc tolize \narod ong\nC FB Sind frmae g,alfre y m-vxfYout lo w\n\n\n\nXht mn is woEhas foroofd ty uhismam the touw ier, wlins erud ig, nougs hit nn worle ay pnrgells hy \n&$, hut sh lis mis unnd\nuolem\ny the .erersDmater,seylr.\nAnns topgovbso nrt oh totlm :umd hadxhe u wow:he bA sep i; lo raa\nwouor se th Uho!llpdd?Qugot tousus sa f hI stheris \noll y, \na nn  ooltcey cany hQm,vf aurswoua ar, aolnvend hey storg oS ;ins ken oob'chit nf hasoufy brOir. momJ thithe",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#mlp-lm-x-model",
    "href": "models.lm.html#mlp-lm-x-model",
    "title": "Neural Net Language Models",
    "section": "MLP LM X Model",
    "text": "MLP LM X Model\n\nsource\n\nNNLM_X\n\n NNLM_X (nnet:__main__.NNLM, num_classes:int,\n         optimizer:torch.optim.optimizer.Optimizer, scheduler:&lt;module'torc\n         h.optim.lr_scheduler'from'/opt/hostedtoolcache/Python/3.10.16/x64\n         /lib/python3.10/site-packages/torch/optim/lr_scheduler.py'&gt;)\n\nHelper class that provides a standard way to create an ABC using inheritance.\n\n\nUsage\n\n# omegaconf\ncfg = OmegaConf.load(\"../config/text/model/nnlm.yaml\")\ncfg.num_classes = len(v)\nprint(len(v))\n# have to convert omegaconf dict to dict for pprint\nopt = instantiate(cfg.optimizer)\nprint(opt.keywords['lr'])\n\npprint.pprint(dict(cfg))\nlm  = instantiate(cfg)\nprint(lm.hparams.optimizer)\n\n[21:06:10] INFO - NNLM: Init\n[21:06:10] INFO - NNLM_X: Init\n\n\n67\n0.001\n{'_target_': 'nimrod.models.lm.NNLM_X',\n 'nnet': {'_target_': 'nimrod.models.lm.NNLM', 'n_vocab': '${num_classes}', 'n_emb': 10, 'n_context': 3, 'n_h': 100},\n 'num_classes': 67,\n 'optimizer': {'_target_': 'torch.optim.Adam', '_partial_': True, 'lr': 0.001, 'weight_decay': 1e-05},\n 'scheduler': {'_target_': 'torch.optim.lr_scheduler.ReduceLROnPlateau', '_partial_': True, 'mode': 'min', 'factor': 0.1, 'patience': 10}}\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:92, in _call_target(_target_, _partial_, args, kwargs, full_key)\n     91 try:\n---&gt; 92     return _target_(*args, **kwargs)\n     93 except Exception as e:\n\nFile ~/Projects/nimrod/nimrod/models/lm.py:99, in NNLM_X.__init__(self, nnet, num_classes, optimizer, scheduler)\n     98 logger.info(\"NNLM_X: Init\")\n---&gt; 99 super().__init__(\n    100     num_classes,\n    101     optimizer,\n    102     scheduler,\n    103     )\n    104 self.save_hyperparameters(logger=False)\n\nTypeError: Classifier.__init__() missing 1 required positional argument: 'scheduler'\n\nThe above exception was the direct cause of the following exception:\n\nInstantiationException                    Traceback (most recent call last)\nCell In[30], line 10\n      7 print(opt.keywords['lr'])\n      9 pprint.pprint(dict(cfg))\n---&gt; 10 lm  = instantiate(cfg)\n     11 print(lm.hparams.optimizer)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:226, in instantiate(config, *args, **kwargs)\n    223     _convert_ = config.pop(_Keys.CONVERT, ConvertMode.NONE)\n    224     _partial_ = config.pop(_Keys.PARTIAL, False)\n--&gt; 226     return instantiate_node(\n    227         config, *args, recursive=_recursive_, convert=_convert_, partial=_partial_\n    228     )\n    229 elif OmegaConf.is_list(config):\n    230     # Finalize config (convert targets to strings, merge with kwargs)\n    231     config_copy = copy.deepcopy(config)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:347, in instantiate_node(node, convert, recursive, partial, *args)\n    342                 value = instantiate_node(\n    343                     value, convert=convert, recursive=recursive\n    344                 )\n    345             kwargs[key] = _convert_node(value, convert)\n--&gt; 347     return _call_target(_target_, partial, args, kwargs, full_key)\n    348 else:\n    349     # If ALL or PARTIAL non structured or OBJECT non structured,\n    350     # instantiate in dict and resolve interpolations eagerly.\n    351     if convert == ConvertMode.ALL or (\n    352         convert in (ConvertMode.PARTIAL, ConvertMode.OBJECT)\n    353         and node._metadata.object_type in (None, dict)\n    354     ):\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:97, in _call_target(_target_, _partial_, args, kwargs, full_key)\n     95 if full_key:\n     96     msg += f\"\\nfull_key: {full_key}\"\n---&gt; 97 raise InstantiationException(msg) from e\n\nInstantiationException: Error in call to target 'nimrod.models.lm.NNLM_X':\nTypeError(\"Classifier.__init__() missing 1 required positional argument: 'scheduler'\")\n\n\n\n\nn_samples = 25\nx = torch.randint(conf.n_vocab, (n_samples, cfg.nnet.n_context))\nprint(\"X:\", x.shape)\n\n\ny = lm(x)\nprint(\"Y_hat logits:\", y.shape)\n\n\n# v = Vocab(data_path='../data/text/tiny_shakespeare.txt', specials=['&lt;unk&gt;','&lt;pad&gt;'])\nlm.sample(\"The country of \", v, max_new_tokens=500, temperature=0.9)\n\n\n\nL Training Loop\n\n# vocab\nprint(len(v))\n\n# data\ncfg = OmegaConf.load(\"../config/text/data/tinyshakespeare.yaml\")\ncfg.context_size = CONTEXT_LEN\ncfg.specials: [\"&lt;pad&gt;\", \"&lt;unk&gt;\"]\ncfg.batch_size = 2048\ncfg.random_split = False\ndm = instantiate(cfg)\ndm.setup()\n\n# model\ncfg = OmegaConf.load(\"../config/text/model/nnlm.yaml\")\nlm  = instantiate(cfg)\n\n\nprint(lm.__dict__)\n\nmodel can be easily trained with L trainer (c.f. recipes/text/ for examples)\n\ntrainer = Trainer(\n    accelerator=\"auto\",\n    max_epochs=1,\n    logger=CSVLogger(\"logs\", name=\"nnlm\")\n    )\n\n\ntrainer.fit(lm, dm.train_dataloader(), dm.val_dataloader())\n\n\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head()\n\n\nplt.plot(metrics['step'], metrics['train/loss_step'],'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.show()\n\n\ntrainer.test(lm, dm.test_dataloader())\n\n\n# infer on CPU\nlm.to('cpu')\nprompt = \"The country of \"\nsequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\nprint(sequences)\n\n\n\nLearning Rate Finder\n\nlm.hparams\n\n\ntrainer = L.Trainer(\n    accelerator=\"auto\",\n    max_epochs=1,\n)\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(\n    lm,\n    datamodule=dm,\n    min_lr=1e-6,\n    max_lr=1.0,\n    num_training=100,  # number of iterations\n    # attr_name=\"optimizer.lr\",\n)\nfig = lr_finder.plot(suggest=True)\nplt.show()\nprint(f\"Suggested learning rate: {lr_finder.suggestion()}\")\n\n\n\nre-train with new lr\n\nnew_lr = lr_finder.suggestion()\nlm.lr = new_lr\n\n\ntrainer = L.Trainer(\n    accelerator=\"auto\",\n    max_epochs=1,\n    logger=CSVLogger(\"logs\", name=\"nnlm\"),\n)\ntrainer.fit(lm, dm.train_dataloader(), dm.val_dataloader())\ntrainer.test(lm, dm.test_dataloader())\n\n\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nplt.plot(metrics['step'], metrics['train/loss_step'],'.-')\n# plt.figure()\n# plot_classifier_metrics_from_csv(csv_path)\n\n\n# infer on CPU\nlm.to('cpu')\nprompt = \"The country of \"\nsequences = lm.sample(prompt, v, max_new_tokens=500, temperature=0.9)\nprint(sequences)",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#nn-bigram",
    "href": "models.lm.html#nn-bigram",
    "title": "Neural Net Language Models",
    "section": "NN Bigram",
    "text": "NN Bigram\n\nsource\n\nNNBigram\n\n NNBigram (vocab_size:int)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\nUsage\n\nB, T, C = 32, 8, 65\nvocab_size = C\nmodel = NNBigram(vocab_size)\nprint(\"vocab size: \",  model.vocab_size)\nX = torch.randint(0,C,(B,T))\nY = torch.randint(0,C,(B,T))\nbatch = (X,Y)\nlogits = model(X) # (B, T, C)\nprint(\"X: \", X.shape, \"Y: \", Y.shape, \"logits: \", logits.shape)\n\n\n# generate\nmodel.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0]\n\n\n# #| export\n# class NNBigramL(ModelModule):\n#     def __init__(self, vocab_size:int, lr:float=1e-3):\n#         model = NNBigram(vocab_size)\n#         super().__init__(model, lr)\n#         self.accuracy = Accuracy(task='multiclass', num_classes=model.vocab_size)\n\n#     def _step(self, batch:torch.tensor, batch_idx:int):\n#         x, y = batch\n#         logits = self.model(x) # (B,T,C)\n#         B, T, C = logits.shape\n#         logits = logits.view(B*T, C)\n#         y = y.view(B*T)\n#         loss = self.loss(logits, y)\n#         acc = self.accuracy(logits, y)\n#         return loss, acc\n    \n#     def predict(self,idx:torch.IntTensor, max_new_tokens:int):\n#         return self.model.predict(idx, max_new_tokens)\n\n\n# model_pl = NNBigramL(vocab_size)\n# logits = model_pl(X) # (B, T, C)\n# print(logits.shape)\n# model_pl.training_step(batch, 0)\n# model_pl._step(batch, 0)\n\n\n\nData\n\nwith open('../data/text/tiny_shakespeare.txt') as f:\n    text = f.read()\n\n\n# dataset\nblock_size = 8\nds = CharDataset(data_path='../data/text/tiny_shakespeare.txt', context_length=block_size)\nX,Y = ds[0]\nprint(\"x:\",  ds.from_tokens(X), \"\\ny:\", ds.from_tokens(Y))\n\n\n# dataloader\ndl = DataLoader(ds, batch_size=32, num_workers=0)\nX, Y = next(iter(dl))\nprint(\"x:\", X.shape, \"\\ny:\", Y.shape)\n\n\n\nTraining\n\nmodel = NNBigram(ds.vocab_size)\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\ndevice = torch.device('cpu')\n\n\nITER_MAX = 1000\ntrain_loss = []\nfor epoch in tqdm(range(ITER_MAX)):\n    model.train()\n    X = X.to(device) # (B,T)\n    Y = Y.to(device) # (B,T)\n    logits = model(X)\n    B, T, C = logits.shape\n    loss = criterion(logits.view(B*T, C), Y.view(B*T))\n    loss.backward()\n    optimizer.step()\n    train_loss.append(loss.item())\n    if not(epoch % 1000):\n        print(loss.item())\n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        logits = model(X).view(B*T,C) \n        # _, predicted = torch.max(logits.data, 1)\n        probs = F.softmax(logits, dim=-1)\n        # print(\"probs: \", probs.shape)\n        preds = torch.argmax(probs, dim=1)\n        # print(\"pred:\", preds.shape)\n        # print(\"Y:\", Y.shape)\n        # print(predicted)\n        # total += Y.size(0)\n        # correct += (predicted == Y).sum()\n        # print(f\"Epoch {epoch + 1}: Accuracy = {100 * correct / total:.2f}%\")\n\n\nplt.plot(train_loss)\n\n\nprint(ds.from_tokens(model.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.lm.html#training-from-module",
    "href": "models.lm.html#training-from-module",
    "title": "Neural Net Language Models",
    "section": "training from module",
    "text": "training from module\n\n# %%time\n# n_epochs = 1\n# train_loss = []\n# for epoch in range(n_epochs):\n#     model_pl.model.train()\n#     loss = model_pl.training_step(batch, None)\n#     loss.backward()\n#     optimizer.step()\n#     train_loss.append(loss.item())\n#     if not(epoch % 100):\n#         print(loss.item())\n\n\n# print(ds.from_tokens(model_pl.predict(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))",
    "crumbs": [
      "Text",
      "Models",
      "Neural Net Language Models"
    ]
  },
  {
    "objectID": "models.conv.html",
    "href": "models.conv.html",
    "title": "Convolution Neural Networks",
    "section": "",
    "text": "cfg = OmegaConf.load('../config/data/image/mnist.yaml')\ndm = instantiate(cfg)\ndm.prepare_data()\ndm.setup()\n\n[14:49:25] INFO - Init ImageDataModule for mnist\n[14:49:29] INFO - loading dataset mnist with args () from split train\n[14:49:29] INFO - loading dataset mnist from split train\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[6], line 4\n      2 cfg = OmegaConf.load('../config/data/image/mnist.yaml')\n      3 dm = instantiate(cfg)\n----&gt; 4 dm.prepare_data()\n      5 dm.setup()\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:415, in ImageDataModule.prepare_data(self)\n    412 \"\"\"Download data if needed \n    413 \"\"\"\n    414 # train set\n--&gt; 415 self.train_ds = ImageDataset(\n    416     self.hparams.name,\n    417     *self.args,\n    418     data_dir = self.hparams.data_dir,\n    419     split='train',\n    420     transforms = self.hparams.transforms,\n    421     **self.kwargs\n    422 )\n    423 # get num classes before setup method converst ImageDataset to Subset\n    424 self._num_classes = self.train_ds.num_classes\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:228, in ImageDataset.__init__(self, name, data_dir, split, transforms, streaming, exclude_grey_scale, verification_mode, from_image_folder, from_disk, *args)\n    226 else:\n    227     logger.info(f\"loading dataset {name} from split {split}\")\n--&gt; 228     self.hf_ds = load_dataset(\n    229         name,\n    230         *args,\n    231         split=split,\n    232         cache_dir=data_dir,\n    233         download_mode='reuse_dataset_if_exists',\n    234         streaming=streaming,\n    235         verification_mode=verification_mode\n    236         \n    237     )\n    239 # CHANGE IMAGE COLUMN NAME IF NEEDED\n    240 self.image_column_name = 'image'\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:2129, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\n   2124 verification_mode = VerificationMode(\n   2125     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\n   2126 )\n   2128 # Create a dataset builder\n-&gt; 2129 builder_instance = load_dataset_builder(\n   2130     path=path,\n   2131     name=name,\n   2132     data_dir=data_dir,\n   2133     data_files=data_files,\n   2134     cache_dir=cache_dir,\n   2135     features=features,\n   2136     download_config=download_config,\n   2137     download_mode=download_mode,\n   2138     revision=revision,\n   2139     token=token,\n   2140     storage_options=storage_options,\n   2141     trust_remote_code=trust_remote_code,\n   2142     _require_default_config_name=name is None,\n   2143     **config_kwargs,\n   2144 )\n   2146 # Return iterable dataset in case of streaming\n   2147 if streaming:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n   1885 # Instantiate the dataset builder\n-&gt; 1886 builder_instance: DatasetBuilder = builder_cls(\n   1887     cache_dir=cache_dir,\n   1888     dataset_name=dataset_name,\n   1889     config_name=config_name,\n   1890     data_dir=data_dir,\n   1891     data_files=data_files,\n   1892     hash=dataset_module.hash,\n   1893     info=info,\n   1894     features=features,\n   1895     token=token,\n   1896     storage_options=storage_options,\n   1897     **builder_kwargs,\n   1898     **config_kwargs,\n   1899 )\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n   1902 return builder_instance\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:342, in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    340     config_kwargs[\"data_dir\"] = data_dir\n    341 self.config_kwargs = config_kwargs\n--&gt; 342 self.config, self.config_id = self._create_builder_config(\n    343     config_name=config_name,\n    344     custom_features=features,\n    345     **config_kwargs,\n    346 )\n    348 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    349 # Prefill datasetinfo\n    350 if info is None:\n    351     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:597, in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    594     raise ValueError(f\"BuilderConfig must have a name, got {builder_config.name}\")\n    596 # resolve data files if needed\n--&gt; 597 builder_config._resolve_data_files(\n    598     base_path=self.base_path,\n    599     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    600 )\n    602 # compute the config id that is going to be used for caching\n    603 config_id = builder_config.create_config_id(\n    604     config_kwargs,\n    605     custom_features=custom_features,\n    606 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:206, in BuilderConfig._resolve_data_files(self, base_path, download_config)\n    204 if isinstance(self.data_files, DataFilesPatternsDict):\n    205     base_path = xjoin(base_path, self.data_dir) if self.data_dir else base_path\n--&gt; 206     self.data_files = self.data_files.resolve(base_path, download_config)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:818, in DataFilesPatternsDict.resolve(self, base_path, download_config)\n    816 out = DataFilesDict()\n    817 for key, data_files_patterns_list in self.items():\n--&gt; 818     out[key] = data_files_patterns_list.resolve(base_path, download_config)\n    819 return out\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:771, in DataFilesPatternsList.resolve(self, base_path, download_config)\n    768 for pattern, allowed_extensions in zip(self, self.allowed_extensions):\n    769     try:\n    770         data_files.extend(\n--&gt; 771             resolve_pattern(\n    772                 pattern,\n    773                 base_path=base_path,\n    774                 allowed_extensions=allowed_extensions,\n    775                 download_config=download_config,\n    776             )\n    777         )\n    778     except FileNotFoundError:\n    779         if not has_magic(pattern):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:388, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\n    383 if protocol == \"hf\" and config.HF_HUB_VERSION &gt;= version.parse(\"0.20.0\"):\n    384     # 10 times faster glob with detail=True (ignores costly info like lastCommit)\n    385     glob_kwargs[\"expand_info\"] = False\n    386 matched_paths = [\n    387     filepath if filepath.startswith(protocol_prefix) else protocol_prefix + filepath\n--&gt; 388     for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n    389     if info[\"type\"] == \"file\"\n    390     and (xbasename(filepath) not in files_to_ignore)\n    391     and not _is_inside_unrequested_special_dir(filepath, fs_pattern)\n    392     and not _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n    393 ]  # ignore .ipynb and __pycache__, but keep /../\n    394 if allowed_extensions is not None:\n    395     out = [\n    396         filepath\n    397         for filepath in matched_paths\n    398         if any(\".\" + suffix in allowed_extensions for suffix in xbasename(filepath).split(\".\")[1:])\n    399     ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:521, in HfFileSystem.glob(self, path, **kwargs)\n    519 kwargs = {\"expand_info\": kwargs.get(\"detail\", False), **kwargs}\n    520 path = self.resolve_path(path, revision=kwargs.get(\"revision\")).unresolve()\n--&gt; 521 return super().glob(path, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:611, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs)\n    608     else:\n    609         depth = None\n--&gt; 611 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n    613 pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n    614 pattern = re.compile(pattern)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:556, in HfFileSystem.find(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\n    533 \"\"\"\n    534 List all files below path.\n    535 \n   (...)\n    553     `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\n    554 \"\"\"\n    555 if maxdepth:\n--&gt; 556     return super().find(\n    557         path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, refresh=refresh, revision=revision, **kwargs\n    558     )\n    559 resolved_path = self.resolve_path(path, revision=revision)\n    560 path = resolved_path.unresolve()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:502, in AbstractFileSystem.find(self, path, maxdepth, withdirs, detail, **kwargs)\n    499 # Add the root directory if withdirs is requested\n    500 # This is needed for posix glob compliance\n    501 if withdirs and path != \"\" and self.isdir(path):\n--&gt; 502     out[path] = self.info(path)\n    504 for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\n    505     if withdirs:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:719, in HfFileSystem.info(self, path, refresh, revision, **kwargs)\n    717     out = out1[0]\n    718 if refresh or out is None or (expand_info and out and out[\"last_commit\"] is None):\n--&gt; 719     paths_info = self._api.get_paths_info(\n    720         resolved_path.repo_id,\n    721         resolved_path.path_in_repo,\n    722         expand=expand_info,\n    723         revision=resolved_path.revision,\n    724         repo_type=resolved_path.repo_type,\n    725     )\n    726     if not paths_info:\n    727         _raise_file_not_found(path, None)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    111 if check_use_auth_token:\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n--&gt; 114 return fn(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3295, in HfApi.get_paths_info(self, repo_id, paths, expand, revision, repo_type, token)\n   3292 revision = quote(revision, safe=\"\") if revision is not None else constants.DEFAULT_REVISION\n   3293 headers = self._build_hf_headers(token=token)\n-&gt; 3295 response = get_session().post(\n   3296     f\"{self.endpoint}/api/{repo_type}s/{repo_id}/paths-info/{revision}\",\n   3297     data={\n   3298         \"paths\": paths if isinstance(paths, list) else [paths],\n   3299         \"expand\": expand,\n   3300     },\n   3301     headers=headers,\n   3302 )\n   3303 hf_raise_for_status(response)\n   3304 paths_info = response.json()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs)\n    626 def post(self, url, data=None, json=None, **kwargs):\n    627     r\"\"\"Sends a POST request. Returns :class:`Response` object.\n    628 \n    629     :param url: URL for the new :class:`Request` object.\n   (...)\n    634     :rtype: requests.Response\n    635     \"\"\"\n--&gt; 637     return self.request(\"POST\", url, data=data, json=json, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    584 send_kwargs = {\n    585     \"timeout\": timeout,\n    586     \"allow_redirects\": allow_redirects,\n    587 }\n    588 send_kwargs.update(settings)\n--&gt; 589 resp = self.send(prep, **send_kwargs)\n    591 return resp\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:724, in Session.send(self, request, **kwargs)\n    721 if allow_redirects:\n    722     # Redirect resolving generator.\n    723     gen = self.resolve_redirects(r, request, **kwargs)\n--&gt; 724     history = [resp for resp in gen]\n    725 else:\n    726     history = []\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:724, in &lt;listcomp&gt;(.0)\n    721 if allow_redirects:\n    722     # Redirect resolving generator.\n    723     gen = self.resolve_redirects(r, request, **kwargs)\n--&gt; 724     history = [resp for resp in gen]\n    725 else:\n    726     history = []\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:265, in SessionRedirectMixin.resolve_redirects(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\n    263     yield req\n    264 else:\n--&gt; 265     resp = self.send(\n    266         req,\n    267         stream=stream,\n    268         timeout=timeout,\n    269         verify=verify,\n    270         cert=cert,\n    271         proxies=proxies,\n    272         allow_redirects=False,\n    273         **adapter_kwargs,\n    274     )\n    276     extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n    278     # extract redirect url, if any, for the next loop\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs)\n    700 start = preferred_clock()\n    702 # Send the request\n--&gt; 703 r = adapter.send(request, **kwargs)\n    705 # Total elapsed time of the request (approximately)\n    706 elapsed = preferred_clock() - start\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:93, in UniqueRequestIdAdapter.send(self, request, *args, **kwargs)\n     91 \"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\n     92 try:\n---&gt; 93     return super().send(request, *args, **kwargs)\n     94 except requests.RequestException as e:\n     95     request_id = request.headers.get(X_AMZN_TRACE_ID)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/adapters.py:667, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    664     timeout = TimeoutSauce(connect=timeout, read=timeout)\n    666 try:\n--&gt; 667     resp = conn.urlopen(\n    668         method=request.method,\n    669         url=url,\n    670         body=request.body,\n    671         headers=request.headers,\n    672         redirect=False,\n    673         assert_same_host=False,\n    674         preload_content=False,\n    675         decode_content=False,\n    676         retries=self.max_retries,\n    677         timeout=timeout,\n    678         chunked=chunked,\n    679     )\n    681 except (ProtocolError, OSError) as err:\n    682     raise ConnectionError(err, request=request)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    784 response_conn = conn if not release_conn else None\n    786 # Make the request on the HTTPConnection object\n--&gt; 787 response = self._make_request(\n    788     conn,\n    789     method,\n    790     url,\n    791     timeout=timeout_obj,\n    792     body=body,\n    793     headers=headers,\n    794     chunked=chunked,\n    795     retries=retries,\n    796     response_conn=response_conn,\n    797     preload_content=preload_content,\n    798     decode_content=decode_content,\n    799     **response_kw,\n    800 )\n    802 # Everything went great!\n    803 clean_exit = True\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connectionpool.py:534, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    532 # Receive the response from the server\n    533 try:\n--&gt; 534     response = conn.getresponse()\n    535 except (BaseSSLError, OSError) as e:\n    536     self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connection.py:516, in HTTPConnection.getresponse(self)\n    513 _shutdown = getattr(self.sock, \"shutdown\", None)\n    515 # Get the response from http.client.HTTPConnection\n--&gt; 516 httplib_response = super().getresponse()\n    518 try:\n    519     assert_header_parsing(httplib_response.msg)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:1390, in HTTPConnection.getresponse(self)\n   1388 try:\n   1389     try:\n-&gt; 1390         response.begin()\n   1391     except ConnectionError:\n   1392         self.close()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:325, in HTTPResponse.begin(self)\n    323 # read until we get a non-100 response\n    324 while True:\n--&gt; 325     version, status, reason = self._read_status()\n    326     if status != CONTINUE:\n    327         break\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:286, in HTTPResponse._read_status(self)\n    285 def _read_status(self):\n--&gt; 286     line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n    287     if len(line) &gt; _MAXLINE:\n    288         raise LineTooLong(\"status line\")\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/socket.py:706, in SocketIO.readinto(self, b)\n    704 while True:\n    705     try:\n--&gt; 706         return self._sock.recv_into(b)\n    707     except timeout:\n    708         self._timeout_occurred = True\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/ssl.py:1314, in SSLSocket.recv_into(self, buffer, nbytes, flags)\n   1310     if flags != 0:\n   1311         raise ValueError(\n   1312           \"non-zero flags not allowed in calls to recv_into() on %s\" %\n   1313           self.__class__)\n-&gt; 1314     return self.read(nbytes, buffer)\n   1315 else:\n   1316     return super().recv_into(buffer, nbytes, flags)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/ssl.py:1166, in SSLSocket.read(self, len, buffer)\n   1164 try:\n   1165     if buffer is not None:\n-&gt; 1166         return self._sslobj.read(len, buffer)\n   1167     else:\n   1168         return self._sslobj.read(len)\n\nKeyboardInterrupt: \n\n\n\n\nx, y = dm.train_ds[0]\nprint(x.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x.squeeze(), cmap='gray')\n\ntorch.Size([1, 28, 28])\n\n\n\n\n\n\n\n\n\n\ntop_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[-1., -1., -1.],\n     [0., 0., 0.],\n     [1., 1., 1.]]\n)\n\nbottom_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[1., 1., 1.],\n     [0., 0., 0.],\n     [-1., -1., -1.]]\n)\n\nleft_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[-1., 0., 1.],\n     [-1., 0., 1.],\n     [-1., 0., 1.]]\n)\n\n\nmy_kernel = left_kernel\n\nc = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\nwith torch.no_grad():\n    c.weight.copy_(my_kernel)\n\nprint(x.shape)\ny = c(x)\nprint(y.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(y.squeeze().detach(), cmap='gray')\nplt.title('Convolution')\n\ndc = nn.ConvTranspose2d(1, 1, kernel_size=3, padding=1, bias=False)\n# with torch.no_grad():\n#     dc.weight.copy_(my_kernel)\n\nx_bar = dc(y)\nprint(x_bar.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x_bar.squeeze().detach(), cmap='gray')\nplt.title('Convolution transpose')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 #| notest\n----&gt; 3 my_kernel = left_kernel\n      5 c = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n      6 with torch.no_grad():\n\nNameError: name 'left_kernel' is not defined",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#conv-filters",
    "href": "models.conv.html#conv-filters",
    "title": "Convolution Neural Networks",
    "section": "",
    "text": "cfg = OmegaConf.load('../config/data/image/mnist.yaml')\ndm = instantiate(cfg)\ndm.prepare_data()\ndm.setup()\n\n[14:49:25] INFO - Init ImageDataModule for mnist\n[14:49:29] INFO - loading dataset mnist with args () from split train\n[14:49:29] INFO - loading dataset mnist from split train\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[6], line 4\n      2 cfg = OmegaConf.load('../config/data/image/mnist.yaml')\n      3 dm = instantiate(cfg)\n----&gt; 4 dm.prepare_data()\n      5 dm.setup()\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:415, in ImageDataModule.prepare_data(self)\n    412 \"\"\"Download data if needed \n    413 \"\"\"\n    414 # train set\n--&gt; 415 self.train_ds = ImageDataset(\n    416     self.hparams.name,\n    417     *self.args,\n    418     data_dir = self.hparams.data_dir,\n    419     split='train',\n    420     transforms = self.hparams.transforms,\n    421     **self.kwargs\n    422 )\n    423 # get num classes before setup method converst ImageDataset to Subset\n    424 self._num_classes = self.train_ds.num_classes\n\nFile ~/Projects/nimrod/nimrod/image/datasets.py:228, in ImageDataset.__init__(self, name, data_dir, split, transforms, streaming, exclude_grey_scale, verification_mode, from_image_folder, from_disk, *args)\n    226 else:\n    227     logger.info(f\"loading dataset {name} from split {split}\")\n--&gt; 228     self.hf_ds = load_dataset(\n    229         name,\n    230         *args,\n    231         split=split,\n    232         cache_dir=data_dir,\n    233         download_mode='reuse_dataset_if_exists',\n    234         streaming=streaming,\n    235         verification_mode=verification_mode\n    236         \n    237     )\n    239 # CHANGE IMAGE COLUMN NAME IF NEEDED\n    240 self.image_column_name = 'image'\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:2129, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\n   2124 verification_mode = VerificationMode(\n   2125     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\n   2126 )\n   2128 # Create a dataset builder\n-&gt; 2129 builder_instance = load_dataset_builder(\n   2130     path=path,\n   2131     name=name,\n   2132     data_dir=data_dir,\n   2133     data_files=data_files,\n   2134     cache_dir=cache_dir,\n   2135     features=features,\n   2136     download_config=download_config,\n   2137     download_mode=download_mode,\n   2138     revision=revision,\n   2139     token=token,\n   2140     storage_options=storage_options,\n   2141     trust_remote_code=trust_remote_code,\n   2142     _require_default_config_name=name is None,\n   2143     **config_kwargs,\n   2144 )\n   2146 # Return iterable dataset in case of streaming\n   2147 if streaming:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n   1885 # Instantiate the dataset builder\n-&gt; 1886 builder_instance: DatasetBuilder = builder_cls(\n   1887     cache_dir=cache_dir,\n   1888     dataset_name=dataset_name,\n   1889     config_name=config_name,\n   1890     data_dir=data_dir,\n   1891     data_files=data_files,\n   1892     hash=dataset_module.hash,\n   1893     info=info,\n   1894     features=features,\n   1895     token=token,\n   1896     storage_options=storage_options,\n   1897     **builder_kwargs,\n   1898     **config_kwargs,\n   1899 )\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n   1902 return builder_instance\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:342, in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    340     config_kwargs[\"data_dir\"] = data_dir\n    341 self.config_kwargs = config_kwargs\n--&gt; 342 self.config, self.config_id = self._create_builder_config(\n    343     config_name=config_name,\n    344     custom_features=features,\n    345     **config_kwargs,\n    346 )\n    348 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    349 # Prefill datasetinfo\n    350 if info is None:\n    351     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:597, in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    594     raise ValueError(f\"BuilderConfig must have a name, got {builder_config.name}\")\n    596 # resolve data files if needed\n--&gt; 597 builder_config._resolve_data_files(\n    598     base_path=self.base_path,\n    599     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    600 )\n    602 # compute the config id that is going to be used for caching\n    603 config_id = builder_config.create_config_id(\n    604     config_kwargs,\n    605     custom_features=custom_features,\n    606 )\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/builder.py:206, in BuilderConfig._resolve_data_files(self, base_path, download_config)\n    204 if isinstance(self.data_files, DataFilesPatternsDict):\n    205     base_path = xjoin(base_path, self.data_dir) if self.data_dir else base_path\n--&gt; 206     self.data_files = self.data_files.resolve(base_path, download_config)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:818, in DataFilesPatternsDict.resolve(self, base_path, download_config)\n    816 out = DataFilesDict()\n    817 for key, data_files_patterns_list in self.items():\n--&gt; 818     out[key] = data_files_patterns_list.resolve(base_path, download_config)\n    819 return out\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:771, in DataFilesPatternsList.resolve(self, base_path, download_config)\n    768 for pattern, allowed_extensions in zip(self, self.allowed_extensions):\n    769     try:\n    770         data_files.extend(\n--&gt; 771             resolve_pattern(\n    772                 pattern,\n    773                 base_path=base_path,\n    774                 allowed_extensions=allowed_extensions,\n    775                 download_config=download_config,\n    776             )\n    777         )\n    778     except FileNotFoundError:\n    779         if not has_magic(pattern):\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/datasets/data_files.py:388, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\n    383 if protocol == \"hf\" and config.HF_HUB_VERSION &gt;= version.parse(\"0.20.0\"):\n    384     # 10 times faster glob with detail=True (ignores costly info like lastCommit)\n    385     glob_kwargs[\"expand_info\"] = False\n    386 matched_paths = [\n    387     filepath if filepath.startswith(protocol_prefix) else protocol_prefix + filepath\n--&gt; 388     for filepath, info in fs.glob(pattern, detail=True, **glob_kwargs).items()\n    389     if info[\"type\"] == \"file\"\n    390     and (xbasename(filepath) not in files_to_ignore)\n    391     and not _is_inside_unrequested_special_dir(filepath, fs_pattern)\n    392     and not _is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir(filepath, fs_pattern)\n    393 ]  # ignore .ipynb and __pycache__, but keep /../\n    394 if allowed_extensions is not None:\n    395     out = [\n    396         filepath\n    397         for filepath in matched_paths\n    398         if any(\".\" + suffix in allowed_extensions for suffix in xbasename(filepath).split(\".\")[1:])\n    399     ]\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:521, in HfFileSystem.glob(self, path, **kwargs)\n    519 kwargs = {\"expand_info\": kwargs.get(\"detail\", False), **kwargs}\n    520 path = self.resolve_path(path, revision=kwargs.get(\"revision\")).unresolve()\n--&gt; 521 return super().glob(path, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:611, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs)\n    608     else:\n    609         depth = None\n--&gt; 611 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\n    613 pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\n    614 pattern = re.compile(pattern)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:556, in HfFileSystem.find(self, path, maxdepth, withdirs, detail, refresh, revision, **kwargs)\n    533 \"\"\"\n    534 List all files below path.\n    535 \n   (...)\n    553     `Union[List[str], Dict[str, Dict[str, Any]]]`: List of paths or dict of file information.\n    554 \"\"\"\n    555 if maxdepth:\n--&gt; 556     return super().find(\n    557         path, maxdepth=maxdepth, withdirs=withdirs, detail=detail, refresh=refresh, revision=revision, **kwargs\n    558     )\n    559 resolved_path = self.resolve_path(path, revision=revision)\n    560 path = resolved_path.unresolve()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/fsspec/spec.py:502, in AbstractFileSystem.find(self, path, maxdepth, withdirs, detail, **kwargs)\n    499 # Add the root directory if withdirs is requested\n    500 # This is needed for posix glob compliance\n    501 if withdirs and path != \"\" and self.isdir(path):\n--&gt; 502     out[path] = self.info(path)\n    504 for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\n    505     if withdirs:\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py:719, in HfFileSystem.info(self, path, refresh, revision, **kwargs)\n    717     out = out1[0]\n    718 if refresh or out is None or (expand_info and out and out[\"last_commit\"] is None):\n--&gt; 719     paths_info = self._api.get_paths_info(\n    720         resolved_path.repo_id,\n    721         resolved_path.path_in_repo,\n    722         expand=expand_info,\n    723         revision=resolved_path.revision,\n    724         repo_type=resolved_path.repo_type,\n    725     )\n    726     if not paths_info:\n    727         _raise_file_not_found(path, None)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114, in validate_hf_hub_args.&lt;locals&gt;._inner_fn(*args, **kwargs)\n    111 if check_use_auth_token:\n    112     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.__name__, has_token=has_token, kwargs=kwargs)\n--&gt; 114 return fn(*args, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/hf_api.py:3295, in HfApi.get_paths_info(self, repo_id, paths, expand, revision, repo_type, token)\n   3292 revision = quote(revision, safe=\"\") if revision is not None else constants.DEFAULT_REVISION\n   3293 headers = self._build_hf_headers(token=token)\n-&gt; 3295 response = get_session().post(\n   3296     f\"{self.endpoint}/api/{repo_type}s/{repo_id}/paths-info/{revision}\",\n   3297     data={\n   3298         \"paths\": paths if isinstance(paths, list) else [paths],\n   3299         \"expand\": expand,\n   3300     },\n   3301     headers=headers,\n   3302 )\n   3303 hf_raise_for_status(response)\n   3304 paths_info = response.json()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:637, in Session.post(self, url, data, json, **kwargs)\n    626 def post(self, url, data=None, json=None, **kwargs):\n    627     r\"\"\"Sends a POST request. Returns :class:`Response` object.\n    628 \n    629     :param url: URL for the new :class:`Request` object.\n   (...)\n    634     :rtype: requests.Response\n    635     \"\"\"\n--&gt; 637     return self.request(\"POST\", url, data=data, json=json, **kwargs)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:589, in Session.request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\n    584 send_kwargs = {\n    585     \"timeout\": timeout,\n    586     \"allow_redirects\": allow_redirects,\n    587 }\n    588 send_kwargs.update(settings)\n--&gt; 589 resp = self.send(prep, **send_kwargs)\n    591 return resp\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:724, in Session.send(self, request, **kwargs)\n    721 if allow_redirects:\n    722     # Redirect resolving generator.\n    723     gen = self.resolve_redirects(r, request, **kwargs)\n--&gt; 724     history = [resp for resp in gen]\n    725 else:\n    726     history = []\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:724, in &lt;listcomp&gt;(.0)\n    721 if allow_redirects:\n    722     # Redirect resolving generator.\n    723     gen = self.resolve_redirects(r, request, **kwargs)\n--&gt; 724     history = [resp for resp in gen]\n    725 else:\n    726     history = []\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:265, in SessionRedirectMixin.resolve_redirects(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\n    263     yield req\n    264 else:\n--&gt; 265     resp = self.send(\n    266         req,\n    267         stream=stream,\n    268         timeout=timeout,\n    269         verify=verify,\n    270         cert=cert,\n    271         proxies=proxies,\n    272         allow_redirects=False,\n    273         **adapter_kwargs,\n    274     )\n    276     extract_cookies_to_jar(self.cookies, prepared_request, resp.raw)\n    278     # extract redirect url, if any, for the next loop\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/sessions.py:703, in Session.send(self, request, **kwargs)\n    700 start = preferred_clock()\n    702 # Send the request\n--&gt; 703 r = adapter.send(request, **kwargs)\n    705 # Total elapsed time of the request (approximately)\n    706 elapsed = preferred_clock() - start\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:93, in UniqueRequestIdAdapter.send(self, request, *args, **kwargs)\n     91 \"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\n     92 try:\n---&gt; 93     return super().send(request, *args, **kwargs)\n     94 except requests.RequestException as e:\n     95     request_id = request.headers.get(X_AMZN_TRACE_ID)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/requests/adapters.py:667, in HTTPAdapter.send(self, request, stream, timeout, verify, cert, proxies)\n    664     timeout = TimeoutSauce(connect=timeout, read=timeout)\n    666 try:\n--&gt; 667     resp = conn.urlopen(\n    668         method=request.method,\n    669         url=url,\n    670         body=request.body,\n    671         headers=request.headers,\n    672         redirect=False,\n    673         assert_same_host=False,\n    674         preload_content=False,\n    675         decode_content=False,\n    676         retries=self.max_retries,\n    677         timeout=timeout,\n    678         chunked=chunked,\n    679     )\n    681 except (ProtocolError, OSError) as err:\n    682     raise ConnectionError(err, request=request)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connectionpool.py:787, in HTTPConnectionPool.urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\n    784 response_conn = conn if not release_conn else None\n    786 # Make the request on the HTTPConnection object\n--&gt; 787 response = self._make_request(\n    788     conn,\n    789     method,\n    790     url,\n    791     timeout=timeout_obj,\n    792     body=body,\n    793     headers=headers,\n    794     chunked=chunked,\n    795     retries=retries,\n    796     response_conn=response_conn,\n    797     preload_content=preload_content,\n    798     decode_content=decode_content,\n    799     **response_kw,\n    800 )\n    802 # Everything went great!\n    803 clean_exit = True\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connectionpool.py:534, in HTTPConnectionPool._make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    532 # Receive the response from the server\n    533 try:\n--&gt; 534     response = conn.getresponse()\n    535 except (BaseSSLError, OSError) as e:\n    536     self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/site-packages/urllib3/connection.py:516, in HTTPConnection.getresponse(self)\n    513 _shutdown = getattr(self.sock, \"shutdown\", None)\n    515 # Get the response from http.client.HTTPConnection\n--&gt; 516 httplib_response = super().getresponse()\n    518 try:\n    519     assert_header_parsing(httplib_response.msg)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:1390, in HTTPConnection.getresponse(self)\n   1388 try:\n   1389     try:\n-&gt; 1390         response.begin()\n   1391     except ConnectionError:\n   1392         self.close()\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:325, in HTTPResponse.begin(self)\n    323 # read until we get a non-100 response\n    324 while True:\n--&gt; 325     version, status, reason = self._read_status()\n    326     if status != CONTINUE:\n    327         break\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/http/client.py:286, in HTTPResponse._read_status(self)\n    285 def _read_status(self):\n--&gt; 286     line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n    287     if len(line) &gt; _MAXLINE:\n    288         raise LineTooLong(\"status line\")\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/socket.py:706, in SocketIO.readinto(self, b)\n    704 while True:\n    705     try:\n--&gt; 706         return self._sock.recv_into(b)\n    707     except timeout:\n    708         self._timeout_occurred = True\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/ssl.py:1314, in SSLSocket.recv_into(self, buffer, nbytes, flags)\n   1310     if flags != 0:\n   1311         raise ValueError(\n   1312           \"non-zero flags not allowed in calls to recv_into() on %s\" %\n   1313           self.__class__)\n-&gt; 1314     return self.read(nbytes, buffer)\n   1315 else:\n   1316     return super().recv_into(buffer, nbytes, flags)\n\nFile ~/miniconda3/envs/nimrod/lib/python3.11/ssl.py:1166, in SSLSocket.read(self, len, buffer)\n   1164 try:\n   1165     if buffer is not None:\n-&gt; 1166         return self._sslobj.read(len, buffer)\n   1167     else:\n   1168         return self._sslobj.read(len)\n\nKeyboardInterrupt: \n\n\n\n\nx, y = dm.train_ds[0]\nprint(x.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x.squeeze(), cmap='gray')\n\ntorch.Size([1, 28, 28])\n\n\n\n\n\n\n\n\n\n\ntop_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[-1., -1., -1.],\n     [0., 0., 0.],\n     [1., 1., 1.]]\n)\n\nbottom_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[1., 1., 1.],\n     [0., 0., 0.],\n     [-1., -1., -1.]]\n)\n\nleft_kernel = torch.tensor( # torch.tensor infers datatype vs. torch.Tensor\n    [[-1., 0., 1.],\n     [-1., 0., 1.],\n     [-1., 0., 1.]]\n)\n\n\nmy_kernel = left_kernel\n\nc = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\nwith torch.no_grad():\n    c.weight.copy_(my_kernel)\n\nprint(x.shape)\ny = c(x)\nprint(y.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(y.squeeze().detach(), cmap='gray')\nplt.title('Convolution')\n\ndc = nn.ConvTranspose2d(1, 1, kernel_size=3, padding=1, bias=False)\n# with torch.no_grad():\n#     dc.weight.copy_(my_kernel)\n\nx_bar = dc(y)\nprint(x_bar.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x_bar.squeeze().detach(), cmap='gray')\nplt.title('Convolution transpose')\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 3\n      1 #| notest\n----&gt; 3 my_kernel = left_kernel\n      5 c = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)\n      6 with torch.no_grad():\n\nNameError: name 'left_kernel' is not defined",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#conv-block",
    "href": "models.conv.html#conv-block",
    "title": "Convolution Neural Networks",
    "section": "Conv Block",
    "text": "Conv Block\nUsing a convolution with a stride of 2 instead of max pooling essentially achieves the same goal of downsampling an image by reducing its spatial dimensions, but with the key difference that the convolution layer can learn more complex feature combinations from overlapping regions, while max pooling only selects the maximum value within a window, potentially losing information about the finer details within that region; making the convolution with stride approach often preferred for preserving more spatial information in a neural network.\n\nsource\n\nConvBlock\n\n ConvBlock (in_channels:int=3, out_channels:int=16, kernel_size:int=3,\n            stride:int=2, bias:bool=True, normalization:Optional[Type[torc\n            h.nn.modules.module.Module]]=&lt;class\n            'torch.nn.modules.batchnorm.BatchNorm2d'&gt;, activation:Optional\n            [Type[torch.nn.modules.module.Module]]=&lt;class\n            'torch.nn.modules.activation.ReLU'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n3\ninput channels\n\n\nout_channels\nint\n16\noutput channels\n\n\nkernel_size\nint\n3\nkernel size\n\n\nstride\nint\n2\nstride\n\n\nbias\nbool\nTrue\nbias is False if BatchNorm\n\n\nnormalization\nOptional\nBatchNorm2d\nnormalization\n\n\nactivation\nOptional\nReLU\nactivation\n\n\n\n\n\nUsage\n\nB, C, H, W = 64, 1, 28, 28\nX = torch.rand(B, C, H,W)\n# stride 2 layer downsample to (W/2, H/2)\nmodel = ConvBlock(\n    in_channels=C,\n    out_channels=16,\n    kernel_size=3,\n    stride=2,\n    bias=True,\n    normalization=nn.BatchNorm2d,\n    )\n\n# get first layer of sequential and init weights\nlayer_0 = model.net[0] # get first layer of sequential\nwith torch.no_grad():\n    model.net[0].weight.copy_(top_kernel)\n\nprint(\"Y: \", model(X).shape)\n# # flatten all dims except batch dim 1\nY = torch.flatten(model(X), 1)\nprint(Y.shape)\nsummary(model, input_size=(B, C, H, W), depth=2)\n\n[13:58:07] WARNING - setting conv bias back to False as Batchnorm is used\n\n\nY:  torch.Size([64, 16, 14, 14])\ntorch.Size([64, 3136])\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nConvBlock                                [64, 16, 14, 14]          --\n├─Sequential: 1-1                        [64, 16, 14, 14]          --\n│    └─Conv2d: 2-1                       [64, 16, 14, 14]          144\n│    └─BatchNorm2d: 2-2                  [64, 16, 14, 14]          32\n│    └─ReLU: 2-3                         [64, 16, 14, 14]          --\n==========================================================================================\nTotal params: 176\nTrainable params: 176\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 1.81\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 3.21\nParams size (MB): 0.00\nEstimated Total Size (MB): 3.41\n==========================================================================================\n\n\n\nnn.Sequential(\n    ConvBlock(1, 8),\n    ConvBlock(8, 16),\n    ConvBlock(16, 32),\n    ConvBlock(32, 16)\n    )(X).shape\n\n[13:58:09] WARNING - setting conv bias back to False as Batchnorm is used\n[13:58:09] WARNING - setting conv bias back to False as Batchnorm is used\n[13:58:09] WARNING - setting conv bias back to False as Batchnorm is used\n[13:58:09] WARNING - setting conv bias back to False as Batchnorm is used\n\n\ntorch.Size([64, 16, 2, 2])\n\n\n\n\nConfigs\n\ncfg = OmegaConf.load('../config/model/image/convblock.yaml')\nnet = instantiate(cfg.defaults)\nB, C, H, W = 64, 1, 28, 28\nX = torch.rand(B, C, H,W)\nprint(summary(net))\nprint(\"Y: \",net(X).shape)\n\nSeed set to 42\n[14:49:51] WARNING - setting conv bias back to False as Batchnorm is used\n\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nConvBlock                                --\n├─Sequential: 1-1                        --\n│    └─Conv2d: 2-1                       144\n│    └─BatchNorm2d: 2-2                  32\n│    └─ReLU: 2-3                         --\n=================================================================\nTotal params: 176\nTrainable params: 176\nNon-trainable params: 0\n=================================================================\nY:  torch.Size([64, 16, 14, 14])",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#pre-activation-conv-block",
    "href": "models.conv.html#pre-activation-conv-block",
    "title": "Convolution Neural Networks",
    "section": "Pre-Activation Conv Block",
    "text": "Pre-Activation Conv Block\n\nsource\n\nPreActivationConvBlock\n\n PreActivationConvBlock (in_channels:int=3, out_channels:int=16,\n                         kernel_size:int=3, stride:int=2, bias:bool=True, \n                         normalization:Optional[Type[torch.nn.modules.modu\n                         le.Module]]=&lt;class\n                         'torch.nn.modules.batchnorm.BatchNorm2d'&gt;, activa\n                         tion:Optional[Type[torch.nn.modules.module.Module\n                         ]]=&lt;class 'torch.nn.modules.activation.ReLU'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n3\ninput channels\n\n\nout_channels\nint\n16\noutput channels\n\n\nkernel_size\nint\n3\nkernel size\n\n\nstride\nint\n2\nstride\n\n\nbias\nbool\nTrue\n\n\n\nnormalization\nOptional\nBatchNorm2d\n\n\n\nactivation\nOptional\nReLU\n\n\n\n\n\nB, C, H, W = 64, 1, 28, 28\nX = torch.rand(B, C, H,W)\n# stride 2 layer downsample to (W/2, H/2)\nmodel = PreActivationConvBlock(\n    in_channels=C,\n    out_channels=16,\n    kernel_size=3,\n    stride=2,\n    bias=True,\n    normalization=nn.BatchNorm2d,\n    )\n\n# get last layer of sequential and init weights\nlayer_0 = model.net[0] # get first layer of sequential\nwith torch.no_grad():\n    model.net[-1].weight.copy_(top_kernel)\n\nprint(\"Y: \", model(X).shape)\n# # flatten all dims except batch dim 1\nY = torch.flatten(model(X), 1)\nprint(Y.shape)\nsummary(model, input_size=(B, C, H, W), depth=2)\n\n[13:58:16] WARNING - setting conv bias back to False as Batchnorm is used\n\n\nY:  torch.Size([64, 16, 14, 14])\ntorch.Size([64, 3136])\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nPreActivationConvBlock                   [64, 16, 14, 14]          --\n├─Sequential: 1-1                        [64, 16, 14, 14]          --\n│    └─BatchNorm2d: 2-1                  [64, 1, 28, 28]           2\n│    └─ReLU: 2-2                         [64, 1, 28, 28]           --\n│    └─Conv2d: 2-3                       [64, 16, 14, 14]          144\n==========================================================================================\nTotal params: 146\nTrainable params: 146\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 1.81\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 2.01\nParams size (MB): 0.00\nEstimated Total Size (MB): 2.21\n==========================================================================================",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#deconv-block",
    "href": "models.conv.html#deconv-block",
    "title": "Convolution Neural Networks",
    "section": "Deconv Block",
    "text": "Deconv Block\n\nsource\n\nDeconvBlock\n\n DeconvBlock (in_channels:int=16, out_channels:int=3, kernel_size:int=3,\n              bias:bool=True, normalization:Optional[Type[torch.nn.modules\n              .module.Module]]=None, activation:Optional[Type[torch.nn.mod\n              ules.module.Module]]=&lt;class\n              'torch.nn.modules.activation.ReLU'&gt;, scale_factor:int=2,\n              use_transposed_conv:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n16\ninput channels\n\n\nout_channels\nint\n3\noutput channels\n\n\nkernel_size\nint\n3\nkernel size\n\n\nbias\nbool\nTrue\n\n\n\nnormalization\nOptional\nNone\n\n\n\nactivation\nOptional\nReLU\n\n\n\nscale_factor\nint\n2\n\n\n\nuse_transposed_conv\nbool\nFalse\n\n\n\n\n\n\nUsage\n\nB, C, H, W = 64, 3, 28, 28\nX = torch.rand(B, C, H, W)\ndeconv = DeconvBlock(3, 8, scale_factor=2, kernel_size=3, use_transposed_conv=True)\nprint(deconv)\nprint(\"Y: \",deconv(X).shape)\n\nDeconvBlock(\n  (_net): Sequential(\n    (0): ConvTranspose2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (1): ReLU()\n  )\n)\nY:  torch.Size([64, 8, 56, 56])",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#conv-deconv",
    "href": "models.conv.html#conv-deconv",
    "title": "Convolution Neural Networks",
    "section": "Conv-Deconv",
    "text": "Conv-Deconv\n\n# one image\nx, y = dm.train_ds[0]\nC, H, W = x.shape\n# make fake batch dimension\nx = x.unsqueeze(0)\nprint(\"x:\", x.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x.squeeze(), cmap='gray')\nplt.title(\"original image\")\n\nmy_kernel = left_kernel\n\nc = ConvBlock(1,3, kernel_size=3, stride=1)\nwith torch.no_grad():\n    c.net[0].weight.copy_(my_kernel) # set kernel weights for convlayer 0 (actual convolution2d)\n\ny = c(x)\nprint(\"y: \", y.shape)\n\nplt.figure(figsize=(3,3))\nplt.imshow(y.detach().squeeze().numpy().transpose(1, 2, 0), cmap='gray')\nplt.title(\"filtered image\")\n\n\ndc = DeconvBlock(3, 1, scale_factor=2, kernel_size=3)\nwith torch.no_grad():\n    dc._net[1].weight.copy_(my_kernel) # set kernel weights for convlayer 1 (actual convolution2d)\nx_bar = dc(y)\nprint(\"x_bar: \", x_bar.shape)\nplt.figure(figsize=(3,3))\nplt.imshow(x_bar.detach().squeeze(), cmap='gray')\nplt.title(\"Deconv image\")\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[14], line 4\n      1 #| notest\n      2 \n      3 # one image\n----&gt; 4 x, y = dm.train_ds[0]\n      5 C, H, W = x.shape\n      6 # make fake batch dimension\n\nTypeError: 'NoneType' object is not subscriptable",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#convnet",
    "href": "models.conv.html#convnet",
    "title": "Convolution Neural Networks",
    "section": "ConvNet",
    "text": "ConvNet\nSimple convolution network for image recognition\n\nsource",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#convnet-1",
    "href": "models.conv.html#convnet-1",
    "title": "Convolution Neural Networks",
    "section": "ConvNet",
    "text": "ConvNet\n\n ConvNet (n_features:List[int]=[1, 8, 16, 32, 64, 128],\n          num_classes:int=10, kernel_size:int=3, bias:bool=False,\n          normalization:torch.nn.modules.module.Module=&lt;class\n          'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n          activation:torch.nn.modules.module.Module=&lt;class\n          'torch.nn.modules.activation.ReLU'&gt;)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_features\nList\n[1, 8, 16, 32, 64, 128]\nchannel/feature expansion\n\n\nnum_classes\nint\n10\nnum_classes\n\n\nkernel_size\nint\n3\nkernel size\n\n\nbias\nbool\nFalse\nconv2d bias\n\n\nnormalization\nModule\nBatchNorm2d\nnormalization (before activation)\n\n\nactivation\nModule\nReLU\nactivation function\n\n\n\n\nUsage\n\n# data\nB, C, H, W = 64, 3, 64, 64\nX = torch.rand(B, C, H, W)\nX.shape\n\nn_features = [3, 8, 16, 32, 64, 128] #28 14 7 4 2 1\nn_features = [3, 8, 16, 32, 64, 128, 64] #64, 32, 16, 8, 4, 2, 1\nnum_classes = 20\n\nconvnet = ConvNet(\n    n_features=n_features, # channel/feature expansion\n    num_classes=num_classes, # num_classes\n    kernel_size=3, # kernel size\n    bias=False, # conv2d bias\n    normalization=nn.BatchNorm2d, # normalization (before activation)\n    activation=nn.ReLU,\n)\nout = convnet(X)\nprint(out.shape)\nprint(summary(convnet, input_size=(X.shape), depth=2))\n\ntorch.Size([64, 20])\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nConvNet                                  [64, 20]                  --\n├─Sequential: 1-1                        [64, 20]                  --\n│    └─ConvBlock: 2-1                    [64, 8, 64, 64]           232\n│    └─ConvBlock: 2-2                    [64, 16, 32, 32]          1,184\n│    └─ConvBlock: 2-3                    [64, 32, 16, 16]          4,672\n│    └─ConvBlock: 2-4                    [64, 64, 8, 8]            18,560\n│    └─ConvBlock: 2-5                    [64, 128, 4, 4]           73,984\n│    └─ConvBlock: 2-6                    [64, 64, 2, 2]            73,856\n│    └─ConvBlock: 2-7                    [64, 20, 1, 1]            11,560\n│    └─Flatten: 2-8                      [64, 20]                  --\n==========================================================================================\nTotal params: 184,048\nTrainable params: 184,048\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 378.27\n==========================================================================================\nInput size (MB): 3.15\nForward/backward pass size (MB): 65.29\nParams size (MB): 0.74\nEstimated Total Size (MB): 69.18\n==========================================================================================\n\n\n\n# from config\ncfg = OmegaConf.load('../config/model/image/convnet.yaml')\n# print(cfg.defaults)\n# convnet = instantiate(cfg.defaults)\nprint(cfg.batchnorm)\nconvnet = instantiate(cfg.baseline)\n\n# print(convnet(X).shape)\n\n{'_target_': 'nimrod.models.conv.ConvNet', 'n_features': [1, 8, 16, 32, 64, 128], 'num_classes': 10, 'kernel_size': 3, 'bias': False, 'normalization': {'_target_': 'hydra.utils.get_class', 'path': 'torch.nn.BatchNorm2d'}, 'activation': {'_target_': 'hydra.utils.get_class', 'path': 'torch.nn.ReLU'}}\n\n\n\n\nTraining\n\nDataloaders\n\n# data module config\ncfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\n\nBATCH_SIZE = 512\ndatamodule = instantiate(cfg, batch_size=BATCH_SIZE)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n# one data point \nX,y = datamodule.test_ds[0]\nprint(\"X (C,H,W): \", X.shape, \"y: \", y)\n\n# a batch of data via dataloader\nXX,YY = next(iter(datamodule.test_dataloader()))\nprint(\"XX (B,C,H,W): \", XX.shape, \"YY: \", YY.shape)\n\nprint(len(datamodule.train_ds))\nprint(len(datamodule.train_ds) // BATCH_SIZE)\n\n[16:28:58] INFO - Init ImageDataModule for fashion_mnist\n[16:29:17] INFO - split train into train/val [0.8, 0.2]\n[16:29:17] INFO - train: 48000 val: 12000, test: 10000\n\n\nX (C,H,W):  torch.Size([1, 32, 32]) y:  9\nXX (B,C,H,W):  torch.Size([512, 1, 32, 32]) YY:  torch.Size([512])\n48000\n93\n\n\n\n\nModel & hardware\n\ndevice = get_device()\nprint(device)\ncfg = OmegaConf.load('../config/model/image/convnet.yaml')\n# print(cfg.defaults)\n# convnet = instantiate(cfg.defaults)\nprint(cfg.baseline)\nconvnet = instantiate(cfg.baseline)\nmodel = convnet.to(device)\n\nsummary(model, input_size=(B, C, H, W), depth=4)\n\n[16:29:17] INFO - Using device: mps\n\n\nmps\n{'_target_': 'nimrod.models.conv.ConvNet', 'n_features': [1, 8, 16, 32, 64], 'num_classes': 10, 'kernel_size': 3, 'bias': True, 'normalization': None, 'activation': {'_target_': 'hydra.utils.get_class', 'path': 'torch.nn.ReLU'}}\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nConvNet                                  [64, 40]                  --\n├─Sequential: 1-1                        [64, 40]                  --\n│    └─ConvLayer: 2-1                    [64, 8, 28, 28]           --\n│    │    └─Sequential: 3-1              [64, 8, 28, 28]           --\n│    │    │    └─Conv2d: 4-1             [64, 8, 28, 28]           80\n│    │    │    └─ReLU: 4-2               [64, 8, 28, 28]           --\n│    └─ConvLayer: 2-2                    [64, 16, 14, 14]          --\n│    │    └─Sequential: 3-2              [64, 16, 14, 14]          --\n│    │    │    └─Conv2d: 4-3             [64, 16, 14, 14]          1,168\n│    │    │    └─ReLU: 4-4               [64, 16, 14, 14]          --\n│    └─ConvLayer: 2-3                    [64, 32, 7, 7]            --\n│    │    └─Sequential: 3-3              [64, 32, 7, 7]            --\n│    │    │    └─Conv2d: 4-5             [64, 32, 7, 7]            4,640\n│    │    │    └─ReLU: 4-6               [64, 32, 7, 7]            --\n│    └─ConvLayer: 2-4                    [64, 64, 4, 4]            --\n│    │    └─Sequential: 3-4              [64, 64, 4, 4]            --\n│    │    │    └─Conv2d: 4-7             [64, 64, 4, 4]            18,496\n│    │    │    └─ReLU: 4-8               [64, 64, 4, 4]            --\n│    └─ConvLayer: 2-5                    [64, 10, 2, 2]            --\n│    │    └─Sequential: 3-5              [64, 10, 2, 2]            --\n│    │    │    └─Conv2d: 4-9             [64, 10, 2, 2]            5,770\n│    └─Flatten: 2-6                      [64, 40]                  --\n==========================================================================================\nTotal params: 30,154\nTrainable params: 30,154\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 53.63\n==========================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 6.16\nParams size (MB): 0.12\nEstimated Total Size (MB): 6.49\n==========================================================================================\n\n\n\n\nLR finder\n\ncfg = OmegaConf.load('../config/model/image/convnet.yaml')\nmodel = instantiate(cfg.batchnorm)\nprint(summary(model, depth=4))\n\n\ncriterion = nn.CrossEntropyLoss()    \noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4) #, weight_decay=1e-5)\n    \n# Initialize LR Finder\nlr_finder = LRFinder(model, optimizer, criterion, device=device)\n    \n# Run LR range test\nlr_finder.range_test(\n    datamodule.train_dataloader(),\n    start_lr=1e-5,      # Extremely small starting learning rate\n    end_lr=10,          # Large ending learning rate\n    num_iter=100,   # Number of iterations to test\n    smooth_f=0.05,   # Smoothing factor for the loss\n    diverge_th=5, \n)\n    \n# Plot the learning rate vs loss\n_, lr_found = lr_finder.plot(log_lr=True)\nprint('Suggested lr:', lr_found)\n    \nlr_finder.reset()\n\n=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\nConvNet                                  --\n├─Sequential: 1-1                        --\n│    └─ConvLayer: 2-1                    --\n│    │    └─Sequential: 3-1              --\n│    │    │    └─Conv2d: 4-1             72\n│    │    │    └─BatchNorm2d: 4-2        16\n│    │    │    └─ReLU: 4-3               --\n│    └─ConvLayer: 2-2                    --\n│    │    └─Sequential: 3-2              --\n│    │    │    └─Conv2d: 4-4             1,152\n│    │    │    └─BatchNorm2d: 4-5        32\n│    │    │    └─ReLU: 4-6               --\n│    └─ConvLayer: 2-3                    --\n│    │    └─Sequential: 3-3              --\n│    │    │    └─Conv2d: 4-7             4,608\n│    │    │    └─BatchNorm2d: 4-8        64\n│    │    │    └─ReLU: 4-9               --\n│    └─ConvLayer: 2-4                    --\n│    │    └─Sequential: 3-4              --\n│    │    │    └─Conv2d: 4-10            18,432\n│    │    │    └─BatchNorm2d: 4-11       128\n│    │    │    └─ReLU: 4-12              --\n│    └─ConvLayer: 2-5                    --\n│    │    └─Sequential: 3-5              --\n│    │    │    └─Conv2d: 4-13            73,728\n│    │    │    └─BatchNorm2d: 4-14       256\n│    │    │    └─ReLU: 4-15              --\n│    └─ConvLayer: 2-6                    --\n│    │    └─Sequential: 3-6              --\n│    │    │    └─Conv2d: 4-16            11,530\n│    └─Flatten: 2-7                      --\n=================================================================\nTotal params: 110,018\nTrainable params: 110,018\nNon-trainable params: 0\n=================================================================\n\n\n\n\n\nStopping early, the loss has diverged\nLearning rate search finished. See the graph with {finder_name}.plot()\nLR suggestion: steepest gradient\nSuggested LR: 2.01E-03\n\n\n\n\n\n\n\n\n\nSuggested lr: 0.0020092330025650463\n\n\n\n\n1-cycle warm-up\n\ndevice = get_device()\n# data module config\ncfg_dm = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\ncfg_dm.batch_size = 512\ndatamodule = instantiate(cfg_dm)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n# device = 'cpu'\nprint(device)\ncfg_mdl = OmegaConf.load('../config/model/image/convnet.yaml')\nconvnet = instantiate(cfg_mdl.batchnorm)\nmodel = convnet.to(device)\n\nN_EPOCHS = 5\n\nlr_found = 3e-4\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nsteps_per_epoch = len(datamodule.train_ds) // cfg_dm.batch_size\ntotal_steps = steps_per_epoch* N_EPOCHS\nprint(f\"size training set: {len(datamodule.train_ds)}, bs: {cfg_dm.batch_size}, steps/epoch: {steps_per_epoch}, total steps: {total_steps}\")\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=steps_per_epochs, epochs=1)\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=lr_found,  # Peak learning rate\n        # total_steps=len(datamodule.train_ds) * N_EPOCHS,  # Total training iterations\n        steps_per_epoch=steps_per_epoch,\n        epochs=N_EPOCHS,\n        pct_start=0.3,  # 30% of training increasing LR, 70% decreasing\n        anneal_strategy='cos',  # Cosine annealing\n        div_factor=10,  # Initial lr = max_lr / div_factor\n        # final_div_factor=1e4,\n        three_phase=False  # Two phase LR schedule (increase then decrease)\n    )\n\n################################\n\n\nlrs = []\ncurrent_step = 0\ntrain_loss_history = []\neval_loss_history = []\navg_train_loss_hist = []\navg_eval_loss_hist = []\nmax_acc = 0\n\nfor epoch in range(N_EPOCHS):\n    i = 0\n    model.train()\n    for images, labels in datamodule.train_dataloader():\n        if current_step &gt;= total_steps:\n            print(f\"Reached total steps: {current_step}/{total_steps}\")\n            break\n        optimizer.zero_grad()\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        loss = criterion(outputs, labels)        \n        loss.backward()\n        optimizer.step()\n        scheduler.step()    \n        current_step += 1\n        train_loss_history.append(loss.item())\n        # current_lr = scheduler.get_last_lr()[0]\n        current_lr = optimizer.param_groups[0]['lr']\n        lrs.append(current_lr)\n        if not (i % 100):\n            print(f\"Loss {loss.item():.4f}, Current LR: {current_lr:.10f}, Step: {current_step}/{total_steps}\")\n        i += 1\n\n    model.eval()\n    with torch.no_grad():\n        correct = 0\n        total = 0\n        for images, labels in datamodule.val_dataloader():\n            # model expects input (B,H*W)\n            images = images.to(device)\n            labels = labels.to(device)\n            # Pass the input through the model\n            outputs = model(images)\n            # eval loss\n            eval_loss = criterion(outputs, labels)\n            eval_loss_history.append(eval_loss.item())\n            # Get the predicted labels\n            _, predicted = torch.max(outputs.data, 1)\n\n            # Update the total and correct counts\n            total += labels.size(0)\n            correct += (predicted == labels).sum()\n            acc = 100 * correct / total\n            if acc &gt; max_acc:\n                max_acc = acc\n\n        # Print the accuracy\n    print(f\"Epoch {epoch + 1}: Last training Loss {loss.item():.4f}, Last Eval loss {eval_loss.item():.4f} Accuracy = {100 * correct / total:.2f}% Best Accuracy: {max_acc:.2f}\")\n    # print(f'Current LR: {optimizer.param_groups[0][\"lr\"]:.5f}')\n\n###################\nplt.figure(1)\nplt.subplot(211)\nplt.ylabel('loss')\nplt.xlabel('step')\nplt.plot(train_loss_history)\nplt.plot(eval_loss_history)\nplt.subplot(212)\nplt.ylabel('lr')\nplt.xlabel('step')\nplt.plot(lrs)\n\nSeed set to 42\nSeed set to 42\n[23:31:47] INFO - Init ImageDataModule for fashion_mnist\n[23:31:52] INFO - loading dataset fashion_mnist with args () from split train\n[23:32:00] INFO - loading dataset fashion_mnist with args () from split test\n[23:32:03] INFO - split train into train/val [0.8, 0.2]\n[23:32:03] INFO - train: 48000 val: 12000, test: 10000\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 11\n      8 datamodule.setup()\n     10 # device = 'cpu'\n---&gt; 11 print(device)\n     12 cfg_mdl = OmegaConf.load('../config/model/image/convnet.yaml')\n     13 convnet = instantiate(cfg_mdl.batchnorm)\n\nNameError: name 'device' is not defined",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "models.conv.html#convnetx",
    "href": "models.conv.html#convnetx",
    "title": "Convolution Neural Networks",
    "section": "ConvNetX",
    "text": "ConvNetX\n\nsource\n\nConvNetX\n\n ConvNetX (nnet:__main__.ConvNet, num_classes:int,\n           optimizer:Callable[...,torch.optim.optimizer.Optimizer],\n           scheduler:Optional[Callable[...,Any]]=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnnet\nConvNet\n\nmodel\n\n\nnum_classes\nint\n\nnumber of classes\n\n\noptimizer\nCallable\n\noptimizer\n\n\nscheduler\nOptional\nNone\nscheduler\n\n\n\n\n\nUsage\n\ncfg = OmegaConf.load('../config/model/image/convnetx.yaml')\nfeats_dim = [3, 8, 16, 32, 64, 128, 64]\ncfg.nnet.n_features = feats_dim\ncfg.nnet.num_classes = 200\n\nmodel = instantiate(cfg.nnet)\n\n\nB, C, H, W = 64, 3, 64, 64\nX = torch.rand(B, C, H, W)\nX.shape\nprint(model(X).shape)\n\ntorch.Size([64, 200])\n\n\n\nsummary(model, input_size=(B, C, H, W), depth=2)\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nConvNet                                  [64, 200]                 --\n├─Sequential: 1-1                        [64, 200]                 --\n│    └─ConvBlock: 2-1                    [64, 8, 64, 64]           232\n│    └─ConvBlock: 2-2                    [64, 16, 32, 32]          1,184\n│    └─ConvBlock: 2-3                    [64, 32, 16, 16]          4,672\n│    └─ConvBlock: 2-4                    [64, 64, 8, 8]            18,560\n│    └─ConvBlock: 2-5                    [64, 128, 4, 4]           73,984\n│    └─ConvBlock: 2-6                    [64, 64, 2, 2]            73,856\n│    └─ConvBlock: 2-7                    [64, 200, 1, 1]           115,600\n│    └─Flatten: 2-8                      [64, 200]                 --\n==========================================================================================\nTotal params: 288,088\nTrainable params: 288,088\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 384.93\n==========================================================================================\nInput size (MB): 3.15\nForward/backward pass size (MB): 65.48\nParams size (MB): 1.15\nEstimated Total Size (MB): 69.78\n==========================================================================================\n\n\n\n\nNimrod training\n\nN_EPOCHS = 5\n\n# data module config\ncfg = OmegaConf.load('../config/data/image/fashion_mnist.yaml')\ncfg.batch_size = 512\ncfg.num_workers = 0\ndatamodule = instantiate(cfg)\ndatamodule.prepare_data()\ndatamodule.setup()\n\n[20:23:50] INFO - Init ImageDataModule for fashion_mnist\n[20:24:08] INFO - split train into train/val [0.8, 0.2]\n[20:24:08] INFO - train: 48000 val: 12000, test: 10000\n\n\n\ncfg = OmegaConf.load('../config/optimizer/adam_w.yaml')\noptimizer = instantiate(cfg)\n\ncfg = OmegaConf.load('../config/scheduler/step_lr.yaml')\nscheduler = instantiate(cfg)\n\ncfg = OmegaConf.load('../config/model/image/convnetx.yaml')\nmodel = instantiate(cfg)(optimizer=optimizer, scheduler=scheduler)\n\n# # with 1-cycle sched\n# cfg.nnet.n_features = [1, 8, 16, 32, 64, 128]\n# cfg.scheduler.total_steps = len(datamodule.train_ds) * N_EPOCHS\n# model = instantiate(cfg)\n\n[14:53:05] INFO - ConvNetX: init\n[14:53:05] INFO - Classifier: init\n/user/s/slegroux/miniconda3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n\n\n\ntrainer = Trainer(\n    accelerator=\"auto\",\n    max_epochs=N_EPOCHS,\n    logger=TensorBoardLogger(\"tb_logs\", name=\"fashion_mnist_convnet\", default_hp_metric=True),\n    # logger=CSVLogger(\"logs\", name=\"mnist_convnet\"),\n    callbacks = [LearningRateMonitor(logging_interval=\"step\")],\n    check_val_every_n_epoch=1,\n    log_every_n_steps=1\n    )\n\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\n\n\nLR finder\n\ntuner = Tuner(trainer)\nlr_finder = tuner.lr_find(\n    model,\n    datamodule=datamodule,\n    min_lr=1e-5,\n    max_lr=1.0,\n    num_training=100,  # number of iterations\n    # attr_name=\"optimizer.lr\",\n)\nfig = lr_finder.plot(suggest=True)\nplt.show()\nprint(f\"Suggested learning rate: {lr_finder.suggestion()}\")\n\n[20:59:14] INFO - Optimizer: &lt;class 'torch.optim.adamw.AdamW'&gt;\n[20:59:14] INFO - Scheduler: &lt;torch.optim.lr_scheduler.StepLR object&gt;\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n`Trainer.fit` stopped: `max_steps=100` reached.\nLearning rate set to 0.0019952623149688807\nRestoring states from the checkpoint path at /Users/slegroux/Projects/nimrod/nbs/.lr_find_61a6646e-2298-4940-9b72-9185e67e8d21.ckpt\nRestored all states from the checkpoint at /Users/slegroux/Projects/nimrod/nbs/.lr_find_61a6646e-2298-4940-9b72-9185e67e8d21.ckpt\n\n\n\n\n\n\n\n\n\nSuggested learning rate: 0.0019952623149688807\n\n\n\nprint(trainer.max_epochs, len(datamodule.train_ds), datamodule.hparams.batch_size)\nprint(5*56000)\nprint(5*56000/2048)\nprint(5*56000//2048)\n\n10 48000 512\n280000\n136.71875\n136\n\n\n\n\n1-cycle scheduling\n\nN_EPOCHS = 1\n# lr_found = lr_finder.suggestion()\nlr_found = 3e-4\n\n# DATA\ncfg = OmegaConf.load('../config/data/image/mnist.yaml')\ncfg.batch_size = 512\ncfg.num_workers = 0\ndatamodule = instantiate(cfg)\ndatamodule.prepare_data()\ndatamodule.setup()\n\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val/loss',  # Metric to monitor\n    dirpath='checkpoints/',  # Directory to save checkpoints\n    filename='epoch{epoch:02d}-val_loss{val/loss:.2f}',\n    auto_insert_metric_name=False,\n    save_top_k=1,  # Save only the best checkpoint\n    mode='min'  # Mode can be 'min' or 'max' depending on the metric\n)\n\nlr_monitor = LearningRateMonitor(logging_interval=\"step\")\n\n# TRAINER \ntrainer = Trainer(\n    accelerator=\"auto\",\n    max_epochs=N_EPOCHS,\n    # logger=TensorBoardLogger(\"tb_logs\", name=\"mnist_convnet\", default_hp_metric=True),\n    logger=CSVLogger(\"logs\", name=\"fashion_mnist_convnet\"),\n    callbacks = [lr_monitor, checkpoint_callback],\n    check_val_every_n_epoch=1,\n    log_every_n_steps=1\n    )\n\nprint(\"estimated steps: \", trainer.estimated_stepping_batches, \"accumulate_grad_batches: \", trainer.accumulate_grad_batches)\n\n# MODEL\nmodel_cfg = OmegaConf.load('../config/model/image/convnetx.yaml')\nmodel_cfg.scheduler.total_steps = trainer.max_epochs * len(datamodule.train_dataloader())\nmodel_cfg.scheduler.max_lr = lr_found#lr_finder.suggestion()\n\nmodel = instantiate(model_cfg)\n\nprint(\"LR: \",model.lr)\ntrainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n\n########################\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head()\n\n##########################\nplt.figure()\nplt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.figure()\nplt.plot(metrics['step'], metrics['lr-AdamW'], 'g.-')\nplt.show()\n\n[23:33:22] INFO - Init ImageDataModule for mnist\n[23:33:26] INFO - loading dataset mnist with args () from split train\n[23:33:33] INFO - loading dataset mnist with args () from split test\n[23:33:36] INFO - split train into train/val [0.8, 0.2]\n[23:33:36] INFO - train: 48000 val: 12000, test: 10000\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nLoading `train_dataloader` to estimate number of stepping batches.\n\n\nestimated steps:  -1 accumulate_grad_batches:  1\n\n\n\n---------------------------------------------------------------------------\nConfigAttributeError                      Traceback (most recent call last)\nCell In[10], line 41\n     39 # MODEL\n     40 model_cfg = OmegaConf.load('../config/model/image/convnetx.yaml')\n---&gt; 41 model_cfg.scheduler.total_steps = trainer.max_epochs * len(datamodule.train_dataloader())\n     42 model_cfg.scheduler.max_lr = lr_found#lr_finder.suggestion()\n     44 model = instantiate(model_cfg)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/dictconfig.py:355, in DictConfig.__getattr__(self, key)\n    351     return self._get_impl(\n    352         key=key, default_value=_DEFAULT_MARKER_, validate_key=False\n    353     )\n    354 except ConfigKeyError as e:\n--&gt; 355     self._format_and_raise(\n    356         key=key, value=None, cause=e, type_override=ConfigAttributeError\n    357     )\n    358 except Exception as e:\n    359     self._format_and_raise(key=key, value=None, cause=e)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/base.py:231, in Node._format_and_raise(self, key, value, cause, msg, type_override)\n    223 def _format_and_raise(\n    224     self,\n    225     key: Any,\n   (...)\n    229     type_override: Any = None,\n    230 ) -&gt; None:\n--&gt; 231     format_and_raise(\n    232         node=self,\n    233         key=key,\n    234         value=value,\n    235         msg=str(cause) if msg is None else msg,\n    236         cause=cause,\n    237         type_override=type_override,\n    238     )\n    239     assert False\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/_utils.py:899, in format_and_raise(node, key, value, msg, cause, type_override)\n    896     ex.ref_type = ref_type\n    897     ex.ref_type_str = ref_type_str\n--&gt; 899 _raise(ex, cause)\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/_utils.py:797, in _raise(ex, cause)\n    795 else:\n    796     ex.__cause__ = None\n--&gt; 797 raise ex.with_traceback(sys.exc_info()[2])\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/dictconfig.py:351, in DictConfig.__getattr__(self, key)\n    348     raise AttributeError()\n    350 try:\n--&gt; 351     return self._get_impl(\n    352         key=key, default_value=_DEFAULT_MARKER_, validate_key=False\n    353     )\n    354 except ConfigKeyError as e:\n    355     self._format_and_raise(\n    356         key=key, value=None, cause=e, type_override=ConfigAttributeError\n    357     )\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/dictconfig.py:442, in DictConfig._get_impl(self, key, default_value, validate_key)\n    438 def _get_impl(\n    439     self, key: DictKeyType, default_value: Any, validate_key: bool = True\n    440 ) -&gt; Any:\n    441     try:\n--&gt; 442         node = self._get_child(\n    443             key=key, throw_on_missing_key=True, validate_key=validate_key\n    444         )\n    445     except (ConfigAttributeError, ConfigKeyError):\n    446         if default_value is not _DEFAULT_MARKER_:\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/basecontainer.py:73, in BaseContainer._get_child(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\n     64 def _get_child(\n     65     self,\n     66     key: Any,\n   (...)\n     70     throw_on_missing_key: bool = False,\n     71 ) -&gt; Union[Optional[Node], List[Optional[Node]]]:\n     72     \"\"\"Like _get_node, passing through to the nearest concrete Node.\"\"\"\n---&gt; 73     child = self._get_node(\n     74         key=key,\n     75         validate_access=validate_access,\n     76         validate_key=validate_key,\n     77         throw_on_missing_value=throw_on_missing_value,\n     78         throw_on_missing_key=throw_on_missing_key,\n     79     )\n     80     if isinstance(child, UnionNode) and not _is_special(child):\n     81         value = child._value()\n\nFile ~/miniforge3/envs/nimrod/lib/python3.11/site-packages/omegaconf/dictconfig.py:480, in DictConfig._get_node(self, key, validate_access, validate_key, throw_on_missing_value, throw_on_missing_key)\n    478 if value is None:\n    479     if throw_on_missing_key:\n--&gt; 480         raise ConfigKeyError(f\"Missing key {key!s}\")\n    481 elif throw_on_missing_value and value._is_missing():\n    482     raise MissingMandatoryValue(\"Missing mandatory value: $KEY\")\n\nConfigAttributeError: Missing key scheduler\n    full_key: scheduler\n    object_type=dict\n\n\n\n\ntrainer.test(model, datamodule.test_dataloader(), ckpt_path=\"best\")\n\nRestoring states from the checkpoint path at /Users/slegroux/Projects/nimrod/nbs/checkpoints/epoch00-val_loss0.13.ckpt\nLoaded model weights from the checkpoint at /Users/slegroux/Projects/nimrod/nbs/checkpoints/epoch00-val_loss0.13.ckpt\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │    0.9707000255584717     │\n│         test/loss         │    0.11260439455509186    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\n[{'test/loss': 0.11260439455509186, 'test/acc': 0.9707000255584717}]\n\n\n\nbest_checkpoint_path = checkpoint_callback.best_model_path\nprint(f\"Best checkpoint path: {best_checkpoint_path}\")\n\nBest checkpoint path: /Users/slegroux/Projects/nimrod/nbs/checkpoints/epoch00-val_loss0.13.ckpt\n\n\n\n\nResume training\n\ncfg = OmegaConf.load('../config/scheduler/reduce_lr_on_plateau.yaml')\nsched = instantiate(cfg)\n# sched.total_steps = len(datamodule.train_ds) * N_EPOCHS\nlr = trainer.optimizers[0].param_groups[0]['lr']\nprint(f\"LR: {lr}\")\nmodel = ConvNetX.load_from_checkpoint(best_checkpoint_path,scheduler=sched, lr=lr)\n\npprint(model.hparams)\n\n[20:56:41] INFO - ConvNetX: init\n[20:56:41] INFO - Classifier: init\n\n\nLR: 7.642883799445691e-07\n\"nnet\":        ConvNet(\n  (net): Sequential(\n    (0): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n    )\n    (1): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n    )\n    (2): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n    )\n    (3): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n    )\n    (4): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n      )\n    )\n    (5): ConvLayer(\n      (net): Sequential(\n        (0): Conv2d(128, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      )\n    )\n    (6): Flatten(start_dim=1, end_dim=-1)\n  )\n)\n\"num_classes\": 10\n\"optimizer\":   functools.partial(&lt;class 'torch.optim.adamw.AdamW'&gt;, lr=0.0001, weight_decay=1e-05)\n\"scheduler\":   functools.partial(&lt;class 'torch.optim.lr_scheduler.ReduceLROnPlateau'&gt;, mode='min', factor=0.1, patience=10)\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'nnet' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['nnet'])`.\n\n\n\n# batchnorm should allow us to try higher LR\n# model_cfg = OmegaConf.load('../config/model/image/convnetx_adam.yaml')\n# model_cfg.optimizer.lr = 0.1\n\n# model = instantiate(model_cfg)\n# opt = instantiate(model_cfg.optimizer)\n# print(opt)\n# sched = instantiate(model_cfg.scheduler)\n# print(sched)\n\n\nN_EPOCHS = 3\n\ntrainer = Trainer(\n    accelerator=\"auto\",\n    max_epochs=N_EPOCHS,\n    # logger=TensorBoardLogger(\"tb_logs\", name=\"mnist_convnet\", default_hp_metric=True),\n    logger=CSVLogger(\"logs\", name=\"fashion_mnist_convnet\"),\n    callbacks = [LearningRateMonitor(logging_interval=\"step\")],\n    check_val_every_n_epoch=1,\n    log_every_n_steps=1\n    )\n\n# use standar adam scheduler\n\n# retrieve last ckpt\ntrainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader(), ckpt_path=best_checkpoint_path)\n\nGPU available: True (mps), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\nRestoring states from the checkpoint path at /Users/slegroux/Projects/nimrod/nbs/checkpoints/epoch00-val_loss0.13.ckpt\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:273: Be aware that when using `ckpt_path`, callbacks used to create the checkpoint need to be provided during `Trainer` instantiation. Please add the following callbacks: [\"ModelCheckpoint{'monitor': 'val/loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\"].\n[20:56:49] INFO - Optimizer: AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 1e-05\n)\n[20:56:49] INFO - Scheduler: &lt;torch.optim.lr_scheduler.ReduceLROnPlateau object&gt;\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:316: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n\n  | Name         | Type               | Params | Mode \n------------------------------------------------------------\n0 | loss         | CrossEntropyLoss   | 0      | train\n1 | train_acc    | MulticlassAccuracy | 0      | train\n2 | val_acc      | MulticlassAccuracy | 0      | train\n3 | test_acc     | MulticlassAccuracy | 0      | train\n4 | train_loss   | MeanMetric         | 0      | train\n5 | val_loss     | MeanMetric         | 0      | train\n6 | test_loss    | MeanMetric         | 0      | train\n7 | val_acc_best | MaxMetric          | 0      | train\n8 | nnet         | ConvNet            | 110 K  | train\n------------------------------------------------------------\n110 K     Trainable params\n0         Non-trainable params\n110 K     Total params\n0.440     Total estimated model params size (MB)\n39        Modules in train mode\n0         Modules in eval mode\nRestored all states from the checkpoint at /Users/slegroux/Projects/nimrod/nbs/checkpoints/epoch00-val_loss0.13.ckpt\n\n\n\n\n\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n\n\n\n[20:57:08] INFO - scheduler is an instance of Reduce plateau\n\n\n\n\n\n[20:57:26] INFO - scheduler is an instance of Reduce plateau\n`Trainer.fit` stopped: `max_epochs=3` reached.\n\n\n\ncsv_path = f\"{trainer.logger.log_dir}/metrics.csv\"\nmetrics = pd.read_csv(csv_path)\nmetrics.head()\nplt.figure()\nplt.plot(metrics['step'], metrics['train/loss_step'], 'b.-')\nplt.plot(metrics['step'], metrics['val/loss'],'r.-')\nplt.figure()\nplt.plot(metrics['step'], metrics['lr-AdamW'], 'g.-')\nplt.show()\ntrainer.test(model, datamodule.test_dataloader(), ckpt_path=\"best\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRestoring states from the checkpoint path at logs/fashion_mnist_convnet/version_14/checkpoints/epoch=2-step=282.ckpt\nLoaded model weights from the checkpoint at logs/fashion_mnist_convnet/version_14/checkpoints/epoch=2-step=282.ckpt\n/Users/slegroux/miniforge3/envs/nimrod/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test/acc          │    0.9711999893188477     │\n│         test/loss         │    0.11154787242412567    │\n└───────────────────────────┴───────────────────────────┘",
    "crumbs": [
      "Image",
      "Models",
      "Convolution Neural Networks"
    ]
  },
  {
    "objectID": "image.vit.html",
    "href": "image.vit.html",
    "title": "Vit",
    "section": "",
    "text": "source\n\n\n\n Mlp (in_features, hidden_features=None, out_features=None,\n      act_layer=&lt;class 'torch.nn.modules.activation.GELU'&gt;, drop=0.0)\n\nMLP as used in Vision Transformer, MLP-Mixer and related networks\n\nsource\n\n\n\n\n Attention (dim, num_heads=8, qkv_bias=False, qk_scale=None,\n            attn_drop=0.0, proj_drop=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Block (dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None,\n        drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=&lt;class\n        'torch.nn.modules.activation.GELU'&gt;, norm_layer=&lt;class\n        'torch.nn.modules.normalization.LayerNorm'&gt;,\n        use_grad_checkpointing=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n interpolate_pos_embed (pos_embed_checkpoint, visual_encoder)\n\n\nsource\n\n\n\n\n VisionTransformer (img_size=224, patch_size=16, in_chans=3,\n                    num_classes=1000, embed_dim=768, depth=12,\n                    num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n                    qk_scale=None, representation_size=None,\n                    drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0,\n                    norm_layer=None, use_grad_checkpointing=False,\n                    ckpt_layer=0)\n\nVision Transformer A PyTorch impl of : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929"
  },
  {
    "objectID": "image.vit.html#modules",
    "href": "image.vit.html#modules",
    "title": "Vit",
    "section": "",
    "text": "source\n\n\n\n Mlp (in_features, hidden_features=None, out_features=None,\n      act_layer=&lt;class 'torch.nn.modules.activation.GELU'&gt;, drop=0.0)\n\nMLP as used in Vision Transformer, MLP-Mixer and related networks\n\nsource\n\n\n\n\n Attention (dim, num_heads=8, qkv_bias=False, qk_scale=None,\n            attn_drop=0.0, proj_drop=0.0)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Block (dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None,\n        drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=&lt;class\n        'torch.nn.modules.activation.GELU'&gt;, norm_layer=&lt;class\n        'torch.nn.modules.normalization.LayerNorm'&gt;,\n        use_grad_checkpointing=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n interpolate_pos_embed (pos_embed_checkpoint, visual_encoder)\n\n\nsource\n\n\n\n\n VisionTransformer (img_size=224, patch_size=16, in_chans=3,\n                    num_classes=1000, embed_dim=768, depth=12,\n                    num_heads=12, mlp_ratio=4.0, qkv_bias=True,\n                    qk_scale=None, representation_size=None,\n                    drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0,\n                    norm_layer=None, use_grad_checkpointing=False,\n                    ckpt_layer=0)\n\nVision Transformer A PyTorch impl of : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929"
  },
  {
    "objectID": "audio.datasets.stt.html#librispeech-datamodule",
    "href": "audio.datasets.stt.html#librispeech-datamodule",
    "title": "Speech to Text Datasets",
    "section": "LibriSpeech DataModule",
    "text": "LibriSpeech DataModule",
    "crumbs": [
      "Audio",
      "Data",
      "Speech to Text Datasets"
    ]
  },
  {
    "objectID": "audio.datasets.stt.html#usage",
    "href": "audio.datasets.stt.html#usage",
    "title": "Speech to Text Datasets",
    "section": "Usage",
    "text": "Usage\n\ndm = LibriSpeechDataModule(\n    target_dir=\"../data/en\", \n    dataset_parts=\"mini_librispeech\",\n    output_dir=\"../data/en/LibriSpeech\",\n    num_jobs=1\n)\n\n\n# skip this at export time to not waste time\n# download\ndm.prepare_data()\n\n\n\n\n\n# libri = prepare_librispeech(\"../data/en/LibriSpeech\", dataset_parts='mini_librispeech')\n\n\n#! rm ../data/en/LibriSpeech/*.gz\n\n\ndm.setup(stage='test')\ndm.cuts_test\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[16], line 2\n      1 #| notest\n----&gt; 2 dm.setup(stage='test')\n      3 dm.cuts_test\n\nCell In[3], line 25, in LibriSpeechDataModule.setup(self, stage)\n     23     self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))\n     24 if stage == \"test\":\n---&gt; 25     self.cuts_test = CutSet.from_manifests(**self.libri[\"dev-clean-2\"])\n     26     self.tokenizer = TokenCollater(self.cuts_test)\n     27     self.tokenizer(self.cuts_test.subset(first=2))\n\nKeyError: 'dev-clean-2'\n\n\n\n\nrecs = RecordingSet.from_file(\"../data/en/LibriSpeech/librispeech_recordings_dev-clean-2.jsonl.gz\")\nsup = SupervisionSet(\"../data/en/LibriSpeech/librispeech_supervisions_dev-clean-2.jsonl.gz\")\nprint(len(recs),len(sup))\n\n1089 68\n\n\n\ntest_dl = dm.test_dataloader()\nb = next(iter(test_dl))\nprint(b[\"feats_pad\"].shape, b[\"tokens_pad\"].shape, b[\"ilens\"].shape)\nplt.imshow(b[\"feats_pad\"][0].transpose(0,1), origin='lower')\n\n# dm.tokenizer.idx2token(b[\"tokens_pad\"][0])\n# dm.tokenizer.inverse(b[\"tokens_pad\"][0], b[\"ilens\"][0])\n\ntorch.Size([39, 1014, 80]) torch.Size([39, 178]) torch.Size([39])\n\n\n\n\n\n\n\n\n\n\nprint(dm.cuts_test)\ncut = dm.cuts_test[0]\n# pprint(cut.to_dict())\ncut.plot_audio()\n\nCutSet(len=1089) [underlying data type: &lt;class 'list'&gt;]",
    "crumbs": [
      "Audio",
      "Data",
      "Speech to Text Datasets"
    ]
  },
  {
    "objectID": "models.attention.html",
    "href": "models.attention.html",
    "title": "Attention",
    "section": "",
    "text": "B, C, H, W = 64, 3, 16, 16\nx = torch.randn(B, C, H, W)\n\n\nt = x.view(B, C, -1).transpose(1, 2) # B, T, C\nt.shape\n\ntorch.Size([64, 256, 3])\n\n\n\nk_projection = nn.Linear(C, C)\nq_projection = nn.Linear(C, C)\nv_projection = nn.Linear(C, C)\n\n\nk = k_projection(t)\nq = q_projection(t)\nv = v_projection(t)\nprint(k.shape)\n\ntorch.Size([64, 256, 3])\n\n\n\n(q@k.transpose(1,2)).shape\n\ntorch.Size([64, 256, 256])\n\n\n\nprint(k.shape)\nk.transpose(1,0).shape\n\ntorch.Size([64, 256, 3])\n\n\ntorch.Size([256, 64, 3])\n\n\n\nsource\n\n\n\n SelfAttention (n_features:int)\n\nSelf-Attention layer for image data.\n\n\n\n\nType\nDetails\n\n\n\n\nn_features\nint\nnumber of features\n\n\n\n\nB, C, H, W = 64, 3, 16, 16\nx = torch.randn(B, C, H, W)\nsa = SelfAttention(3)\nsa(x).shape\n\ntorch.Size([64, 3, 16, 16])\n\n\n\nsource\n\n\n\n\n MultiheadAttention (n_features:int, n_heads:int)\n\nMultihead Attention module.\n\n\n\n\nType\nDetails\n\n\n\n\nn_features\nint\nnumber of features\n\n\nn_heads\nint\nnumber of heads or groups of features\n\n\n\n\nB, C, H, W = 64, 32, 16, 16\nx = torch.randn(B, C, H, W)\nmha = MultiheadAttention(32, 8)\nmha(x).shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nnm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)"
  },
  {
    "objectID": "models.attention.html#self-attention",
    "href": "models.attention.html#self-attention",
    "title": "Attention",
    "section": "",
    "text": "B, C, H, W = 64, 3, 16, 16\nx = torch.randn(B, C, H, W)\n\n\nt = x.view(B, C, -1).transpose(1, 2) # B, T, C\nt.shape\n\ntorch.Size([64, 256, 3])\n\n\n\nk_projection = nn.Linear(C, C)\nq_projection = nn.Linear(C, C)\nv_projection = nn.Linear(C, C)\n\n\nk = k_projection(t)\nq = q_projection(t)\nv = v_projection(t)\nprint(k.shape)\n\ntorch.Size([64, 256, 3])\n\n\n\n(q@k.transpose(1,2)).shape\n\ntorch.Size([64, 256, 256])\n\n\n\nprint(k.shape)\nk.transpose(1,0).shape\n\ntorch.Size([64, 256, 3])\n\n\ntorch.Size([256, 64, 3])\n\n\n\nsource\n\n\n\n SelfAttention (n_features:int)\n\nSelf-Attention layer for image data.\n\n\n\n\nType\nDetails\n\n\n\n\nn_features\nint\nnumber of features\n\n\n\n\nB, C, H, W = 64, 3, 16, 16\nx = torch.randn(B, C, H, W)\nsa = SelfAttention(3)\nsa(x).shape\n\ntorch.Size([64, 3, 16, 16])\n\n\n\nsource\n\n\n\n\n MultiheadAttention (n_features:int, n_heads:int)\n\nMultihead Attention module.\n\n\n\n\nType\nDetails\n\n\n\n\nn_features\nint\nnumber of features\n\n\nn_heads\nint\nnumber of heads or groups of features\n\n\n\n\nB, C, H, W = 64, 32, 16, 16\nx = torch.randn(B, C, H, W)\nmha = MultiheadAttention(32, 8)\nmha(x).shape\n\ntorch.Size([64, 32, 16, 16])\n\n\n\nnm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)"
  },
  {
    "objectID": "text.embeddings.html",
    "href": "text.embeddings.html",
    "title": "Text embeddings",
    "section": "",
    "text": "https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30 https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n\nsource\n\n\n\n Decoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Encoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)"
  },
  {
    "objectID": "text.embeddings.html#word2vec",
    "href": "text.embeddings.html#word2vec",
    "title": "Text embeddings",
    "section": "",
    "text": "https://towardsdatascience.com/word2vec-research-paper-explained-205cb7eecc30 https://muhark.github.io/python/ml/nlp/2021/10/21/word2vec-from-scratch.html\n\nsource\n\n\n\n Decoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nsource\n\n\n\n\n Encoder ()\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\nenc = Encoder()\nbatch = torch.rand((10, 28*28))\nencoded = enc(batch)\nprint(encoded.shape)\n\n\ndec = Decoder()\ndecoded = dec(encoded)\nprint(decoded.shape)"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n get_device ()\n\n\n\n\n\ndevice = get_device()\nprint(device)\nprint(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n\nmps\nIs MPS (Metal Performance Shader) built? True",
    "crumbs": [
      "Misc",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#device",
    "href": "utils.html#device",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n get_device ()\n\n\n\n\n\ndevice = get_device()\nprint(device)\nprint(f\"Is MPS (Metal Performance Shader) built? {torch.backends.mps.is_built()}\")\n\nmps\nIs MPS (Metal Performance Shader) built? True",
    "crumbs": [
      "Misc",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#seeding",
    "href": "utils.html#seeding",
    "title": "Utils",
    "section": "Seeding",
    "text": "Seeding\n\nsource\n\nset_seed\n\n set_seed (seed:int=42)\n\n\n\nUsage\n\nset_seed()\n\nSeed set to 42",
    "crumbs": [
      "Misc",
      "Utils"
    ]
  },
  {
    "objectID": "utils.html#timing",
    "href": "utils.html#timing",
    "title": "Utils",
    "section": "Timing",
    "text": "Timing\n\nsource\n\ntime_it\n\n time_it (func)\n\n\n\nUsage\n\n@time_it\ndef hello(name):\n    \"\"\"Says hello to someone\"\"\"\n    sleep(0.001)\n    return f\"Hello {name}!\"\n\nprint(hello.__name__)  # Prints: \"hello\"\nprint(hello.__doc__)   # Prints: \"Says hello to someone\"\nprint(hello('sylain'))\n\nhello\nSays hello to someone\nhello: 0.105 seconds\nHello sylain!",
    "crumbs": [
      "Misc",
      "Utils"
    ]
  }
]