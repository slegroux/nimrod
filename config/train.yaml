# @package _global_

defaults:
  - _self_
  - data: image/mnist
  - model: image/mlp
  - callbacks: default
  - logger: default # set logger here or use command line (e.g. `python train.py logger=tensorboard`)
  - paths: default
  - extras: default
  - hydra: default
  - trainer: default
  - experiment: null
  - profiler: default

project: "Template"
# ckpt_path: ${paths.log_dir}/${run_id}/checkpoints/last.ckpt # ckpt to resume training from
ckpt_path: null
tags: ['mnist', 'mlp', 'dev']
task_name: 'mnist-mlp'
seed: 42
tune_batch_size: False
tune_lr: False
train: True
test: True



# # HYPERPARAMETER OPTIMIZATION
# optimized_metric: "val/loss"
# defaults:
#   - override hydra/sweeper: optuna

# hydra:
#   defaults:
#     - override hydra/hydra_logging: colorlog
#     - override hydra/job_logging: colorlog
#   run:
#     dir: ${paths.log_dir}/${run_id}
#   sweep:
#     dir: ${paths.log_dir}/multiruns/${run_id}
#     subdir: ${hydra.job.num}
#   sweeper:
#     _target_: hydra_plugins.hydra_optuna_sweeper.optuna_sweeper.OptunaSweeper
#     direction: minimize
#     n_trials: 20
#     n_jobs: 0
#     storage: null
#     study_name: ${project}
#     sampler:
#       _target_: optuna.samplers.TPESampler
#       seed: ${seed}
#       n_startup_trials: 10 # number of random sampling runs before optimization starts
#       consider_prior: true
#       prior_weight: 1.0
#       consider_magic_clip: true
#       consider_endpoints: false
#       n_ei_candidates: 24
#       multivariate: false
#       warn_independent_sampling: true
#     params:
#       model.lr: tag(log, interval(1e-4, 1e-1))
#       # model.lr: interval(1e-3, 1e-2)
#       # model.dropout: interval(0.15, 0.5)
#       # model.n_h: choice(128, 256, 512)
#       datamodule.batch_size: choice(64, 128)
