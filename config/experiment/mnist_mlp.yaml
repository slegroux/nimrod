# @package _global_

# python train.py experiment=mnist_mlp

defaults:
  - override /data: image/mnist
  - override /model: image/mlp
  # - override /callbacks: default
  # - override /trainer: default
  - override /logger: wandb


tags: ["mnist", "mlp", "dev"]
project: "mnist-mlp"
seed: 42
tune_lr: true
test: true

data:
  batch_size: 1024
  num_workers: 0
  pin_memory: true

trainer:
  max_epochs: 5
  check_val_every_n_epoch: 1
  log_every_n_steps: 1
  
logger:
  wandb:
    tags: ${tags}
    group: "mnist"
    project: ${project}
    name: tune_lr_one_cycle #bs:${data.batch_size}-lr:${model.optimizer.lr}