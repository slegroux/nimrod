"""Neural net model"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.unet.ipynb.

# %% auto 0
__all__ = ['logger', 'init_weights', 'zero_weights', 'TinyUnet']

# %% ../../nbs/models.unet.ipynb 4
import torch.nn as nn
import torch.nn.functional as F
import torch
from torch_lr_finder import LRFinder

from omegaconf import OmegaConf
from hydra.utils import instantiate

from matplotlib import pyplot as plt

from .conv import ConvLayer
from .resnet import ResBlock
from .superres import UpBlock
from ..utils import get_device, set_seed

from functools import partial

from typing import List
import logging

# %% ../../nbs/models.unet.ipynb 5
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.unet.ipynb 9
def init_weights(m, leaky=0.):
    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)

def zero_weights(layer):
    with torch.no_grad():
        layer.weight.zero_()
        if hasattr(layer, 'bias') and hasattr(layer.bias, 'zero_'): layer.bias.zero_()

class TinyUnet(nn.Module):
    def __init__(
        self,
        n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024], # Number of features in each layer
        activation=partial(nn.LeakyReLU, negative_slope=0.1), # Activation function
        leaky:float=0.1,# Leaky ReLU negative slope
        normalization=nn.BatchNorm2d # Normalization function
    ):
        super().__init__()

        # first layer
        self.start = ResBlock(n_features[0], n_features[1], kernel_size=3, stride=1, activation=activation, normalization=normalization)
        self.encoder = nn.ModuleList()
        # encoder downsample receptive field
        down = partial(ResBlock, kernel_size=3,  stride=2, activation=activation, normalization=normalization)
        for i in range(1, len(n_features) - 1):
            self.encoder += [down(n_features[i], n_features[i+1])]

        # decoder upsampling receptive field
        up = partial(UpBlock, kernel_size=3, activation=activation, normalization=normalization)
        self.decoder = nn.ModuleList()
        for i in range(len(n_features) - 1, 1, -1):
            self.decoder += [up(n_features[i], n_features[i-1])]
        self.decoder += [up(n_features[1], n_features[0])]
        self.end = ResBlock(n_features[0], n_features[0], kernel_size=3, stride=2, activation=nn.Identity, normalization=normalization)

    def forward(self, x:torch.Tensor)->torch.Tensor:
        layers = [] # store the output of each layer
        layers.append(x)
        x = self.start(x)
        for layer in self.encoder:
            layers.append(x)
            x = layer(x)
        n = len(layers)
        for i, layer in enumerate(self.decoder):
            if i != 0:
                x += layers[n-i]
            x = layer(x)
        return self.end(x+layers[0])
        
