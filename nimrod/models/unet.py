"""Neural net model"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.unet.ipynb.

# %% auto 0
__all__ = ['logger', 'init_weights', 'zero_weights', 'TinyUnet', 'TinyUnetX']

# %% ../../nbs/models.unet.ipynb 3
import torch.nn as nn
import torch.nn.functional as F
import torch
from torch_lr_finder import LRFinder

from omegaconf import OmegaConf
from hydra.utils import instantiate

from matplotlib import pyplot as plt

from .conv import ConvBlock, DeconvBlock
from .resnet import ResBlock
from .core import Regressor
from ..utils import get_device, set_seed

from functools import partial

from typing import List, Optional, Callable, Any
import logging

# %% ../../nbs/models.unet.ipynb 4
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.unet.ipynb 6
def init_weights(m, leaky=0.):
    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)

def zero_weights(layer):
    with torch.no_grad():
        layer.weight.zero_()
        if hasattr(layer, 'bias') and hasattr(layer.bias, 'zero_'): layer.bias.zero_()

class TinyUnet(nn.Module):
    def __init__(
        self,
        n_features:List[int]=[3, 32, 64, 128, 256, 512, 1024], # Number of features in each layer
        activation=partial(nn.LeakyReLU, negative_slope=0.1), # Activation function
        leaky:float=0.1,# Leaky ReLU negative slope
        normalization=nn.BatchNorm2d # Normalization function
    ):
        super().__init__()
        if len(n_features) < 3:
            raise ValueError("n_features must be at least 3")
        # first layer
        self.start = ResBlock(n_features[0], n_features[1], kernel_size=3, stride=1, activation=activation, normalization=normalization)
        self.encoder = nn.ModuleList()
        # encoder downsample receptive field
        down = partial(ResBlock, kernel_size=3,  stride=2, activation=activation, normalization=normalization)
        for i in range(1, len(n_features)-1):
            self.encoder.append(down(n_features[i], n_features[i+1]))

        # decoder upsampling receptive field
        up = partial(DeconvBlock, kernel_size=3, activation=activation, normalization=normalization)
        self.decoder = nn.ModuleList()
        for i in range(len(n_features)-1, 1, -1):
            self.decoder.append(up(n_features[i], n_features[i-1]))
        self.decoder += [down(n_features[1], n_features[0], stride=1)]
        self.end = ResBlock(n_features[0], n_features[0], kernel_size=3, stride=1, activation=nn.Identity, normalization=normalization)

    def forward(self, x:torch.Tensor)->torch.Tensor:
        layers = [] # store the output of each layer
        layers.append(x)
        x = self.start(x)
        for layer in self.encoder:
            layers.append(x)
            x = layer(x)
        n = len(layers)
        for i, layer in enumerate(self.decoder):
            if i != 0:
                x += layers[n-i]
            x = layer(x)
        return self.end(x+layers[0])
        

# %% ../../nbs/models.unet.ipynb 8
class TinyUnetX(Regressor):
    def __init__(
        self,
        nnet:TinyUnet, # super res autoencoder neural net
        optimizer: Callable[...,torch.optim.Optimizer], # optimizer partial
        scheduler: Optional[Callable[...,Any]]=None, # scheduler partial
    ):
        logger.info("SuperResAutoencoderX: init")
        super().__init__(
            nnet=nnet,
            optimizer=optimizer,
            scheduler=scheduler
            )
        self.nnet = nnet
        self.register_module('nnet', self.nnet)
