# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.lm.ipynb.

# %% auto 0
__all__ = ['Vocab', 'NNLMConfig', 'NNLM']

# %% ../../nbs/models.lm.ipynb 3
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

from torchtext.vocab import vocab

from matplotlib import pyplot as plt
import pandas as pd
import numpy as np

from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, OrderedDict
from dataclasses import dataclass, asdict

from plum import dispatch

# %% ../../nbs/models.lm.ipynb 5
class Vocab:
    def __init__(self,
                 data:List[List[str]], # one line per sentence. each line is a list of tokens
                 specials=['<pad>', '<unk>', '<bos>', '<eos>'] # special characters
                 ):
        # count individual tokens
        c = Counter()
        for row in data:
            for token in row:
                c.update(token)
        ordered_tuple = sorted(c.items(), key=lambda x:x[1], reverse=True)
        dict = OrderedDict(ordered_tuple)        
        # leverage torchtext vocab
        self.voc = vocab(dict, specials=specials)
        if '<unk>' in specials:
            self.voc.set_default_index(self.voc['<unk>'])
        else:
            self.voc.set_default_index(-1)
        self._stoi = self.voc.get_stoi()
        self._itos = self.voc.get_itos()

    @dispatch
    def stoi(self, token:str)->int:
        if len(token) > 1 and token not in ['<pad>', '<unk>', '<bos>', '<eos>']:
            raise ValueError("input should be a token or list of tokens")
        return self._stoi[token]

    @dispatch
    def stoi(self, tokens:List[str])->List[int]:
        return [self._stoi[tok] for tok in tokens]
    
    # @dispatch #TODO
    # def stoi(self, tokens:List[List[str]])->List[List[int]]:
    #     return [self._stoi[u] for tok in tokens for ]
    # TODO:
    # support torch tensors

    @dispatch    
    def itos(self, index:int)->str:
        return self._itos[index]
    
    @dispatch    
    def itos(self, indices:List[int])->List[str]:
        return [self._itos[index] for index in indices]
        
    def __len__(self):
        return len(self.voc)
    
    @property
    def vocabulary(self)->Set:
        return sorted(set([k for k,v in self._stoi.items()]))
    

# %% ../../nbs/models.lm.ipynb 16
@dataclass
class NNLMConfig:
    n_vocab:int = 30
    n_emb:int = 10
    n_context:int = 3
    n_h:int = 100

class NNLM(nn.Module):
    def __init__(self, n_vocab:int, n_emb:int, n_context:int, n_h:int):
        super().__init__()
        self.embedder = nn.Embedding(n_vocab, n_emb) # (B,T)->(B,T,C)
        self.n_emb = n_emb
        self.n_context = n_context
        # we concatenate input of n_context length:
        self.l1 = nn.Linear(n_emb*n_context, n_h)
        self.l2 = nn.Linear(n_h, n_vocab)
    
    def forward(self, x:torch.Tensor)->torch.Tensor:
        # input: (B,T)
        embedding = self.embedder(x) # ->(B,T,C)
        h = self.l1(embedding.view(-1, self.n_emb*self.n_context))
        h = torch.tanh(h)
        logits = self.l2(h)
        return(logits)
    
    def sample(self, n_iterations:int=10, eos:int=3, pad:int=0, bos:int=2)->str:
        res = []
        for _ in range(n_iterations):
            out = [] # current sequence prediction
            context = [pad] * (self.n_context-1) + [bos]
            while True:
                logits = self(torch.tensor([context]))
                probs = F.softmax(logits, dim=1)
                ix = torch.multinomial(probs, num_samples=1).item()
                context = context[1:] + [ix]
                if ix == eos:
                    break
                else:
                    out.append(ix)
            res.append(out)
        return(res)
