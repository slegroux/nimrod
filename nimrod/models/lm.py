# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.lm.ipynb.

# %% auto 0
__all__ = ['logger', 'NNLMConfig', 'NNLM', 'NNLM_X', 'NNBigram']

# %% ../../nbs/models.lm.ipynb 4
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.optim import SGD
from torch.utils.data import DataLoader

import lightning as L
from lightning import Trainer, LightningModule
from lightning.pytorch.tuner.tuning import Tuner
from lightning.pytorch.loggers import CSVLogger

from matplotlib import pyplot as plt
# plt.set_loglevel('INFO')
import pandas as pd
from tqdm import tqdm
import pprint

from omegaconf import OmegaConf
from hydra.utils import instantiate

from typing import List
from dataclasses import dataclass, asdict

from ..text.datasets import CharDataset, Vocab
from ..utils import set_seed, get_device
from .core import Classifier

import logging
logger = logging.getLogger(__name__)


# %% ../../nbs/models.lm.ipynb 17
@dataclass
class NNLMConfig:
    n_vocab:int = 30
    n_emb:int = 10
    n_context:int = 3
    n_h:int = 100

class NNLM(nn.Module):
    def __init__(self,
                n_vocab:int = 30, # vocabulary size 
                n_emb:int = 10, # embedding dimension
                n_context:int = 3, # context size bigram/trigram, etc.
                n_h:int = 100 # hidden layer size
                ):

        logger.info(f"NNLM: Init")
        super().__init__()
        # to each token id from n_vocab in sequence T coresponds a embedding of size n_emb (C)
        self.embedder = nn.Embedding(n_vocab, n_emb) # (B,T)->(B,T,C)
        self.n_emb = n_emb
        self.n_context = n_context
        # we concatenate input of [n_context length, n_emb] into linear layer (T*C):
        self.l1 = nn.Linear(n_context * n_emb, n_h) 
        self.l2 = nn.Linear(n_h, n_vocab)
    
    def forward(self, x:torch.Tensor)->torch.Tensor:
        # input: (B,T)
        embedding = self.embedder(x) # ->(B,T,C)
        # we concatenate input of n_context length * n_emb (T*C) into linear layer:
        h = self.l1(embedding.view(-1,self.n_context * self.n_emb))
        h = torch.tanh(h)
        logits = self.l2(h)
        return(logits)

    @torch.no_grad()
    def sample(self, prompt:str, vocab:Vocab, max_new_tokens:int=50, temperature:float=1.0):

        for _ in range(max_new_tokens):            
            # limit prompt to context size
            context = prompt[-self.n_context:]
            context = vocab.stoi(list(context))

            logits = self(torch.tensor(context))
            logits = logits / temperature
            probs = F.softmax(logits, dim=1)
            ix = torch.multinomial(probs, num_samples=1).item()
            prompt += vocab.itos(ix)
        return(prompt)

# %% ../../nbs/models.lm.ipynb 43
class NNLM_X(Classifier, LightningModule):
    def __init__(
            self,
            nnet: NNLM,
            num_classes:int,
            optimizer: torch.optim.Optimizer,
            scheduler: torch.optim.lr_scheduler,
            ):

        logger.info("NNLM_X: Init")
        super().__init__(
            num_classes,
            optimizer,
            scheduler,
            )
        self.save_hyperparameters(logger=False)
        # required attribute for lr finder
        self.lr = optimizer.keywords['lr']
        self.nnet = nnet
    
    def forward(self, x:torch.Tensor)->torch.Tensor:
        return self.nnet(x)
    
    def _step(self, batch, batch_idx):
        x, y = batch
        y = y[:, -1]
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y = y[:, -1]  # Get the last token as target
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
    
    def sample(self, prompt:str, vocab:Vocab, max_new_tokens:int=50, temperature:float=1.0):
        return self.nnet.sample(prompt, vocab, max_new_tokens, temperature)

# %% ../../nbs/models.lm.ipynb 68
class NNBigram(nn.Module):
    def __init__(self, vocab_size:int) -> None:
        super().__init__()
        self._vocab_size = vocab_size
        self.emb = nn.Embedding(vocab_size, vocab_size)

    def forward(self, x:torch.tensor) -> torch.tensor:
        logits = self.emb(x) # B,T,C
        return logits
    
    def predict(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            logits = self(idx)
            logits = logits[:,-1,:] # last time step
            probs = F.softmax(logits, dim=-1) #(B,C)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx
    
    @property
    def vocab_size(self)->int:
        return self._vocab_size

