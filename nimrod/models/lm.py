# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.lm.ipynb.

# %% auto 0
__all__ = ['Vocab', 'NNLMConfig', 'NNLM']

# %% ../../nbs/models.lm.ipynb 3
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

from torchtext.vocab import vocab

from matplotlib import pyplot as plt
import pandas as pd
import numpy as np

from typing import Dict, List, Tuple, Optional, Set
from collections import Counter, OrderedDict
from dataclasses import dataclass, asdict

from plum import dispatch

# %% ../../nbs/models.lm.ipynb 5
class Vocab:
    def __init__(self, data:pd.Series, specials=['<pad>', '<unk>', '<bos>', '<eos>']):
        c = Counter()
        for row in data.items():
            name = list(row[1])
            c.update(name)

        ordered_tuple = sorted(c.items(), key=lambda x:x[1], reverse=True)
        dict = OrderedDict(ordered_tuple)        
        self.voc = vocab(dict, specials=specials)
        if '<unk>' in specials:
            self.voc.set_default_index(self.voc['<unk>'])
        else:
            self.voc.set_default_index(-1)
        self._stoi = self.voc.get_stoi()
        self._itos = self.voc.get_itos()

    @dispatch
    def stoi(self, token:str)->int:
        return self._stoi[token]

    @dispatch
    def stoi(self, tokens:List[str])->List[int]:
        return [self._stoi[tok] for tok in tokens]
    
    # @dispatch #TODO
    # def stoi(self, tokens:List[List[str]])->List[List[int]]:
    #     return [self._stoi[u] for tok in tokens for ]

    @dispatch    
    def itos(self, index:int)->str:
        return self._itos[index]

    @dispatch    
    def itos(self, indices:List[int])->List[str]:
        return [self._itos[index] for index in indices]
    
    def __len__(self):
        return len(self.voc)
    


# %% ../../nbs/models.lm.ipynb 15
@dataclass
class NNLMConfig:
    n_vocab:int = 30
    n_emb:int = 10
    n_context:int = 3
    n_h:int = 100


class NNLM(nn.Module):
    def __init__(self, n_vocab:int, n_emb:int, n_context:int, n_h:int):
        super().__init__()
        self.embedder = nn.Embedding(n_vocab, n_emb)
        self.n_emb = n_emb
        self.n_context = n_context
        self.l1 = nn.Linear(n_emb*n_context, n_h)
        self.l2 = nn.Linear(n_h, n_vocab)
    
    def forward(self, x:torch.Tensor)->torch.Tensor:
        # in: (n_sample, n_context)
        embedding = self.embedder(x) #(n_sample, n_context, n_embed)
        h = self.l1(embedding.view(-1, self.n_emb*self.n_context))
        h = torch.tanh(h)
        logits = self.l2(h)
        return(logits)
    
    def sample(self, n_iterations=10)->str:
        for _ in range(n_iterations):
            out = []
            context = [0] * self.n_context
            while True:
                logits = self(torch.tensor([context]))
                probs = F.softmax(logits, dim=1)
                ix = torch.multinomial(probs, num_samples=1).item()
                context = context[1:] + [ix]
                out.append(ix)
                if ix == 0:
                    break
        return(out)

