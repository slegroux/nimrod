"""Neural net modules"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.superres.ipynb.

# %% auto 0
__all__ = ['logger', 'device', 'UpBlock', 'init_weights', 'SuperResAutoencoder', 'SuperResAutoencoderX']

# %% ../../nbs/models.superres.ipynb 3
import torch.nn as nn
import torch

from .resnet import ResBlock
from .core import Regressor
from ..utils import get_device, set_seed

from rich import print
from typing import Optional, Type, List, Callable, Any

from functools import partial
import logging

set_seed(42)
logger = logging.getLogger(__name__)
device = get_device()

# %% ../../nbs/models.superres.ipynb 5
class UpBlock(nn.Module):
    def __init__(
        self,
        in_channels:int, # Number of input channels
        out_channels:int, # Number of output channels
        kernel_size:int=3, # Kernel size
        activation:Optional[Type[nn.Module]]=nn.ReLU, # Activation function
        normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d # Normalization function
    ):
        super().__init__()
        layers = []
        # upsample receptive field
        layers.append(nn.UpsamplingNearest2d(scale_factor=2))
        # resnet block increase channels
        layers.append(ResBlock(in_channels, out_channels, kernel_size=kernel_size, activation=activation, normalization=normalization))
        self.nnet = nn.Sequential(*layers)

    def forward(self, x):
        return self.nnet(x)

# %% ../../nbs/models.superres.ipynb 7
def init_weights(m, leaky=0.):
    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d)): nn.init.kaiming_normal_(m.weight, a=leaky)

class SuperResAutoencoder(nn.Module):
    def __init__(
        self,
        n_features:List[int]=[3, 8, 16, 32, 64, 128], # Number of features in each layer
        activation=partial(nn.LeakyReLU, negative_slope=0.1), # Activation function
        leaky:float=0.1,# Leaky ReLU negative slope
        normalization=nn.BatchNorm2d # Normalization function
    ):
        super().__init__()

        down = partial(ResBlock, kernel_size=3,  stride=2, activation=activation, normalization=normalization)
        # first layer
        enc  =  [down(n_features[0], n_features[1], kernel_size=5, stride=1)]
        # encoder downsample receptive field
        for i in range(1, len(n_features) - 1):
            enc += [down(n_features[i], n_features[i+1])]

        # decoder upsampling receptive field
        up = partial(UpBlock, kernel_size=3, activation=activation, normalization=normalization)
        dec = []
        for i in range(len(n_features) - 1, 1, -1):
            dec += [up(n_features[i], n_features[i-1])]
        dec += [up(n_features[1], n_features[0])]
        dec += [down(n_features[0], n_features[0], activation=nn.Identity)]

        self.autoencoder = nn.Sequential(*enc, *dec).apply(partial(init_weights, leaky=leaky))

    def forward(self, x:torch.Tensor)->torch.Tensor:
        return self.autoencoder(x)
        

# %% ../../nbs/models.superres.ipynb 10
class SuperResAutoencoderX(Regressor):
    def __init__(
        self,
        nnet:SuperResAutoencoder, # super res autoencoder neural net
        optimizer: Callable[...,torch.optim.Optimizer], # optimizer partial
        scheduler: Optional[Callable[...,Any]]=None, # scheduler partial
    ):
        logger.info("SuperResAutoencoderX: init")
        super().__init__(
            nnet=nnet,
            optimizer=optimizer,
            scheduler=scheduler
            )
        self.nnet = nnet
        self.register_module('nnet', self.nnet)
