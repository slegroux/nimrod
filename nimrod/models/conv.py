# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.conv.ipynb.

# %% auto 0
__all__ = ['logger', 'ConvLayer', 'PreActivationConvLayer', 'DeconvLayer', 'ConvNet', 'ConvNetX']

# %% ../../nbs/models.conv.ipynb 3
import torch.nn as nn
import torch

from lightning import Trainer
from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger
from lightning.pytorch.tuner.tuning import Tuner
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint

from torch_lr_finder import LRFinder
from torchinfo import summary

from hydra.utils import instantiate
from omegaconf import OmegaConf

from matplotlib import pyplot as plt
import pandas as pd
from typing import List, Optional, Type, Callable, Any
from functools import partial

from ..utils import get_device, set_seed
from .core import Classifier

from pprint import pprint
import logging


# %% ../../nbs/models.conv.ipynb 4
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.conv.ipynb 11
class ConvLayer(nn.Module):
  
    def __init__(
        self,
        in_channels:int=3, # input channels
        out_channels:int=16, # output channels
        kernel_size:int=3, # kernel size
        stride:int=2, # stride
        bias:bool=True,
        normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d,
        activation:Optional[Type[nn.Module]]=nn.ReLU,
        # padding=kernel_size//2
        
    ):

        super().__init__()
        
        if bias and normalization and issubclass(normalization, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
            logger.warning('setting conv bias back to False as Batchnorm is used')
            # https://x.com/karpathy/status/1013245864570073090
            bias = False

        # use stride 2 for downsampling to (W/2, H/2) instead of max or average pooling with stride 1
        conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=kernel_size//2,
            bias=bias
            )
        layers = [conv]
        if normalization:
            if issubclass(normalization,  (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                layers.append(normalization(out_channels))
        if activation:
            layers.append(activation())
        self.net = nn.Sequential(*layers)

    def forward(
            self,
            x:torch.Tensor # input image tensor of dimension (B, C, W, H)
            ) -> torch.Tensor: # output image tensor of dimension (B, C, W/2, H/2)
        
        """forward method of the ConvLayer"""
        return self.net(x)


# %% ../../nbs/models.conv.ipynb 20
class PreActivationConvLayer(nn.Module):
  
    def __init__(
        self,
        in_channels:int=3, # input channels
        out_channels:int=16, # output channels
        kernel_size:int=3, # kernel size
        stride:int=2, # stride
        bias:bool=True,
        normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d,
        activation:Optional[Type[nn.Module]]=nn.ReLU,
        # padding=kernel_size//2
        
    ):

        super().__init__()
        
        if bias and normalization and issubclass(normalization, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
            logger.warning('setting conv bias back to False as Batchnorm is used')
            # https://x.com/karpathy/status/1013245864570073090
            bias = False

        # use stride 2 for downsampling to (W/2, H/2) instead of max or average pooling with stride 1
        conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=kernel_size//2,
            bias=bias
            )
        layers = []
        if normalization:
            if issubclass(normalization,  (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                layers.append(normalization(in_channels))
        if activation:
            layers.append(activation())
        layers.append(conv)
        self.net = nn.Sequential(*layers)

    def forward(
            self,
            x:torch.Tensor # input image tensor of dimension (B, C, W, H)
            ) -> torch.Tensor: # output image tensor of dimension (B, C, W/2, H/2)
        
        """forward method of the ConvLayer"""
        return self.net(x)

    

# %% ../../nbs/models.conv.ipynb 23
class DeconvLayer(nn.Module):
    def __init__(
        self,
        in_channels:int=16, # input channels
        out_channels:int=3, # output channels
        kernel_size:int=3, # kernel size
        bias:bool=True,
        normalization:Optional[Type[nn.Module]]=None,
        activation:Optional[Type[nn.Module]]=nn.ReLU,
        stride:int = 1,
        scale_factor:int = 2
    ):
        super().__init__()
        layers = []
        if normalization:
            if issubclass(normalization,  (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
                logger.warning('setting conv bias to False as Batchnorm is used')
                # https://x.com/karpathy/status/1013245864570073090
                bias = None

        layers.append(nn.UpsamplingNearest2d(scale_factor=scale_factor))

        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2, bias=bias))
        if normalization:
            layers.append(normalization(out_channels))
        if activation:
            layers.append(activation())
        self._net = nn.Sequential(*layers)

    def forward(self, x:torch.Tensor # input image tensor of dimension (B, C, W, H)
                ) -> torch.Tensor: # output image tensor of dimension (B, C, W*2, H*2)
        return self._net(x) 

# %% ../../nbs/models.conv.ipynb 30
class ConvNet(nn.Module):

    def __init__(
            self,
            n_features:List[int]=[1, 8, 16, 32, 64, 128], # channel/feature expansion
            num_classes:int=10, # num_classes
            kernel_size:int=3, # kernel size
            bias:bool=False, # conv2d bias
            normalization:nn.Module=nn.BatchNorm2d, # normalization (before activation)
            activation:nn.Module=nn.ReLU, # activation function
        ):

        super().__init__()

        net = []
        conv_ = partial(ConvLayer, kernel_size=kernel_size, bias=bias, normalization=normalization, activation=activation)
        # first layer stride 1 to be able to go deeper while keeping the same spatial resolution
        conv_1 = conv_(in_channels=n_features[0], out_channels=n_features[1], stride=1)
        # conv_1 = ConvLayer(
        #     in_channels=n_features[0],
        #     out_channels=n_features[1],
        #     stride=1,
        #     kernel_size=kernel_size,
        #     bias=bias,
        #     normalization=normalization,
        #     activation=activation
        # )
        net.append(conv_1)

        for i in range(1, len(n_features) - 1):

            # conv = ConvLayer(
            #         in_channels=n_features[i],
            #         out_channels=n_features[i+1],
            #         kernel_size=kernel_size,
            #         bias=bias,
            #         normalization=normalization,
            #         activation=activation
            # )
            net.append(conv_(in_channels=n_features[i], out_channels=n_features[i+1], stride=2))
        
        # last layer -> flatten
        net.append(
            conv_(in_channels=n_features[-1], out_channels=num_classes, stride=2)
            # ConvLayer(
            #     in_channels=n_features[-1],
            #     out_channels=num_classes,
            #     kernel_size=kernel_size,
            #     bias=True,
            #     normalization=None,
            #     activation=None
            #     )
            )
        net.append(nn.Flatten(start_dim=1, end_dim=-1))

        self.net = nn.Sequential(*net)


    def forward(
        self,
        x:torch.Tensor # input image tensor of dimension (B, C, W, H)
        ) -> torch.Tensor: # output probs (B, N_classes)
        return self.net(x)

# %% ../../nbs/models.conv.ipynb 45
class ConvNetX(Classifier):
    """
    Parameters
    ----------
    nnet : ConvNet
        The neural network model.
    num_classes : int
        The number of classes.
    optimizer : callable
        The optimizer.
    scheduler : callable
        The learning rate scheduler.
    """

    def __init__(
            self,
            nnet:ConvNet, # model
            num_classes:int, # number of classes
            optimizer:Callable[...,torch.optim.Optimizer], # optimizer
            scheduler: Optional[Callable[...,Any]]=None, # scheduler
            ):

        logger.info("ConvNetX: init")
        super().__init__(
            nnet=nnet,
            num_classes=num_classes,
            optimizer=optimizer,
            scheduler=scheduler
            )
        
    
    def _step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
