# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.conv.ipynb.

# %% auto 0
__all__ = ['logger', 'ConvLayer', 'DeconvLayer', 'ConvNet', 'ConvNetX']

# %% ../../nbs/models.conv.ipynb 3
import torch.nn as nn
import torch

from lightning import LightningModule, Trainer
from lightning.pytorch.loggers import CSVLogger, TensorBoardLogger
from lightning.pytorch.tuner.tuning import Tuner
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint

from torch_lr_finder import LRFinder
from torchinfo import summary

from hydra.utils import instantiate
from omegaconf import OmegaConf

from matplotlib import pyplot as plt
import pandas as pd
from typing import List, Optional, Type, Callable, Any

from ..utils import get_device, set_seed
from .core import Classifier

from pprint import pprint
import logging


# %% ../../nbs/models.conv.ipynb 4
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.conv.ipynb 11
class ConvLayer(nn.Module):
    """A 2D convolutional layer with optional batch normalization and activation.

    This layer performs 2D convolution with stride 2 for downsampling, optionally followed by
    batch normalization and activation.

    Parameters
    ----------
    in_channels : int, default=3
        Number of input channels
    out_channels : int, default=16 
        Number of output channels / number of features
    kernel_size : int, default=3
        Size of the convolving kernel
    bias : bool, default=True
        If True, adds a learnable bias to the convolution
    normalization : nn.Module, default=nn.BatchNorm2d
        Normalization layer to use after convolution
    activation : nn.Module, default=nn.ReLU
        Activation function to use after normalization

    Notes
    -----
    When using batch normalization, the convolution bias is automatically disabled
    since it would be redundant.

    The spatial dimensions are reduced by half due to stride=2 convolution:
    output_size = input_size/2
    """
  
    def __init__(
        self,
        in_channels:int=3, # input channels
        out_channels:int=16, # output channels
        kernel_size:int=3, # kernel size
        stride:int=2, # stride
        bias:bool=True,
        normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d,
        activation:Optional[Type[nn.Module]]=nn.ReLU,
        
    ):

        super().__init__()
        
        if bias and normalization and issubclass(normalization, (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
            logger.warning('setting conv bias to False as Batchnorm is used')
            # https://x.com/karpathy/status/1013245864570073090
            bias = None

        # use stride 2 for downsampling to (W/2, H/2) instead of max or average pooling with stride 1
        conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=kernel_size//2,
            bias=bias
            )
        layers = [conv]
        if normalization:
            if issubclass(normalization,  (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                layers.append(normalization(out_channels))
        if activation:
            layers.append(activation())
        self.net = nn.Sequential(*layers)

    def forward(
            self,
            x:torch.Tensor # input image tensor of dimension (B, C, W, H)
            ) -> torch.Tensor: # output image tensor of dimension (B, C, W/2, H/2)
        
        """forward method of the ConvLayer"""
        return self.net(x)


# %% ../../nbs/models.conv.ipynb 21
class DeconvLayer(nn.Module):
    def __init__(
        self,
        in_channels:int=16, # input channels
        out_channels:int=3, # output channels
        kernel_size:int=3, # kernel size
        bias:bool=True,
        normalization:Optional[Type[nn.Module]]=None,
        activation:Optional[Type[nn.Module]]=nn.ReLU,
        stride:int = 1,
        scale_factor:int = 2
    ):
        super().__init__()
        layers = []
        if normalization:
            if issubclass(normalization,  (nn.BatchNorm1d,nn.BatchNorm2d,nn.BatchNorm3d)):
                logger.warning('setting conv bias to False as Batchnorm is used')
                # https://x.com/karpathy/status/1013245864570073090
                bias = None

        layers.append(nn.UpsamplingNearest2d(scale_factor=scale_factor))

        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size//2, bias=bias))
        if normalization:
            layers.append(normalization(out_channels))
        if activation:
            layers.append(activation())
        self._net = nn.Sequential(*layers)

    def forward(self, x:torch.Tensor # input image tensor of dimension (B, C, W, H)
                ) -> torch.Tensor: # output image tensor of dimension (B, C, W*2, H*2)
        return self._net(x) 

# %% ../../nbs/models.conv.ipynb 28
class ConvNet(nn.Module):
    """
    ConvNet: a simple convolutional neural network

    Parameters
    ----------
    n_features : List[int]
        channel/feature expansion. The length of the list will determine the
        number of convolutional layers. The first element is the number of
        channels in the input image, and the last element is the number of
        output channels (i.e. the number of classes).
    num_classes : int
        number of classes
    kernel_size : int
        kernel size
    bias : bool
        conv2d bias
    normalization : nn.Module
        normalization (before activation)
    activation : nn.Module
        activation function

    Notes
    -----
    The number of convolutional layers is determined by the length of the list
    `n_features`.

    Examples
    --------
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64])
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64], num_classes=10)
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64], kernel_size=5)
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64], bias=True)
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64], normalization=None)
    >>> model = ConvNet(n_features=[1, 8, 16, 32, 64], activation=None)
    """


    def __init__(
            self,
            n_features:List[int]=[1, 8, 16, 32, 64], # channel/feature expansion
            num_classes:int=10, # num_classes
            kernel_size:int=3, # kernel size
            bias:bool=False, # conv2d bias
            normalization:nn.Module=nn.BatchNorm2d, # normalization (before activation)
            activation:nn.Module=nn.ReLU, # activation function
        ):

        super().__init__()

        net = []
        # TODO:first layer stride 1 ?
        conv_stride_1 = ConvLayer(
            in_channels=n_features[0],
            out_channels=n_features[1],
            stride=2,
            kernel_size=kernel_size,
            bias=bias,
            normalization=normalization,
            activation=activation
        )
        net.append(conv_stride_1)

        for i in range(1, len(n_features) - 1):
            conv = ConvLayer(
                    in_channels=n_features[i],
                    out_channels=n_features[i+1],
                    kernel_size=kernel_size,
                    bias=bias,
                    normalization=normalization,
                    activation=activation
            )
            net.append(conv)
        
        # last layer -> flatten
        net.append(
            ConvLayer(
                in_channels=n_features[-1],
                out_channels=num_classes,
                kernel_size=kernel_size,
                bias=True,
                normalization=None,
                activation=None
                )
            )
        net.append(nn.Flatten(start_dim=1, end_dim=-1))

        self.net = nn.Sequential(*net)


    def forward(
        self,
        x:torch.Tensor # input image tensor of dimension (B, C, W, H)
        ) -> torch.Tensor: # output probs (B, N_classes)
        return self.net(x)

# %% ../../nbs/models.conv.ipynb 43
class ConvNetX(Classifier):
    """
    Parameters
    ----------
    nnet : ConvNet
        The neural network model.
    num_classes : int
        The number of classes.
    optimizer : callable
        The optimizer.
    scheduler : callable
        The learning rate scheduler.
    """

    def __init__(
            self,
            nnet:ConvNet, # model
            num_classes:int, # number of classes
            optimizer:Callable[...,torch.optim.Optimizer], # optimizer
            scheduler: Optional[Callable[...,Any]]=None, # scheduler
            ):

        logger.info("ConvNetX: init")
        super().__init__(
            nnet=nnet,
            num_classes=num_classes,
            optimizer=optimizer,
            scheduler=scheduler
            )
        
    
    def _step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
