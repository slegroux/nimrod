"""Simple feedforward Multilayer perceptron models"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.mlp.ipynb.

# %% auto 0
__all__ = ['logger', 'MLP', 'MLP_X']

# %% ../../nbs/models.mlp.ipynb 3
import torch.nn as nn
import torch

from lightning import LightningModule, Trainer
from lightning.pytorch.loggers import CSVLogger

from hydra.utils import instantiate
from omegaconf import OmegaConf
from matplotlib import pyplot as plt
import pandas as pd

from ..utils import get_device
from .core import Classifier
# torch.set_num_interop_threads(1)
# from IPython.core.debugger import set_trace

import logging
logger = logging.getLogger(__name__)


# %% ../../nbs/models.mlp.ipynb 6
class MLP(nn.Module):
    def __init__(
                self,
                n_in:int=784, # input dimension e.g. (H,W) for image
                n_h:int=64, # hidden dimension
                n_out:int=10, # output dimension (= number of classes for classification)
                dropout:float=0.2
                ) -> None:
        logger.info("MLP: init")
        super().__init__()
        l1 = nn.Linear(n_in, n_h)
        dropout = nn.Dropout(dropout)
        relu = nn.ReLU()
        l2 = nn.Linear(n_h, n_out)
        self.layers = nn.Sequential(l1, dropout, relu, l2)
        
    def forward(self, x: torch.Tensor # dim (B, H*W)
                ) -> torch.Tensor:
        return self.layers(x)

# %% ../../nbs/models.mlp.ipynb 17
class MLP_X(Classifier, LightningModule):
    def __init__(
            self,
            nnet:MLP,
            num_classes:int,
            optimizer:torch.optim.Optimizer,
            scheduler:torch.optim.lr_scheduler
        ):
        
        logger.info("MLP_X init")
        super().__init__(num_classes, optimizer, scheduler)
        self.nnet = nnet
        self.save_hyperparameters(logger=False,ignore=['nnet'])
        self.lr = optimizer.keywords['lr'] # for lr finder
    
    def forward(self, x:torch.Tensor)->torch.Tensor:
        return self.nnet(x)
    
    def _step(self, batch, batch_idx):
        x, y = batch
        x = x.view(x.size(0), -1)
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        x = x.view(x.size(0), -1)
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
