"""Neural net model"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.resnet.ipynb.

# %% auto 0
__all__ = ['logger', 'ResBlock', 'ResNet', 'ResNetX']

# %% ../../nbs/models.resnet.ipynb 3
import torch.nn as nn

import torch
from torchinfo import summary


from omegaconf import OmegaConf
from hydra.utils import instantiate


from .conv import ConvLayer
from .core import Classifier
from ..utils import get_device, set_seed

from typing import List, Optional, Callable, Any, Type
import logging
from functools import partial


# %% ../../nbs/models.resnet.ipynb 4
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.resnet.ipynb 6
class ResBlock(nn.Module):
    def __init__(
            self,
            in_channels:int, # Number of input channels
            out_channels:int, # Number of output channels
            stride:int=1,
            kernel_size:int=3,
            activation:Optional[Type[nn.Module]]=nn.ReLU,
            normalization:Optional[Type[nn.Module]]=None
        ):

        super().__init__()
        self.activation = activation()
        conv_block = []
        # ConvLayer parameters:
        # in_channels:int=3, # input channels
        # out_channels:int=16, # output channels
        # kernel_size:int=3, # kernel size
        # stride:int=2, # stride
        # bias:bool=False,
        # normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d,
        # activation:Optional[Type[nn.Module]]=nn.ReLU,
        conv_ = partial(ConvLayer, stride=1, activation=activation, normalization=nn.BatchNorm2d)
        # conv stride 1 to be able to go deeper while keeping the same spatial resolution
        c1 = conv_(in_channels, out_channels, stride=1, kernel_size=kernel_size)
        # conv stride to be able to go wider in number of channels
        # activation will be added at very end
        c2 = conv_(out_channels, out_channels, stride=stride, activation=None, kernel_size=kernel_size) #adding activation to the whole layer at the end c.f. forward
        conv_block += [c1,c2]
        self.conv_layer = nn.Sequential(*conv_block)

        if in_channels == out_channels:
            self.id = nn.Identity()
        else:
            # resize x to match channels
            self.id = conv_(in_channels, out_channels, kernel_size=1, stride=1, activation=None)
        
        if stride == 1:
            self.pooling = nn.Identity()
        else:
            # resize x to match the stride
            self.pooling = nn.AvgPool2d(stride, ceil_mode=True)


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.activation(self.conv_layer(x) + self.id(self.pooling(x)))

# %% ../../nbs/models.resnet.ipynb 10
class ResNet(nn.Module):
    def __init__(
            self,
            n_features: List[int]=[1, 8, 16, 32, 64, 32], # Number of input & output channels
            num_classes: int=10, # Number of classes
        ):

        super().__init__()
        logger.info("ResNet: init")
        layers = []
        res_ = partial(ResBlock, stride=2)

        layers.append(res_(in_channels=n_features[0], out_channels=n_features[1], stride=1))

        for i in range(1, len(n_features)-1):
            layers += [res_(in_channels=n_features[i], out_channels=n_features[i+1])]

        # last layer back to n_classes and flatten
        layers.append(res_(in_channels=n_features[-1], out_channels=num_classes))
        layers.append(nn.Flatten())

        # layers += [nn.Flatten(), nn.Linear(n_features[-1], num_classes, bias=False), nn.BatchNorm1d(num_classes)]
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)

# %% ../../nbs/models.resnet.ipynb 13
class ResNetX(Classifier):
    def __init__(
        self,
        nnet:ResNet,
        num_classes:int,
        optimizer:Callable[...,torch.optim.Optimizer], # optimizer,
        scheduler: Optional[Callable[...,Any]]=None, # scheduler
        ):
        
        logger.info("ResNetX: init")
        super().__init__(
            nnet=nnet,
            num_classes=num_classes,
            optimizer=optimizer,
            scheduler=scheduler
            )

    def _step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
