"""Neural net model"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/models.resnet.ipynb.

# %% auto 0
__all__ = ['logger', 'ResBlock', 'ResNet', 'ResNetX']

# %% ../../nbs/models.resnet.ipynb 3
import torch.nn as nn

import torch
from torchinfo import summary


from omegaconf import OmegaConf
from hydra.utils import instantiate


from .conv import ConvLayer, PreActivationConvLayer
from .core import Classifier
from ..utils import get_device, set_seed

from typing import List, Optional, Callable, Any, Type
import logging
from functools import partial


# %% ../../nbs/models.resnet.ipynb 4
logger = logging.getLogger(__name__)
set_seed()

# %% ../../nbs/models.resnet.ipynb 6
class ResBlock(nn.Module):
    def __init__(
            self,
            in_channels:int, # Number of input channels
            out_channels:int, # Number of output channels
            kernel_size:int=3,
            stride:int=1,
            activation:Optional[Type[nn.Module]]=nn.ReLU, # use nn.Identity for no activation
            normalization:Optional[Type[nn.Module]]=nn.BatchNorm2d
        ):

        super().__init__()
        self.activation = activation()
        conv_block = []

        conv_ = partial(
            PreActivationConvLayer, 
            kernel_size=kernel_size,
            stride=stride,
            activation=activation,
            normalization=normalization,
            )
        # conv stride 1 to be able to go deeper while keeping the same spatial resolution
        c1 = conv_(in_channels, out_channels, stride=1)
        # conv stride to be able to go wider in number of channels
        # activation will be added at very end
        c2 = conv_(out_channels, out_channels, activation=None) #adding activation to the whole layer at the end c.f. forward
        conv_block += [c1,c2]
        self.conv_layer = nn.Sequential(*conv_block)

        if in_channels == out_channels:
            self.id = nn.Identity()
        else:
            # resize x to match channels
            self.id = conv_(in_channels, out_channels, kernel_size=1, stride=1, activation=None) #, normalization=None)
        
        if stride == 1:
            self.pooling = nn.Identity()
        else:
            # resize x to match the stride
            self.pooling = nn.AvgPool2d(stride, ceil_mode=True)


    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.activation(self.conv_layer(x) + self.id(self.pooling(x)))

# %% ../../nbs/models.resnet.ipynb 10
class ResNet(nn.Module):
    def __init__(
            self,
            n_features: List[int]=[1, 8, 16, 32, 64, 32], # Number of input & output channels
            num_classes: int=10, # Number of classes
            activation: Optional[Type[nn.Module]]=partial(nn.LeakyReLU, negative_slope=0.1)
            ):

        super().__init__()
        logger.info("ResNet: init")
        layers = []
        res_ = partial(ResBlock, stride=2, normalization=nn.Identity, activation=activation)

        layers.append(res_(in_channels=n_features[0], out_channels=n_features[1], stride=1))

        for i in range(1, len(n_features)-1):
            layers += [res_(in_channels=n_features[i], out_channels=n_features[i+1])]

        # last layer back to n_classes and flatten
        layers.append(res_(in_channels=n_features[-1], out_channels=num_classes))
        layers.append(nn.Flatten())

        # layers += [nn.Flatten(), nn.Linear(n_features[-1], num_classes, bias=False), nn.BatchNorm1d(num_classes)]
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.layers(x)

# %% ../../nbs/models.resnet.ipynb 13
class ResNetX(Classifier):
    def __init__(
        self,
        nnet:ResNet,
        num_classes:int,
        optimizer:Callable[...,torch.optim.Optimizer], # optimizer,
        scheduler: Optional[Callable[...,Any]]=None, # scheduler
        ):
        
        logger.info("ResNetX: init")
        super().__init__(
            nnet=nnet,
            num_classes=num_classes,
            optimizer=optimizer,
            scheduler=scheduler
            )

    def _step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        loss = self.loss(y_hat, y)
        preds = y_hat.argmax(dim=1)
        return loss, preds, y
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_hat = self.forward(x)
        return y_hat.argmax(dim=1)
