# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/audio.datasets.tts.ipynb.

# %% auto 0
__all__ = ['LhotseTTSDataset', 'TTSDataset', 'LibriTTSDataModule']

# %% ../../../nbs/audio.datasets.tts.ipynb 3
import torch
from torch.utils.data import DataLoader, Dataset
from pytorch_lightning import LightningDataModule, LightningModule

from matplotlib import pyplot as plt

from lhotse import CutSet, RecordingSet, SupervisionSet, Fbank, FbankConfig
from lhotse.dataset import BucketingSampler, OnTheFlyFeatures
from lhotse.dataset.collation import TokenCollater
from lhotse.dataset.vis import plot_batch
from lhotse.recipes import download_librispeech, prepare_librispeech

from pathlib import Path
from pprint import pprint
from typing import List, Dict, Optional, Union

# %% ../../../nbs/audio.datasets.tts.ipynb 6
class LhotseTTSDataset(Dataset):
    def __init__(self,
                tokenizer=TokenCollater, # text tokenizer
                extractor=OnTheFlyFeatures(Fbank(FbankConfig(num_mel_bins=80))) # feature extractor
                ):
        self.extractor = extractor
        self.tokenizer = tokenizer

    def __getitem__(self, cuts: CutSet) -> dict:
        cuts = cuts.sort_by_duration()
        feats, feat_lens = self.extractor(cuts)
        tokens, token_lens = self.tokenizer(cuts)
        return {"feats_pad": feats, "feats_lens": feat_lens, "tokens_pad": tokens, "tokens_lens": token_lens}

# %% ../../../nbs/audio.datasets.tts.ipynb 9
class TTSDataset(Dataset):
    def __init__(self,
        tokenizer, # text tokenizer
        num_mel_bins:int=80,  # number of mel spectrogram bins
        sampling_rate:int=16000 # sampling rate
        ):
        self.extractor = OnTheFlyFeatures(Fbank(FbankConfig(num_mel_bins=num_mel_bins, sampling_rate=sampling_rate)))
        self.tokenizer = tokenizer

    def __getitem__(self, cuts: CutSet) -> dict:
        cuts = cuts.sort_by_duration()
        feats, feat_lens = self.extractor(cuts)
        tokens, token_lens = self.tokenizer(cuts)
        return {"feats_pad": feats, "feats_lens": feat_lens, "tokens_pad": tokens, "tokens_lens": token_lens}

# %% ../../../nbs/audio.datasets.tts.ipynb 11
from lhotse.recipes import download_libritts, prepare_libritts
from ...text.tokenizers import Tokenizer
from ..embedding import EncoDec
from torchaudio.datasets import LIBRITTS
from ..utils import plot_waveform

# %% ../../../nbs/audio.datasets.tts.ipynb 14
class LibriTTSDataModule(LightningDataModule):
    def __init__(self,
        target_dir="/data/en/libriTTS", # where data will be saved / retrieved
        dataset_parts=["dev-clean", "test-clean"], # either full libritts or subset
        output_dir="/home/syl20/slg/nimrod/recipes/libritts/data", # where to save manifest
        num_jobs=0 # num_jobs depending on number of cpus available
    ):
        super().__init__()
        self.save_hyperparameters(logger=False)

    def prepare_data(self,) -> None:
        # takes a while to download from openslr mirror (~15 min each for test/dev-clean)
        download_libritts(target_dir=self.hparams.target_dir, dataset_parts=self.hparams.dataset_parts)

    def setup(self, stage = None):
        self.libri = prepare_libritts(corpus_dir=Path(self.hparams.target_dir) / "LibriTTS", output_dir=self.hparams.output_dir, num_jobs=self.hparams.num_jobs)
        if stage == 'fit' or stage == None:
            self.cuts_train = CutSet.from_manifests(**self.libri["dev-clean"])
            self.cuts_test = CutSet.from_manifests(**self.libri["test-clean"])
            self.tokenizer = TokenCollater(self.cuts_train)
            self.tokenizer(self.cuts_test.subset(first=2))
            # self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))
        if stage == "test":
            self.cuts_test = CutSet.from_manifests(**self.libri["test-clean"])
            self.tokenizer = TokenCollater(self.cuts_test)
            self.tokenizer(self.cuts_test.subset(first=2))
            # self.tokenizer.inverse(*self.tokenizer(self.cuts_test.subset(first=2)))

    def train_dataloader(self):
        train_sampler = BucketingSampler(self.cuts_train, max_duration=300, shuffle=True) #, bucket_method="equal_duration")
        return DataLoader(TTSDataset(self.tokenizer, sampling_rate=24000), sampler=train_sampler, batch_size=None, num_workers=self.hparams.num_jobs)

    def test_dataloader(self):
        test_sampler = BucketingSampler(self.cuts_test, max_duration=400, shuffle=False) #, bucket_method="equal_duration")
        return DataLoader(TTSDataset(self.tokenizer, sampling_rate=24000), sampler=test_sampler, batch_size=None, num_workers=self.hparams.num_jobs)

    @property
    def model_kwargs(self):
        return {
            "odim": len(self.tokenizer.idx2token),
        }
