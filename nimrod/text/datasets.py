# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.datasets.ipynb.

# %% auto 0
__all__ = ['logger', 'Vocab', 'CharDataset', 'CharDataModule']

# %% ../../nbs/text.datasets.ipynb 4
# torch
import torch
from torch.optim import SGD
import torchtext; torchtext.disable_torchtext_deprecation_warning()
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import vocab as torchtext_vocab
from torch.utils.data import DataLoader, dataset, Dataset, random_split

# L
from lightning import LightningDataModule

# hf
import datasets
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorForLanguageModeling

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# param
from omegaconf import DictConfig, OmegaConf
from hydra.utils import instantiate

# python
from typing import Dict, List, Tuple, Optional, Set, Any
from collections import Counter, OrderedDict
from plum import dispatch
import random
import os
from pathlib import Path
import requests
import re

# nimrod
# from nimrod.models.lm import Vocab
from ..utils import set_seed
from ..data.core import DataModule

import logging
logger = logging.getLogger(__name__)


# %% ../../nbs/text.datasets.ipynb 8
class Vocab:
    def __init__(self,
                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to text data file
                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters
                ):

        self.data_path = Path(data_path)
        if not self.data_path.exists():
            self._download_data()

        logger.info(f"Vocab: read text file")
        with open(self.data_path, 'r') as f:
            text = f.read()

        chars = set(text)
        if specials is not None:
            for special in specials:
                chars.add(special)

        self._stoi = {c: i for i, c in enumerate(chars)}
        self._itos = {i: c for i, c in enumerate(chars)}
        self.voc = chars


    
    def _download_data(self):
        logger.info(f"Vocab: download data from url")
        url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        response = requests.get(url)
        response.raise_for_status()
        
        with open(self.data_path, 'w') as f:
            f.write(response.text)

    @dispatch
    def stoi(self, token:str)->int:
        if token not in self._stoi:
            return self._stoi['<unk>']
        return self._stoi[token]

    @dispatch
    # for list of characters
    def stoi(self, tokens:List[str])->List[int]:
        # TODO: deal with unknown tokens
        return [self._stoi[tok] if tok in self._stoi else self._stoi['<unk>'] for tok in tokens]
    
    # @dispatch #TODO
    # def stoi(self, tokens:List[List[str]])->List[List[int]]:
    #     return [self._stoi[u] for tok in tokens for ]
    # TODO:
    # support torch tensors

    @dispatch    
    def itos(self, index:int)->str:
        return self._itos[index]
    
    @dispatch    
    def itos(self, indices:List[int])->List[str]:
        return [self._itos[index] for index in indices]
        
    def __len__(self):
        return len(self.voc)
    
    @property
    def vocabulary(self)->Set:
        return sorted(set([k for k,v in self._stoi.items()]))


# %% ../../nbs/text.datasets.ipynb 15
class CharDataset(Dataset):
    def __init__(self,
                data_path: str | os.PathLike='../data/text/tiny_shakespeare.txt', # path to the data file
                context_length: int=3, # context length
                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters
                add_sentence_tokens: bool = True, # add special tokens to the data
                ):
        logger.info(f"CharDataset: init")
        
        # vocab will download data if not found
        self.v = Vocab(data_path=data_path, specials=specials)
        self._vocab_size = len(self.v)
        self.context_length = context_length
        self.special_token_pattern = re.compile(f'({re.escape("<bos>")}|{re.escape("<eos>")})')

        with open(data_path, 'r') as f:
            text = f.read()

        if add_sentence_tokens:
            # Split into sentences (roughly, using periods) and add special tokens
            sentences = [s.strip() for s in text.split('.') if s.strip()]
            sentences = ['<bos>' + s + '<eos>' for s in sentences]
            # Join list of words into single continuous text
            text = " ".join(sentences)

        # text = text.replace("\n", " ")
        tokens = self._tokenizer(text)
        self.data = torch.tensor(self.v.stoi(tokens), dtype=torch.long)

    def __len__(self) -> int:
        return len(self.data)

    def _tokenizer(self, text: str) -> List[str]:
        parts = self.special_token_pattern.split(text)
        tokens = []
        for part in parts:
            if part in ["<bos>", "<eos>"]:
                tokens.append(part)
            else:
                tokens.extend(part)
        return tokens

    @property
    def vocabulary(self)->Set:
        return sorted(set([k for k,v in self.v._stoi.items()]))
    
    @property
    def vocab_size(self)->int:
        return self._vocab_size

    @property
    def vocab_class(self)->Vocab:
        return self.v

    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # i = random.randint(0, len(self.data) - (self.context_length + 1))
        max_index = len(self.data) - (self.context_length + 1)
        if i > max_index:
            # wrap around to the beginning if we hit the end
            i = i % (max_index + 1)
        chunk = self.data[i : i + self.context_length + 1]
        x = chunk[:-1]
        y = chunk[1:]
        return x, y

    def to_tokens(self, message: str) -> torch.Tensor:
        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)

    def from_tokens(self, tokens: torch.Tensor) -> str:
        return "".join([self.v.itos(int(i)) for i in tokens])

# %% ../../nbs/text.datasets.ipynb 21
class CharDataModule(DataModule, LightningDataModule):
    def __init__(self,
            # dataset
            data_path: str | os.PathLike = '../data/text/tiny_shakespeare.txt',
            specials=['<pad>', '<unk>', '<bos>', '<eos>'],
            add_sentence_tokens: bool = False,
            # data module
            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),
            context_size: int = 3,
            batch_size: int = 32,
            num_workers: int = 1,
            pin_memory: bool = False,
            persistent_workers: bool = False,
            random_split: bool = True
            ):

        logger.info(f"CharDataModule: init")

        super().__init__(
            train_val_test_split=train_val_test_split,
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=persistent_workers,
            )
        self.save_hyperparameters()
        self.ds: CharDataset = None
    
    def prepare_data(self) -> None:
        pass

    
    def setup(self, stage: Optional[str] = None) -> None:
        logger.info("CharDataModule: setup, split datasets")
        # run in each GPU process. define, split DS, etc.
        self.ds = CharDataset(
            self.hparams.data_path,
            self.hparams.context_size,
            self.hparams.specials,
            self.hparams.add_sentence_tokens,
            )
        if self.hparams.random_split:
            lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]
            lengths[-1] = len(self.ds) - sum(lengths[:-1])
            self.data_train, self.data_val, self.data_test = random_split(self.ds, lengths)
        else:
            self.data_train, self.data_val, self.data_test = self._sequential_split(self.ds, self.hparams.train_val_test_split)
    
    @property
    def vocab_size(self)->int:
        if self.ds is None:
            raise ValueError("Dataset not initialized")
        return self.ds.vocab_size


