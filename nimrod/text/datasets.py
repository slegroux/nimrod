# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.datasets.ipynb.

# %% auto 0
__all__ = ['SEED', 'Vocab', 'CharDataset', 'CharDataModule']

# %% ../../nbs/text.datasets.ipynb 3
# torch
import torch
from torch.optim import SGD
import torchtext; torchtext.disable_torchtext_deprecation_warning()
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import vocab
from torch.utils.data import DataLoader, dataset, Dataset, random_split

# pl
from lightning import LightningDataModule, seed_everything

# hf
import datasets
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DefaultDataCollator, default_data_collator

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# param
from omegaconf import DictConfig, OmegaConf
from hydra.utils import instantiate

# python
from typing import Dict, List, Tuple, Optional, Set, Union
from collections import Counter, OrderedDict

from plum import dispatch
import urllib
import random
import os
from typing import Dict, List, Tuple, Optional, Set, Any

# nimrod
# from nimrod.models.lm import Vocab
SEED = 42
seed_everything(SEED)

# %% ../../nbs/text.datasets.ipynb 5
class Vocab:
    def __init__(self,
                data_path: str | os.PathLike, # path to text data file
                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters
                add_sentence_tokens=True, # add <bos> and <eos> tokens to each sentence
                ):
        # read data
        df = pd.read_fwf(data_path, header=None, names=['text'])
        if add_sentence_tokens:
            df.loc[:, 'text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])
        data = list(df['text'])
        # count individual tokens
        c = Counter()
        for row in data:
            for token in row:
                c.update(token)
        ordered_tuple = sorted(c.items(), key=lambda x:x[1], reverse=True)
        dict = OrderedDict(ordered_tuple)        
        # leverage torchtext vocab
        self.voc = vocab(dict, specials=specials)
        if '<unk>' in specials:
            self.voc.set_default_index(self.voc['<unk>'])
        else:
            self.voc.set_default_index(-1)
        self._stoi = self.voc.get_stoi()
        self._itos = self.voc.get_itos()

    @dispatch
    def stoi(self, token:str)->int:
        if len(token) > 1 and token not in ['<pad>', '<unk>', '<bos>', '<eos>']:
            raise ValueError("input should be a token or list of tokens")
        return self._stoi[token]

    @dispatch
    # for list of characters
    def stoi(self, tokens:List[str])->List[int]:
        return [self._stoi[tok] for tok in tokens]
    
    # @dispatch #TODO
    # def stoi(self, tokens:List[List[str]])->List[List[int]]:
    #     return [self._stoi[u] for tok in tokens for ]
    # TODO:
    # support torch tensors

    @dispatch    
    def itos(self, index:int)->str:
        return self._itos[index]
    
    @dispatch    
    def itos(self, indices:List[int])->List[str]:
        return [self._itos[index] for index in indices]
        
    def __len__(self):
        return len(self.voc)
    
    @property
    def vocabulary(self)->Set:
        return sorted(set([k for k,v in self._stoi.items()]))


# %% ../../nbs/text.datasets.ipynb 12
class CharDataset(Dataset):
    def __init__(self,
                data_path: str | os.PathLike, # path to the data file
                context_length: int, # context length
                vocab: Vocab, # vocab object
                add_sentence_tokens: bool = True, # add special tokens to the data
                verbose: bool = False, # print info
                ):
        
        df = pd.read_fwf(data_path, header=None, names=['text'])

        if add_sentence_tokens:
            df['text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])

        data = list(df['text'])
        self.data = []
        # flatten list of list of chars into one big list of chars with <bos> and <eos>
        for row in data:
            self.data.extend(row)

        self.context_length = context_length
        self.v = vocab
        self.vocab_size = len(self.v)
        
    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        i = random.randint(0, len(self.data) - (self.context_length + 1))
        chunk = self.data[i : i + self.context_length + 1]
        dix = self.v.stoi(chunk)
        x = torch.tensor(dix[:-1], dtype=torch.long)
        y = torch.tensor(dix[1:], dtype=torch.long)
        return x, y

    def to_tokens(self, message: str) -> torch.Tensor:
        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)

    def from_tokens(self, tokens: torch.Tensor) -> str:
        return "".join([self.v.itos(int(i)) for i in tokens])

# %% ../../nbs/text.datasets.ipynb 18
class CharDataModule(LightningDataModule):
    def __init__(
            self,
            data_path: str | os.PathLike,
            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),
            context_size: int = 3,
            batch_size: int = 32,
            num_workers: int = 0,
            pin_memory: bool = False,
            persistent_workers: bool = False,
            ):
        
        super().__init__()
        self.save_hyperparameters()
        self.train_ds: Optional[Dataset] = None
        self.val_ds: Optional[Dataset] = None
        self.test_ds: Optional[Dataset] = None
        self.ds: Optional[Dataset] = None
        # we extract vocab from the full char dataset
        # TODO: add option to pass in vocab
        self.v = Vocab(data_path)

        if sum(train_val_test_split) != 1.0:
            raise ValueError("train_val_test_split must sum to 1.0")

    def prepare_data(self) -> None:
        # run in main process. download, tokenize, etc.
        pass
    
    def setup(self, stage: Optional[str] = None) -> None:
        # run in each GPU process. define, split DS, etc.
        self.ds = CharDataset(self.hparams.data_path, self.hparams.context_size, self.v)
        lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]
        lengths[-1] = len(self.ds) - sum(lengths[:-1])

        self.train_ds, self.val_ds, self.test_ds = random_split(self.ds, lengths)

    
    def train_dataloader(self) -> DataLoader:
        return DataLoader(
            self.train_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=True,
            persistent_workers=self.hparams.persistent_workers
        )

    def val_dataloader(self) -> DataLoader:
        return DataLoader(
            self.val_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=False,
            persistent_workers=self.hparams.persistent_workers
        )
 
    def test_dataloader(self) -> DataLoader:
        return DataLoader(
            self.test_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=False,
            persistent_workers=self.hparams.persistent_workers
        )
 

    def teardown(self, stage: Optional[str] = None) -> None:
        return super().teardown(stage)

    def state_dict(self) -> Dict[str, Any]:
        return super().state_dict()

    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        return super().load_state_dict(state_dict)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return super().forward(x)

    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:
        return super().training_step(batch, batch_idx)
    
