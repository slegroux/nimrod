# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.datasets.ipynb.

# %% auto 0
__all__ = ['CharDataset', 'CharDataModule']

# %% ../../nbs/text.datasets.ipynb 3
# torch
import torch
from torch.utils.data import DataLoader, dataset, Dataset, random_split
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

# torchtext
import torchtext
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import vocab, build_vocab_from_iterator

# hf
import datasets
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DefaultDataCollator, default_data_collator

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# python
from typing import Dict, List, Tuple, Optional, Set, Union
from collections import Counter, OrderedDict
from dataclasses import dataclass, asdict
from plum import dispatch
import urllib
import math
import random
import os

# nimrod
from ..data.utils import DataModule, split_train_valid_test

# conf
from hydra.utils import instantiate
from omegaconf import OmegaConf

# %% ../../nbs/text.datasets.ipynb 5
class CharDataset(Dataset):
    def __init__(self,
                data: Union[str, os.PathLike], # text as a long continuous string
                block_size: int # context length
                ):
        chars = list(set(data))
        data_size, vocab_size = len(data), len(chars)

        self.stoi = {ch: i for i, ch in enumerate(chars)}
        self.itos = {i: ch for i, ch in enumerate(chars)}
        self.block_size = block_size
        self.vocab_size = vocab_size
        self.data = data

    def __len__(self) -> int:
        return math.ceil(len(self.data) / (self.block_size + 1))

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        i = random.randint(0, len(self.data) - (self.block_size + 1))
        chunk = self.data[i : i + self.block_size + 1]
        dix = [self.stoi[s] for s in chunk]
        x = torch.tensor(dix[:-1], dtype=torch.long)
        y = torch.tensor(dix[1:], dtype=torch.long)
        return x, y

    def to_tokens(self, message: str) -> torch.Tensor:
        return torch.tensor([self.stoi[s] for s in message], dtype=torch.long)

    def from_tokens(self, tokens: torch.Tensor) -> str:
        return "".join([self.itos[int(i)] for i in tokens])

# %% ../../nbs/text.datasets.ipynb 10
class CharDataModule(DataModule):
    def __init__(
                self,
                text_file:Union[str, os.PathLike], # path to text file
                context_length:int=8, # context length or block size
                train_val_test_split:List[float] = [0,8,0.1,0.1], #dataset splits in %
                batch_size:int=64,
                num_workers: int = 0, # num_workers equal 0 means that itâ€™s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but youâ€™ll only have a single worker, so it might be slow
                pin_memory: bool = False, # If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer
                persistent_workers: bool = False
                ):
        super().__init__()
        self.dataset, self.data_train, self.data_val, self.data_test = None, None, None, None
    
    def prepare_data(self) -> None:
        # concat all lines of a text file and splice it in context_length chunks
        with open(self.hparams.text_file) as f:
            text = f.read()
        self.dataset = CharDataset(text, self.hparams.context_length)

    def setup(self, stage: Optional[str]=None) -> None:
        # stage: {fit,validate,test,predict}
        self.data_train, self.data_val, self.data_test = split_train_valid_test(self.dataset, self.hparams.train_val_test_split)
            

