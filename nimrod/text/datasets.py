# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.datasets.ipynb.

# %% auto 0
<<<<<<< HEAD
__all__ = ['CharDataset', 'CharDataModule']
=======
__all__ = ['SEED', 'Vocab', 'CharDataset', 'CharDataModule']
>>>>>>> main

# %% ../../nbs/text.datasets.ipynb 3
# torch
import torch
<<<<<<< HEAD
from torch.utils.data import DataLoader, dataset, Dataset, random_split
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

# torchtext
import torchtext
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import vocab, build_vocab_from_iterator
=======
from torch.optim import SGD
import torchtext; torchtext.disable_torchtext_deprecation_warning()
from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import vocab
from torch.utils.data import DataLoader, dataset, Dataset, random_split

# pl
from lightning import LightningDataModule, seed_everything
>>>>>>> main

# hf
import datasets
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, DefaultDataCollator, default_data_collator

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# param
from omegaconf import DictConfig, OmegaConf
from hydra.utils import instantiate

# python
from typing import Dict, List, Tuple, Optional, Set, Union
from collections import Counter, OrderedDict

from plum import dispatch
import urllib
import random
import os
<<<<<<< HEAD

# nimrod
from ..data.utils import DataModule, split_train_valid_test

# conf
from hydra.utils import instantiate
from omegaconf import OmegaConf
=======
from typing import Dict, List, Tuple, Optional, Set, Any

# nimrod
# from nimrod.models.lm import Vocab
SEED = 42
seed_everything(SEED)
>>>>>>> main

# %% ../../nbs/text.datasets.ipynb 5
class Vocab:
    def __init__(self,
                data_path: str | os.PathLike, # path to text data file
                specials=['<pad>', '<unk>', '<bos>', '<eos>'], # encode special characters
                add_sentence_tokens=True, # add <bos> and <eos> tokens to each sentence
                ):
        # read data
        df = pd.read_fwf(data_path, header=None, names=['text'])
        if add_sentence_tokens:
            df.loc[:, 'text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])
        data = list(df['text'])
        # count individual tokens
        c = Counter()
        for row in data:
            for token in row:
                c.update(token)
        ordered_tuple = sorted(c.items(), key=lambda x:x[1], reverse=True)
        dict = OrderedDict(ordered_tuple)        
        # leverage torchtext vocab
        self.voc = vocab(dict, specials=specials)
        if '<unk>' in specials:
            self.voc.set_default_index(self.voc['<unk>'])
        else:
            self.voc.set_default_index(-1)
        self._stoi = self.voc.get_stoi()
        self._itos = self.voc.get_itos()

    @dispatch
    def stoi(self, token:str)->int:
        if len(token) > 1 and token not in ['<pad>', '<unk>', '<bos>', '<eos>']:
            raise ValueError("input should be a token or list of tokens")
        return self._stoi[token]

    @dispatch
    # for list of characters
    def stoi(self, tokens:List[str])->List[int]:
        return [self._stoi[tok] for tok in tokens]
    
    # @dispatch #TODO
    # def stoi(self, tokens:List[List[str]])->List[List[int]]:
    #     return [self._stoi[u] for tok in tokens for ]
    # TODO:
    # support torch tensors

    @dispatch    
    def itos(self, index:int)->str:
        return self._itos[index]
    
    @dispatch    
    def itos(self, indices:List[int])->List[str]:
        return [self._itos[index] for index in indices]
        
    def __len__(self):
        return len(self.voc)
    
    @property
    def vocabulary(self)->Set:
        return sorted(set([k for k,v in self._stoi.items()]))


# %% ../../nbs/text.datasets.ipynb 12
class CharDataset(Dataset):
    def __init__(self,
<<<<<<< HEAD
                data: Union[str, os.PathLike], # text as a long continuous string
                block_size: int # context length
=======
                data_path: str | os.PathLike, # path to the data file
                context_length: int, # context length
                vocab: Vocab, # vocab object
                add_sentence_tokens: bool = True, # add special tokens to the data
                verbose: bool = False, # print info
>>>>>>> main
                ):
        
        df = pd.read_fwf(data_path, header=None, names=['text'])

        if add_sentence_tokens:
            df['text'] = df['text'].apply(lambda x: ['<bos>'] + list(x)+ ['<eos>'])

        data = list(df['text'])
        self.data = []
        # flatten list of list of chars into one big list of chars with <bos> and <eos>
        for row in data:
            self.data.extend(row)

        self.context_length = context_length
        self.v = vocab
        self.vocab_size = len(self.v)
        
    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        i = random.randint(0, len(self.data) - (self.context_length + 1))
        chunk = self.data[i : i + self.context_length + 1]
        dix = self.v.stoi(chunk)
        x = torch.tensor(dix[:-1], dtype=torch.long)
        y = torch.tensor(dix[1:], dtype=torch.long)
        return x, y

    def to_tokens(self, message: str) -> torch.Tensor:
        return torch.tensor([self.v.stoi(s) for s in message], dtype=torch.long)

    def from_tokens(self, tokens: torch.Tensor) -> str:
<<<<<<< HEAD
        return "".join([self.itos[int(i)] for i in tokens])

# %% ../../nbs/text.datasets.ipynb 10
class CharDataModule(DataModule):
    def __init__(
                self,
                text_file:Union[str, os.PathLike], # path to text file
                context_length:int=8, # context length or block size
                train_val_test_split:List[float] = [0,8,0.1,0.1], #dataset splits in %
                batch_size:int=64,
                num_workers: int = 0, # num_workers equal 0 means that it’s the main process that will do the data loading when needed, num_workers equal 1 is the same as any n, but you’ll only have a single worker, so it might be slow
                pin_memory: bool = False, # If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer by enabling pin_memory. This lets your DataLoader allocate the samples in page-locked memory, which speeds-up the transfer
                persistent_workers: bool = False
                ):
        super().__init__()
        self.dataset, self.data_train, self.data_val, self.data_test = None, None, None, None
    
    def prepare_data(self) -> None:
        # concat all lines of a text file and splice it in context_length chunks
        with open(self.hparams.text_file) as f:
            text = f.read()
        self.dataset = CharDataset(text, self.hparams.context_length)

    def setup(self, stage: Optional[str]=None) -> None:
        # stage: {fit,validate,test,predict}
        self.data_train, self.data_val, self.data_test = split_train_valid_test(self.dataset, self.hparams.train_val_test_split)
            

=======
        return "".join([self.v.itos(int(i)) for i in tokens])

# %% ../../nbs/text.datasets.ipynb 18
class CharDataModule(LightningDataModule):
    def __init__(
            self,
            data_path: str | os.PathLike,
            train_val_test_split: Tuple[int, int, int] = (0.8, 0.1, 0.1),
            context_size: int = 3,
            batch_size: int = 32,
            num_workers: int = 0,
            pin_memory: bool = False,
            persistent_workers: bool = False,
            ):
        
        super().__init__()
        self.save_hyperparameters()
        self.train_ds: Optional[Dataset] = None
        self.val_ds: Optional[Dataset] = None
        self.test_ds: Optional[Dataset] = None
        self.ds: Optional[Dataset] = None
        # we extract vocab from the full char dataset
        # TODO: add option to pass in vocab
        self.v = Vocab(data_path)

        if sum(train_val_test_split) != 1.0:
            raise ValueError("train_val_test_split must sum to 1.0")

    def prepare_data(self) -> None:
        # run in main process. download, tokenize, etc.
        pass
    
    def setup(self, stage: Optional[str] = None) -> None:
        # run in each GPU process. define, split DS, etc.
        self.ds = CharDataset(self.hparams.data_path, self.hparams.context_size, self.v)
        lengths = [int(p * len(self.ds)) for p in self.hparams.train_val_test_split]
        lengths[-1] = len(self.ds) - sum(lengths[:-1])

        self.train_ds, self.val_ds, self.test_ds = random_split(self.ds, lengths)

    
    def train_dataloader(self) -> DataLoader:
        return DataLoader(
            self.train_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=True,
            persistent_workers=self.hparams.persistent_workers
        )

    def val_dataloader(self) -> DataLoader:
        return DataLoader(
            self.val_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=False,
            persistent_workers=self.hparams.persistent_workers
        )
 
    def test_dataloader(self) -> DataLoader:
        return DataLoader(
            self.test_ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=self.hparams.pin_memory,
            shuffle=False,
            persistent_workers=self.hparams.persistent_workers
        )
 

    def teardown(self, stage: Optional[str] = None) -> None:
        return super().teardown(stage)

    def state_dict(self) -> Dict[str, Any]:
        return super().state_dict()

    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        return super().load_state_dict(state_dict)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return super().forward(x)

    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:
        return super().training_step(batch, batch_idx)
    
>>>>>>> main
