# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.tokenizers.ipynb.

# %% auto 0
__all__ = ['Tokenizer', 'Numericalizer']

# %% ../../nbs/text.tokenizers.ipynb 3
# torch
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

# torchtext
import torchtext
from torchtext.vocab import vocab
from torchtext.data.utils import get_tokenizer
from torchtext.datasets import AG_NEWS

# hf
import datasets
from transformers import AutoTokenizer, DataCollatorForLanguageModeling

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# python
from typing import Dict, List, Tuple, Optional, Set, Iterable
from collections import Counter, OrderedDict
from dataclasses import dataclass, asdict
from plum import dispatch

# nimrod
# from nimrod.models.lm import Vocab

# %% ../../nbs/text.tokenizers.ipynb 5
class Tokenizer:
    def __init__(self,
                backend:str='spacy', # backend tokenizer default to spacy
                language:str='en', # language on which tokenization is applied
                bos:bool=False, # add beginning of sentence tag <bos>
                eos:bool=False, # add end of sentence tag <eos>
                ):
        if backend == 'spacy' and language == 'en':
            language = 'en_core_web_sm'
        if backend== 'character_based':
            self.tokenizer = self.character_tokenizer
        else:
            self.tokenizer = get_tokenizer(backend, language=language)
        self.bos = bos
        self.eos = eos
        self.backend = backend
        print(f"# Tokenizer uses {self.backend} backend")
    
    @staticmethod
    def character_tokenizer(text:str)->List[str]:
        return [c for c in text]
    
    @dispatch
    def __call__(self, text:str)->List[str]:
        res = self.tokenizer(text)
        if self.bos:
            res = ['<bos>'] + res
        if self.eos:
            res = res + ['<eos>']
        return(res)
    
    @dispatch
    def __call__(self, texts:List[str])->List[List[str]]:
        return [self(text) for text in texts]
    
    @dispatch # to replace Iterable
    # works with agnews type of dataset [(index, text)]
    def __call__(self, data_iter:Iterable)->Iterable:
        for _, text in data_iter:
            yield self(text)

    @dispatch    
    def inverse(self, tokens:List[str])->str:
        if self.backend == 'character_based':
            return ''.join(tokens)
        # TODO: take care of white spaces
        else:
            return ' '.join(tokens)

    @dispatch
    def inverse(self, list_of_tokens:List[List[str]])->List[str]:
        s = []
        for tokens in list_of_tokens:
            s.append(self.inverse(tokens))
        return s

# %% ../../nbs/text.tokenizers.ipynb 14
# TODO: add more special characters
class Numericalizer():
    def __init__(self, tokens_iter:Iterable, specials=["<pad>", "<unk>", "<bos>", "<eos>"]):
        self._vocab = self.build_map_from_iter(tokens_iter, specials)
    
    def build_map_from_iter(self,data_iter:Iterable, specials=None):
        self._vocab = torchtext.vocab.build_vocab_from_iterator(data_iter, specials=specials)
        if "<unk>" in specials:
            self._vocab.set_default_index(self._vocab["<unk>"])
        return self._vocab

    @dispatch
    def __call__(self, texts:List[str])->List[List[int]]:
        # TODO: check self._vocab has been built
        return [self._vocab[text] for text in texts]
    
    @dispatch
    def __call__(self, texts:List[List[str]]):
        # TODO: use nested list comprehension
        res = []
        for row in texts:
            res.append([self._vocab[text] for text in row])
        return res
        
    @dispatch
    def __call__(self, text:str)->int:
        return self._vocab[text]
    
    @property
    def vocab(self):
        return(self._vocab)
    
    @dispatch
    def inverse(self, idx:int)->str:
        return self._vocab.get_itos()[idx]

    @dispatch
    def inverse(self, indices:List[int])->List[str]:
        return [self._vocab.get_itos()[i] for i in indices]
