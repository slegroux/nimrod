# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.tokenizers.ipynb.

# %% auto 0
__all__ = ['Phonemizer', 'Tokenizer', 'Numericalizer', 'TextCollater']

# %% ../../nbs/text.tokenizers.ipynb 4
from phonemizer.backend import EspeakBackend
from phonemizer.backend.espeak.language_switch import LanguageSwitch
from phonemizer.backend.espeak.words_mismatch import WordMismatch
from phonemizer.punctuation import Punctuation
from phonemizer.separator import Separator
from phonemizer import phonemize
from torch.utils.data import DataLoader
from plum import dispatch
from typing import List, Tuple

# %% ../../nbs/text.tokenizers.ipynb 6
class Phonemizer():
    def __init__(self,
        separator=Separator(word=" ", syllable="|", phone=None), # separator
        language='en-us', # language
        backend='espeak', # phonemization backend (espeak)
        strip=True, # strip
        preserve_punctuation=True # preserve punctuation
        ):
        self.separator = separator
        self.language = language
        self.backend = backend
        self.strip = strip
        self.preserve_punctuation = preserve_punctuation
    
    @dispatch
    def __call__(self, text:str, n_jobs=1)->str:
        return(
            phonemize(
                text,
                language=self.language,
                backend=self.backend,
                separator=self.separator,
                strip=self.strip,
                preserve_punctuation=self.preserve_punctuation,
                njobs=n_jobs
                )
        )

    @dispatch
    def __call__(self, texts:List[str], n_jobs=1)->List[str]:
        return(
            [phonemize(
                text,
                language=self.language,
                backend=self.backend,
                separator=self.separator,
                strip=self.strip,
                preserve_punctuation=self.preserve_punctuation,
                njobs=n_jobs
                )
        for text in texts])

# %% ../../nbs/text.tokenizers.ipynb 11
import torch
from collections import Counter
import torchtext
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from collections import Counter
from torchtext.datasets import AG_NEWS
from typing import Iterable, List, Tuple
from torch.nn.utils.rnn import pad_sequence

# %% ../../nbs/text.tokenizers.ipynb 12
class Tokenizer:
    def __init__(self, backend='spacy', language='en'):
        if language == 'en':
            language = 'en_core_web_sm'
        self.tokenizer = get_tokenizer(backend, language=language)

    @dispatch
    def __call__(self, text:str)->List[str]:
        return self.tokenizer(text)
    
    @dispatch
    def __call__(self, texts:List[str])->List[List[str]]:
        return [self.tokenizer(text) for text in texts]
    
    @dispatch # to replace Iterable
    # works with agnews type of dataset [(index, text)]
    def __call__(self, data_iter:Iterable)->Iterable:
        for _, text in data_iter:
            yield self.tokenizer(text)

    @dispatch    
    def inverse(self, tokens:List[str])->str:
        # TODO: take care of white spaces
        return ' '.join(tokens)

    @dispatch
    def inverse(self, list_of_tokens:List[List[str]])->List[str]:
        s = []
        for tokens in list_of_tokens:
            s.append(' '.join(tokens)) 
        return s

# %% ../../nbs/text.tokenizers.ipynb 17
# TODO: add more special characters
class Numericalizer():
    def __init__(self, tokens_iter:Iterable, specials=["<pad>", "<unk>", "<bos>", "<eos>"]):
        self._vocab = self.build_map_from_iter(tokens_iter, specials)
    
    def build_map_from_iter(self,data_iter:Iterable, specials=None):
        self._vocab = torchtext.vocab.build_vocab_from_iterator(data_iter, specials=specials)
        if "<unk>" in specials:
            self._vocab.set_default_index(self._vocab["<unk>"])
        return self._vocab

    @dispatch
    def __call__(self, texts:List[str])->List[List[int]]:
        # TODO: check self._vocab has been built
        return [self._vocab[text] for text in texts]
    
    @dispatch
    def __call__(self, texts:List[List[str]]):
        # TODO: use nested list comprehension
        res = []
        for row in texts:
            res.append([self._vocab[text] for text in row])
        return res
        
    @dispatch
    def __call__(self, text:str)->int:
        return self._vocab[text]
    
    @property
    def vocab(self):
        return(self._vocab)
    
    @dispatch
    def inverse(self, idx:int)->str:
        return self._vocab.get_itos()[idx]

    @dispatch
    def inverse(self, indices:List[int])->List[str]:
        return [self._vocab.get_itos()[i] for i in indices]
    

# %% ../../nbs/text.tokenizers.ipynb 23
class TextCollater:
    def __init__(self,
                 tokenizer,
                 numericalizer,
                 padding_value:int=0
                ):
        self._numericalizer = numericalizer
        self._tokenizer = tokenizer
        self.padding_value = padding_value

    def collate_list(self, texts:List[str])->Tuple[torch.Tensor, torch.Tensor]:
        token_list = self._tokenizer(texts)
        token_list = [torch.LongTensor(tokens) for tokens in self._numericalizer(token_list)]
        text_lens = torch.LongTensor([tokens.shape[0] for tokens in token_list])
        text_pad = pad_sequence(token_list, batch_first=True, padding_value=self.padding_value)
        return text_pad, text_lens

    def collate_agnews(self, batch)->Tuple[torch.Tensor, torch.Tensor]:
        texts = [item[1] for item in batch]
        token_list = self._tokenizer(texts)
        token_list = [torch.LongTensor(tokens) for tokens in self._numericalizer(token_list)]
        text_lens = torch.LongTensor([tokens.shape[0] for tokens in token_list])
        text_pad = pad_sequence(token_list, batch_first=True, padding_value=self.padding_value)
        return text_pad, text_lens
