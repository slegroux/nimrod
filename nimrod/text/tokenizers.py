# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/text.tokenizers.ipynb.

# %% auto 0
__all__ = ['CharTokenizer']

# %% ../../nbs/text.tokenizers.ipynb 4
# torch
import torch.nn as nn
import torch
import torch.nn.functional as F
from torch.nn.utils.rnn import pad_sequence
from torch.optim import SGD

# hf
import datasets
from transformers import AutoTokenizer, DataCollatorForLanguageModeling

# data 
import pandas as pd
import numpy as np

# ui
from matplotlib import pyplot as plt
from tqdm.notebook import tqdm

# python
from typing import Dict, List, Tuple, Optional, Set, Iterable
from collections import Counter, OrderedDict
from dataclasses import dataclass, asdict
from plum import dispatch
from pathlib import Path

# nimrod
# from nimrod.models.lm import Vocab


# %% ../../nbs/text.tokenizers.ipynb 7
class CharTokenizer:
    def __init__(self, vocabulary:List[str]):
        self.ctoi = {char: tok_id for tok_id, char in enumerate(vocabulary)}
        self.itoc = {tok_id: char for tok_id, char in enumerate(vocabulary)}
    
    @classmethod
    def from_text(cls, text:str):
        vocabulary = set(text)
        return cls(sorted(list(vocabulary)))

    def encode(self, text:str):
        ids = [self.ctoi[c] for c in text]
        return torch.tensor(ids, dtype=torch.long)
    
    def decode(self, ids:torch.tensor):
        chars = [self.itoc[id] for id in ids.tolist()]
        return ''.join(chars)
    
    def __len__(self):
        return len(self.ctoi)
