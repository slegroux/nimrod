# @package _global_

run_id: ${now:%Y-%m-%d}_${now:%H-%M-%S}
project: "MNIST-TEST"
ckpt_path: null # ckpt to resume training from
seed: 42
test: True
train: True
ignore_warnings: True
tags: ["dev"]

paths:
  root_dir: .
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}

hydra:
  defaults:
    - override hydra_logging: colorlog
    - override job_logging: colorlog
  run:
    dir: ${paths.log_dir}/runs/${run_id}
  sweep:
    dir: ${paths.log_dir}/multiruns/${run_id}
    subdir: ${hydra.job.num}

datamodule:
  _target_: nimrod.data.datasets.MNISTDataModule
  data_dir: "~/Data"
  train_val_test_split: [0.8, 0.1, 0.1]
  batch_size: 64
  num_workers: 4
  pin_memory: False

model:
  _target_: nimrod.models.mlp.MLP_PL
  mlp:
    _target_: nimrod.models.mlp.MLP
    n_in: 784
    n_h: 64
    n_out: 10
  lr: 1e-3

trainer:
  _target_: pytorch_lightning.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 1 # prevents early stopping
  max_epochs: 2
  devices: 1
  accelerator: 'cpu'
  #strategy: ddp
  precision: "16-mixed"
  # auto_lr_find: False
  # perform a validation loop every N training epochs
  check_val_every_n_epoch: 1
  # set True to to ensure deterministic results makes training slower but gives more reproducibility than just setting seeds
  deterministic: False
  benchmark: False
  fast_dev_run: False

callbacks:
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/loss
    mode: min
    patience: 10
    min_delta: 0
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val/loss"
    mode: min
    save_top_k: 1
    save_last: True
    verbose: True
    dirpath: ${paths.output_dir}/checkpoints
    filename: "epoch{epoch:03d}-val_loss{val/loss:.2f}"
    auto_insert_metric_name: False

logger:
  _target_: pytorch_lightning.loggers.wandb.WandbLogger
  project: ${project}
  name: ${run_id}
  save_dir: ${paths.output_dir}
  offline: False # store only locally
  id: null # give id to resume experiment
  entity: "slegroux"
  log_model: False # log checkpoints at end of training
  prefix: ""
  group: ""
  tags: ${tags}

profiler:
  _target_: pytorch_lightning.profilers.SimpleProfiler
  dirpath: ${paths.output_dir}
  filename: "simple_perf_logs"
  extended: True